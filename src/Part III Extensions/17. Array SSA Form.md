# Chapter 17. Array SSA Form
**Vivek Sarkar, Kathleen Knobe, and Stephen Fink**

In this chapter, we introduce an Array SSA form that captures element-level data-flow information for array variables and coincides with standard SSA form when applied to scalar variables. Any program with arbitrary control-flow structures and arbitrary array subscript expressions can be automatically converted to this Array SSA form, thereby making it applicable to structures, heap objects, and any other data structure that can be modelled as a logical array. A key extension over standard SSA form is the introduction of a _definition-$\Phi$_ function (denoted $\Phi_{\mathit{def}}$) that is capable of merging values from distinct array definitions on an element-by-element basis. There are several potential applications of Array SSA form in compiler analysis and optimization of sequential and parallel programs. In this chapter, we focus on sequential programs and use _constant propagation_ as an exemplar of a program analysis that can be extended to array variables using Array SSA form and _redundant load elimination_ as an exemplar of a program optimization that can be extended to heap objects using Array SSA form.

The rest of the chapter is organized as follows. Section 17.1 introduces _full Array SSA form_ for runtime evaluation and _partial Array SSA form_ for static analysis. Section 17.2 extends the scalar SSA constant propagation algorithm to enable constant propagation through array elements. This includes an extension to the constant propagation lattice to efficiently record information about array elements and an extension to the worklist algorithm to support _definition-$\Phi$_ functions(Sect. 17.2.1) and a further extension to support non-constant (symbolic) array subscripts (Sect. 17.2.2). Section 17.3 shows how Array SSA form can be extended to support elimination of redundant loads of object fields and array elements in strongly typed languages, and Sect. 17.4 contains suggestions for further reading.

## 17.1 Array SSA Form

To introduce full Array SSA form with runtime evaluation of $\Phi$-functions, we use the concept of an _iteration vector_ to differentiate among multiple dynamic instances of a static definition, $S_{k}$, that occur in the same dynamic instance of $S_{k}$'s enclosing procedure, $f()$. Let $n$ be the number of loops that enclose $S_{k}$ in procedure $f()$. These loops could be for-loops, while-loops, or even loops constructed out of goto statements. For convenience, we treat the outermost region of acyclic control flow in a procedure as a dummy outermost loop with a single iteration, thereby ensuring that $n\geq 1$.

A single point in the iteration space is specified by the iteration vector $\vec{i}=(i_{1},\,\ldots,\,i_{n})$, which is an $n$-tuple of iteration numbers, one for each enclosing loop. For convenience, this definition of iteration vectors assumes that all loops are single-entry, or equivalently, that the control-flow graph is _reducible_. (This assumption is not necessary for partial Array SSA form.) For single-entry loops, we know that each def executes at most once in a given iteration of its surrounding loops, and hence the iteration vector serves the purpose of a "timestamp." The key extensions in Array SSA form relative to standard SSA form are as follows:

1. **Renamed array variables:** All array variables are renamed so as to satisfy the Static Single Assignment property. Analogous to standard SSA form, control $\Phi$-functions are introduced to generate new names for merging two or more prior definitions at control-flow join points and to ensure that each use refers to precisely one definition.
2. **Array-valued @ variables:** For each static definition $A_{j}$, we introduce an @_variable_ (pronounced "at variable") @$A_{j}$ that identifies the most recent _iteration vector at_ which definition $A_{j}$ was executed. We assume that all @ variables are initialized to the empty vector, ( ), at the start of program execution. Each update of a single array element, $A_{j}[k]\leftarrow\ldots$, is followed by the statement, @$A_{j}[k]\leftarrow\vec{i}$, where $\vec{i}$ is the iteration vector for the loops surrounding the definition of $A_{j}$.
3. **Definition $\Phi^{\star}$s:** A _definition_$\Phi$ operator $\Phi_{def}$ is introduced in Array SSA form to deal with preserving ("non-killing") definitions of arrays. Consider $A_{0}$ and $A_{1}$, two renamed arrays that originated from the same array variable in the source program such that $A_{1}[k]\leftarrow\ldots$ is an update of a single array element and $A_{0}$ is the prevailing definition at the program point just prior to the definition of $A_{1}$. A definition $\Phi$,$A_{2} \leftarrow \Phi_{d e f}\left(A_{1}, @ A_{1}, A_{0}, @ A_{0}\right)$ , is inserted immediately after the definitions for $A_{1}$ and @$A_{1}$. Since definition $A_{1}$ only updates one element of $A_{0}$$A_{2}$ represents an element-level merge of arrays $A_{1}$ and $A_{0}$. Definition $\Phi$'s did not need to be inserted in standard SSA form because a scalar definition completely kills the old value of the variable.
4. **Array-valued $\Phi$-functions:** Another consequence of renaming arrays is that a $\Phi$-function for array variables must also return an array value. Consider a (control or definition) $\Phi$-function of the form, $A_{2} \leftarrow \Phi\left(A_{1}, @ A_{1}, A_{0}, @ A_{0}\right)$. Its semantics can be specified precisely by the following conditional expression for each element, $A_{2}[j]$, in the result array $A_{2}$:

![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310092235304.png)

The key extension over the scalar case is that the conditional expression specifies an element-level merge of arrays $A_{1}$ and $A_{0}$.

Figure 17.1 shows an example program with an array variable and the conversion of the program to full Array SSA form as defined above.

We now introduce a _partial Array SSA form_ for static analysis, which serves as an approximation of full Array SSA form. Consider a (control or definition) $\Phi$-function, $A_{2} \leftarrow \Phi\left(A_{1}, @ A_{1}, A_{0}, @ A_{0}\right)$. A static analysis will need to approximate the computation of this $\Phi$-function by some data-flow transfer function, $\mathscr{L}_{\Phi}$. The inputs and output of $\mathscr{L}_{\Phi}$ will be _lattice elements_ for scalar/array variables that are compile-time approximations of their runtime values. We use the notation $\mathscr{L}(V)$ to denote the lattice element for a scalar or array variable $V$. Therefore, the statement,$A_{2}\leftarrow\Phi(A_{1},\,@\!A_{1},\,A_{0},\,@\!A_{0})$, will in general be modelled by the data-flow equation, $\mathscr{L}(A_{2})=\mathscr{L}_{\Phi}(\mathscr{L}(A_{1}),\,\mathscr{L}(@\!A_{1}),\,\mathscr{L}(A_{0}),\,\mathscr{L}(@\!A_{0}))$.

![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310092236277.png)

While the _runtime_ semantics of $\Phi$-functions for array variables critically depends on @ variables (Eq. 17.1), many _compile-time analyses_ do not need the full generality of @ variables. For analyses that do not distinguish among iteration instances, it is sufficient to model $A_{2}\leftarrow\Phi(A_{1},\,@\!A_{1},\,A_{0},\,@\!A_{0})$ by a data-flow equation, $\mathscr{L}(A_{2})=\mathscr{L}_{\Phi}(\mathscr{L}(A_{1}),\,\mathscr{L}(A_{0}))$, that does not use lattice variables $\mathscr{L}(@\!A_{1})$ and $\mathscr{L}(@\!A_{0})$. For such cases, a _partial_ Array SSA form can be obtained by dropping @ variables, and using the $\phi$ operator, $A_{2}\leftarrow\phi(A_{1},\,A_{0})$ instead of $A_{2}\leftarrow\Phi(A_{1},\,@\!A_{1},\,A_{0},\,@\!A_{0})$. A consequence of dropping @ variables is that partial Array SSA form does not need to deal with iteration vectors and therefore does not require the control-flow graph to be reducible as in full Array SSA form. For scalar variables, the resulting $\phi$-function obtained by dropping @ variables exactly coincides with standard SSA form.

## 17.2 Sparse Constant Propagation of Array Elements

### 17.2.1 Array Lattice for Sparse Constant Propagation

In this section, we describe the lattice representation used to model array values for constant propagation. Let $\mathscr{U}_{ind}^{A}$ and $\mathscr{U}_{elem}^{A}$ be the universal set of _index values_ and the universal set of array _element values_, respectively, for an array variable $A$. For an array variable, the set denoted by lattice element $\mathscr{L}(A)$ is a subset of index-element pairs in $\mathscr{U}_{ind}^{A}\times\mathscr{U}_{elem}^{A}$. There are three kinds of lattice elements for array variables that are of interest in our framework:

1. $\mathscr{L}(A)=\top\quad\Rightarrow\quad\mathtt{set}(\mathscr{L}(A))=\{\ \}$ This "top" case indicates that the set of possible index-element pairs that have been identified thus far for $A$ is the empty set, $\{\ \}$.
2. $\mathscr{L}(A)=\langle(i_{1},\,e_{1}),\,(i_{2},\,e_{2}),\ldots\rangle$ $\Rightarrow\quad\mathtt{set}(\mathscr{L}(A))=\{(i_{1},\,e_{1}),\,(i_{2},\,e_{ 2}),\,\ldots\}\ \cup\ (\mathscr{U}_{ind}^{A}-\{i_{1},\,i_{2},\ldots\})\times \mathscr{U}_{elem}^{A}$ The lattice element for this "constant" case is represented by a finite list of constant index-element pairs, $\langle(i_{1},\,e_{1}),\,(i_{2},\,e_{2}),\,\ldots\rangle$. The constant indices, $i_{1},\,i_{2},\,\ldots$, must represent distinct (non-equal) index values. The meaning of this lattice element is that the current stage of analysis has identified some finite number of constant index-element pairs for array variable $A$, such that $A[i_{1}]=e_{1}$, $A[i_{2}]=e_{2}$, etc. All other elements of $A$ are assumed to be non-constant. (Extensions to handle non-constant indices are described in Sect. 17.2.2.)
3. $\mathscr{L}(A)=\bot\quad\Rightarrow\quad\mathtt{set}(\mathscr{L}(A))= \mathscr{U}_{ind}^{A}\times\mathscr{U}_{elem}^{A}$ This "bottom" case indicates that, according to the approximation in the current stage of analysis, array $A$ may take on any value from the universal set of index-element pairs. Note that $\mathscr{L}(A)=\bot$ is equivalent to an empty list, $\mathscr{L}(A)=\langle\ \rangle$, in case (2) above; they both denote the universal set of index-element pairs.

![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310092237970.png)

We now describe how array lattice elements are computed for various operations that appear in Array SSA form. We start with the simplest operation viz. a read access to an array element. Figure 17.2 shows how $\mathscr{L}(A_{1}[k])$, the lattice element for array reference $A_{1}[k]$, is computed as a function of $\mathscr{L}(A_{1})$ and $\mathscr{L}(k)$, the lattice elements for $A_{1}$ and $k$. We denote this function by $\mathscr{L}_{[\,]}$, i.e., $\mathscr{L}(A_{1}[k])=\mathscr{L}_{[\,]}(\mathscr{L}(A_{1}),\,\mathscr{L}(k))$. The interesting case in Fig. 17.2 occurs in the middle cell when neither $\mathscr{L}(A_{1})$ nor $\mathscr{L}(k)$ is $\top$ or $\bot$. The notation $\mathscr{DS}$ in the middle cell in Fig. 17.2 represents a "definitely same" binary relation, i.e., $\mathscr{DS}(a,\,b)=\mathsf{true}$ if and only if $a$ and $b$ are known to have exactly the same value.

Next, consider a write access of an array element, which in general has the form $A_{1}[k]\gets i$. Figure 17.3 shows how $\mathscr{L}(A_{1})$, the lattice element for the array being written into, is computed as a function of $\mathscr{L}(k)$ and $\mathscr{L}(i)$, the lattice elements for $k$ and $i$. We denote this function by $\mathscr{L}_{d[\,]}$, i.e., $\mathscr{L}(A_{1})=\mathscr{L}_{d[\,]}(\mathscr{L}(k),\,\mathscr{L}(i))$. As before, the interesting case in Fig. 17.3 occurs in the middle cell when both $\mathscr{L}(k)$ and $\mathscr{L}(i)$ are constant. For this case, the value returned for $\mathscr{L}(A_{1})$ is simply the singleton list, $\langle\ (\mathscr{L}(k),\,\mathscr{L}(i))\ \rangle$, which contains exactly one constant index-element pair.

Now, we turn our attention to the $\phi$-function. Consider a definition-$\phi$-function of the form, $A_{2}\leftarrow\phi_{\mathit{def}}(A_{1},\,A_{0})$. The lattice computation for $\mathscr{L}(A_{2})=\mathscr{L}_{\phi_{\mathit{def}}}(\mathscr{L}(A_{1}),\, \mathscr{L}(A_{0}))$ is shown in Fig. 17.4. Since $A_{1}$ corresponds to a definition of a single array element, the list for $\mathscr{L}(A_{1})$ can contain at most one pair (see Fig. 17.3). Therefore, the three cases considered for $\mathscr{L}(A_{1})$ in Fig. 17.4 are $\mathscr{L}(A_{1})=\top$, $\mathscr{L}(A_{1})=\langle(i^{\prime},e^{\prime})\rangle$, and $\mathscr{L}(A_{1})=\bot$.

![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310092239252.png)

The notation $\texttt{update}((i^{\prime},e^{\prime}),\,\langle(i_{1},e_{1}),\ldots\rangle)$ used in the middle cell in Fig. 17.4 denotes a special update of the list $\mathscr{L}(A_{0})=\langle(i_{1},e_{1}),\ldots\rangle$ with respect to the constant index-element pair $(i^{\prime},e^{\prime})$. update involves four steps:

1. Compute the list $T=\{\ (i_{j},e_{j})\mid(i_{j},e_{j})\in\mathscr{L}(A_{0})\ \text{and}\ \mathscr{D} \mathscr{D}(i^{\prime},i_{j})=\text{true}\ \}$. Analogous to $\mathscr{D}\mathscr{F}$, $\mathscr{D}\mathscr{D}$ denotes a "definitely different" binary relation, i.e., $\mathscr{D}\mathscr{D}(a,b)=\text{true}$ if and only if $a$ and $b$ are known to have distinct (non-equal) values.
2. Insert the pair $(i^{\prime},e^{\prime})$ into $T$ to obtain a new list, $I$.
3. (Optional) If there is a desire to bound the height of the lattice due to compile-time considerations and the size of list $I$ exceeds a threshold size $Z$, then one of the pairs in $I$ can be dropped from the output list so as to satisfy the size constraint.
4. Return $I$ as the value of $\texttt{update}((i^{\prime},e^{\prime}),\,\langle(i_{1},e_{1})\ \ldots\rangle)$.

Finally, consider a control $\phi$-function that merges two array values, $A_{2}\leftarrow\phi(A_{1},\,A_{0})$. The join operator ($\sqcap$) is used to compute $\mathscr{L}(A_{2})$, the lattice element for $A_{2}$, as a function of $\mathscr{L}(A_{1})$ and $\mathscr{L}(A_{0})$, the lattice elements for $A_{1}$ and $A_{0}$, i.e., $\mathscr{L}(A_{2})=\mathscr{L}_{\phi}(\mathscr{L}(A_{1}),\,\mathscr{L}(A_{0} ))=\mathscr{L}(A_{1})\sqcap\mathscr{L}(A_{0})$. The rules for computing this join operator are shown in Fig. 17.5, depending on different cases for $\mathscr{L}(A_{1})$ and $\mathscr{L}(A_{0})$. The notation $\mathscr{L}(A_{1})\cap\mathscr{L}(A_{0})$ used in the middle cell in Fig. 17.5 denotes a simple intersection of lists $\mathscr{L}(A_{1})$ and $\mathscr{L}(A_{0})$--the result is a list of pairs that appear in both $\mathscr{L}(A_{1})$ and $\mathscr{L}(A_{0})$.


![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310092239721.png)

![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310092240902.png)

We conclude this section by discussing the example program in Fig. 17.6a. The partial Array SSA form for this example is shown in Fig. 17.6b, and the data-flow equations for this example are shown in Fig. 17.7a. Each assignment statement in the partial Array SSA form (in Fig. 17.6b) results in one data-flow equation (in Fig. 17.7a); the numbering S1 through S8 indicates the cor can be used for these data-flow equations, including the standard worklist-based algorithm for constant propagation using scalar SSA form. The fixpoint solution is shown in Fig. 17.7b. This solution was obtained assuming $\mathscr{L}(I)=\bot$. If, instead, variable $I$ is known to equal 3, i.e., $\mathscr{L}(I)=3$, then the lattice variables that would be obtained after the fixpoint iteration step has completed are shown in Fig. 17.7c. In either case ($\mathscr{L}(I)=\bot$ or $\mathscr{L}(I)=3$), the resulting array element constants revealed by the algorithm can be used in whatever analyses or transformations the compiler considers to be profitable to perform.

### 17.2.2 Beyond Constant Indices

In this section, we address constant propagation through _non-constant array subscripts_, as a generalization of the algorithm for constant subscripts described in Sect. 17.2.1. As an example, consider the program fragment in Fig. 17.8. In the loop, we see that the read access of $a[i]$ will have a constant value ($k\times 5=10$), even though the index/subscript value $i$ is not a constant. We would like to extend the framework from Sect. 17.2.1 to be able to recognize the read of $a[i]$ as constant in such programs. There are two key extensions that need to be considered for non-constant (symbolic) subscript values:
![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310092240606.png)
* For constants, $C_{1}$ and $C_{2}$, $\mathscr{DS}(C_{1}$, $C_{2})\neq\mathscr{DD}(C_{1}$, $C_{2})$. However, for two symbols, $S_{1}$ and $S_{2}$, it is possible that both $\mathscr{DS}(S_{1}$, $S_{2})$ and $\mathscr{DD}(S_{1}$, $S_{2})$ are false, that is, we do not know if they are the same or different.
* For constants, $C_{1}$ and $C_{2}$, the values for $\mathscr{DS}(C_{1}$, $C_{2})$ and $\mathscr{DD}(C_{1}$, $C_{2})$ can be computed by inspection. For symbolic indices, however, some program analysis is necessary to compute the $\mathscr{DS}$ and $\mathscr{DD}$ relations.

We now discuss the compile-time computation of $\mathscr{DS}$ and $\mathscr{DD}$ for symbolic indices. Observe that, given index values $I_{1}$ and $I_{2}$, only one of the following three cases is possible:
![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310092241128.png)
The first case is the most conservative solution. In the absence of any other knowledge, it is always correct to state that $\mathscr{DS}(I_{1}$, $I_{2})=\textsf{false}$ and $\mathscr{DD}(I_{1}$, $I_{2})=\textsf{false}$.

The problem of determining if two symbolic index values are the same is equivalent to the classical problem of _global value numbering_. If two indices $i$ and $j$ have the same value number, then $\mathscr{DS}(i$, $j)$ must $=\textsf{true}$. The problem of computing $\mathscr{DD}$ is more complex. Note that $\mathscr{DD}$, unlike $\mathscr{DS}$, is not an equivalence relation because $\mathscr{DD}$ is not transitive. If $\mathscr{DD}(A$, $B)=\textsf{true}$ and $\mathscr{DD}(B$, $C)=\textsf{true}$, it does not imply that $\mathscr{DD}(A$, $C)=\textsf{true}$. However, we can leverage past work on array dependence analysis to identify cases for which $\mathscr{DD}$ can be evaluated to true. For example, it is clear that $\mathscr{DD}(i,i+1)=\textsf{true}$ and that $\mathscr{DD}(i,0)=\textsf{true}$ if $i$ is a loop index variable that is known to be $\geq 1$.

Let us consider how the $\mathscr{DS}$ and $\mathscr{DD}$ relations for symbolic index values are used by our constant propagation algorithms. Note that the specification of how $\mathscr{DS}$ and $\mathscr{DD}$ are used is a separate issue from the precision of the $\mathscr{DS}$ and $\mathscr{DD}$ values. We now describe how the lattice and the lattice operations presented in Sect. 17.2.1 can be extended to deal with non-constant subscripts.

![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310092243209.png)

First, consider the lattice itself. The $\top$ and $\bot$ lattice elements retain the same meaning as in Sect. 17.2.1 viz. set($\top$) = { } and set($\bot$) = $\mathscr{U}^{A}_{ind}\times\mathscr{U}^{A}_{elem}$. Each element in the lattice is a list of index-value pairs where the value is still required to be constant but the index may be symbolic--the index is represented by its value number ($\mathsf{VN}(i)$ for SSA variable $i$).

![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310092243924.png)

We now revisit the processing of an array element read of $A_{1}[k]$ and the processing of an array element write of $A_{1}[k]$. These operations were presented in Sect. 17.2.1 (Figs. 17.2 and 17.3) for constant indices. The versions for non-constant indices appear in Figs. 17.9 and 17.10. For the read operation in Fig. 17.9, if there exists a pair $(i_{j},e_{j})$ such that $\mathscr{D}\mathscr{S}(i_{j},\,k)=\mathsf{true}$ (i.e., $i_{j}$ and $\mathsf{VN}(k)$ have the same value number), then the result is $e_{j}$. Otherwise, the result is $\top$ or $\bot$ as specified in Fig. 17.9. For the write operation in Fig. 17.10, if the value of the right-hand side, $i$, is a constant, the result is the singleton list $((\mathsf{VN}(k),\,\mathscr{L}(i)))$. Otherwise, the result is $\top$ or $\bot$ as specified in Fig. 17.10.

Let us now consider the propagation of lattice values through $\phi_{def}$ operators. The only extension required relative to Fig. 17.4 is that the $\mathscr{D}\mathscr{D}$ relation used in performing the update operation should be able to determine when $\mathscr{D}\mathscr{D}(i^{\prime},i_{j})=\mathsf{true}$ if $i^{\prime}$ and $i_{j}$ are symbolic value numbers rather than constants. (If no symbolic information is available for $i^{\prime}$ and $i_{j}$, then it is always safe to return $\mathscr{D}\mathscr{D}(i^{\prime},i_{j})=\mathsf{false}$.)and definitely different analyses can be extended to heap array indices (Sect. 17.3.2), followed by a _scalar promotion_ transformation that uses the analysis results to perform load elimination (Sect. 17.3.3).
## 17.3 Extension to Objects: Redundant Load Elimination

In this section, we introduce redundantload elimination as an exemplar of a program optimization that can be extended to heap objects in strongly typed languages by using Array SSA form. This extension models object references (pointers) as indices into hypothetical heap arrays (Sect. 17.3.1). We then describe how definitely same and definitely different analyses can be extended to heap array indices (Sect. 17.3.2), followed by a $\text{scalar promotion}$ transformation that uses the analysis results to perform load elimination (Sect. 17.3.3).

### 17.3.1 Analysis Framework

We introduce a formalism called _heap arrays_ which allows us to model object references as associative arrays. An _extended Array SSA_ form is constructed on heap arrays by adding _use_-$\phi$ functions (denoted $\phi_{use}$). For each field $x$, we introduce a hypothetical one-dimensional heap array, $\mathscr{H}^{x}$. Heap array $\mathscr{H}^{x}$ consolidates all instances of field $x$ present in the heap. Heap arrays are indexed by object references. Thus, a getfield of $p.x$ is modelled as a read of element $\mathscr{H}^{x}[p]$, and a putfield of $q.x$ is modelled as a write of element $\mathscr{H}^{x}[q]$. The use of distinct heap arrays for distinct fields leverages the fact that accesses to distinct fields must be directed to distinct memory locations in a strongly typed language. Note that field $x$ is considered to be the same field for objects of types $C_{1}$ and $C_{2}$, if $C_{2}$ is a subtype of $C_{1}$. Accesses to one-dimensional array objects with the same element type are modelled as accesses to a single _two-dimensional_ heap array for that element type, with one dimension indexed by the object reference as in heap arrays for fields and the second dimension indexed by the integer subscript.

Heap arrays are renamed in accordance with an _extended_ Array SSA form that contains three kinds of $\phi$-functions:

1. A _control_-$\phi$-function ($\phi$) from scalar SSA form.
2. A _definition_-$\phi$-function ($\phi_{def}$) from Array SSA form.
3. A _use_-$\phi$-function ($\phi_{use}$) function creates a new name whenever a statement reads a heap array element. $\phi_{use}$-functions represent the extension in "extended" Array SSA form.

The main purpose of the $\phi_{use}$-function is to link together load instructions for the same heap array in control-flow order. While $\phi_{use}$-functions are used by the redundant load elimination optimization presented in this chapter, it is not necessary for analysis algorithms (e.g., constant propagation) that do not require the creation of a new name at each use.

### 17.3.2 Definitely Same and Definitely Different Analyses for Heap Array Indices

In this section, we show how global value numbering and allocation-site information can be used to efficiently compute _definitely same_ ($\mathscr{D}\mathscr{S}$) and _definitely different_ ($\mathscr{D}\mathscr{D}$) information for heap array indices, thereby reducing pointer analysis queriesto array index queries. If more sophisticated pointer analyses are available in a compiler, they can be used to further refine the $\mathscr{D}\mathscr{S}$ and $\mathscr{D}\mathscr{D}$ information.

![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310092245857.png)

As an example, Fig. 17.11 illustrates two different cases of scalar promotion (load elimination) for object fields. The notation $\mathtt{Hx}[\mathtt{p}]$ refers to a read/write access of heap array element, $\mathscr{H}^{x}[\mathtt{p}]$. For the original program in Fig. 17.11a, introducing a scalar temporary $\mathtt{T1}$ for the store (def) of $\mathtt{p}.\mathtt{x}$ can enable the load (use) of $\mathtt{r}.\mathtt{x}$ to be eliminated, i.e., to be replaced by a use of $\mathtt{T1}$. Figure 17.11b contains an example in which a scalar temporary ($\mathtt{T2}$) is introduced for the first load of $\mathtt{p}.\mathtt{x}$, thus enabling the second load of $\mathtt{r}.\mathtt{x}$ to be eliminated, i.e., replaced by $\mathtt{T2}$. In both cases, the goal of our analysis is to determine that the load of $\mathtt{r}.\mathtt{x}$ is redundant, thereby enabling the compiler to replace it by a use of scalar temporary that captures the value in $\mathtt{p}.\mathtt{x}$. We need to establish two facts to perform this transformation: 1. object references $p$ and $r$ are identical (definitely same) in all program executions, and 2. object references $q$ and $r$ are distinct (definitely different) in all program executions.

As before, we use the notation $\mathsf{VN}(i)$ to denote the value number of SSA variable $i$. Therefore, if $\mathsf{VN}(i)=\mathsf{VN}(j)$, then $\mathscr{D}\mathscr{S}(i,j)=\mathsf{true}$. For the code fragment above, the statement, $p\;\leftarrow\;r$, ensures that $p$ and $r$ are given the same value number, $\mathsf{VN}(p)=\mathsf{VN}(r)$, so that $\mathscr{D}\mathscr{S}(p,r)=\mathsf{true}$. The problem of computing $\mathscr{D}\mathscr{D}$ for object references is more complex than value numbering and relates to pointer alias analysis. We outline a simple and sound approach below, which can be replaced by more sophisticated techniques as needed. It relies on two observations related to allocation-sites:

1. Object references that contain the results of distinct allocation-sites must be different.
2. An object reference containing the result of an allocation-site must be different from any object reference that occurs at a program point that dominates the allocation-site in the control-flow graph. For example, in Fig. 17.11, the presence of the allocation-site in $q\leftarrow$ new Type1 ensures that $\mathscr{DD}(p,q)=\mathsf{true}$.

### 17.3.3 Scalar Promotion Algorithm

The main program analysis needed to enable redundant load elimination is _index propagation_, which identifies the set of indices that are _available_ at a specific def/use $A_{i}$ of heap array $A$. Index propagation is a data-flow problem, the goal of which is to compute a lattice value $\mathscr{L}(\mathscr{H})$ for each renamed heap variable $\mathscr{H}$ in the Array SSA form such that a load of $\mathscr{H}[\vec{i}]$ is _available_ if $\mathscr{V}(\vec{i})\in\mathscr{L}(\mathscr{H})$. Note that the lattice element does not include the value of $\mathscr{H}[\vec{i}]$ (as in constant propagation), just the fact that it is available. Figures 17.12, 17.13, and 17.14 give the lattice computations

![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310092246793.png)
![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310092246329.png)


which define the index propagation solution. The notation update($\vec{i^{\prime}}$, $\langle\vec{i_{1}},\ldots\rangle$) used in the middle cell in Fig. 17.12 denotes a special update of the list $\mathscr{L}(A_{0})=\langle\vec{i_{1}},\ldots\rangle$ with respect to index $\vec{i^{\prime}}$. update involves four steps:

1. Compute the list $T=\{\ \vec{i_{j}}\ |\ \vec{i_{j}}\in\mathscr{L}(A_{0})\text{ and }\mathscr{D} \mathscr{D}(\vec{i^{\prime}},\vec{i_{j}})=\text{true}\ \}$. List $T$ contains only those indices from $\mathscr{L}(A_{0})$ that are _definitely different_ from $\vec{i^{\prime}}$.
2. Insert $\vec{i^{\prime}}$ into $T$ to obtain a new list, $I$.
3. (Optional) As before, if there is a desire to bound the height of the lattice due to compile-time considerations and the size of list $I$ exceeds a threshold size $Z$, then any one of the indices in $I$ can be dropped from the output list.
4. Return $I$ as the value of update($\vec{i^{\prime}}$, $\langle\vec{i_{1}},\ldots\rangle$).

After index propagation, the algorithm selects a load, $A_{j}[\vec{x}]$, for scalar promotion if and only if index propagation determines that an index with value number $\mathsf{VN}(()\vec{x})$ is available at the def of $A_{j}$. Figure 17.15 illustrates a trace of this load elimination algorithm for the example program in Fig. 17.11a. Figure 17.15a shows the extended Array SSA form computed for this example program. The results of index propagation are shown in Fig. 17.15b. These results depend on definitely different analysis establishing that $\mathsf{VN}(p)\neq\mathsf{VN}(q)$ and definitely same analysis establishing that $\mathsf{VN}(p)=\mathsf{VN}(r)$. Figure 17.15c shows the transformed code after performing the scalar promotion actions. The load of p.x has thus been eliminated in the transformed code and replaced by a use of the scalar temporary, T1.

## 17.4 Further Reading

In this chapter, we introduced an Array SSA form that captures element-level data-flow information for array variables, illustrated how it can be used to extend program analyses for scalars to array variables using constant propagation as an exemplar, and illustrated how it can be used to extend optimizations for scalars to array variables using load elimination in heap objects as an exemplar. In addition to reading the other chapters in this book for related topics, the interested reader can consult [168] for details on full Array SSA form, [169] for details on constant propagation using Array SSA form, [120] for details on load and store elimination for pointer-based objects using Array SSA form, [253] for efficient dependence analysis of pointer-based array objects, and [18] for extensions of this load elimination algorithm to parallel programs.

There are many possible directions for future research based on this work. The definitely same and definitely different analyses outlined in this chapter are sound but conservative. In restricted cases, they can be made more precise using array subscript analysis from polyhedral compilation frameworks. Achieving a robust integration of Array SSA and polyhedral approaches is an interesting goal for future research. Past work on Fuzzy Array Data-flow Analysis (FADA) [19] may provide a useful foundation for exploring such an integration. Another interesting direction is to extend the value numbering and definitely different analyses mentioned in Sect. 17.2.2 so that they can be combined with constant propagation rather than performed as a pre-pass. An ultimate goal is to combine conditional constant, type propagation, value numbering, partial redundancy elimination, and otion analyses within a single framework that can be used to analyse scalar variables, array variables, and pointer objects with a unified approach.