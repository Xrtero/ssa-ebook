# Chapter 16. Hashed SSA Form - HSSA

**Massimiliano Mantione and Fred Chow**


Hashed SSA (or in short, HSSA) is an SSA extension that can effectively represent how aliasing relations affect a program in SSA form. It works equally well for aliasing among scalar variables and, more generally, for indirect load and store operations on arbitrary memory locations. Thus, all common SSA-based optimizations can be applied uniformly to any storage area no matter how they are represented in the program.

It should be noted that only the representation of aliasing is discussed here. HSSA relies on a separate alias analysis pass that runs before its creation. Depending on the actual alias analysis algorithm used, the HSSA representation will reflect the accuracy produced by the alias analysis pass.

This chapter starts by defining notations to model the effects of aliasing for scalar variables in SSA form. We then introduce a technique that can reduce the overhead linked to the SSA representation by avoiding an explosion in the number of SSA versions for aliased variables. Next, we introduce the concept of _virtual variables_ to model indirect memory operations as if they were scalar variables, effectively allowing indirect memory operations to be put into SSA form together with scalar variables. Finally, we apply global value numbering (GVN) to the program to derive Hashed SSA form[^1] as the effective SSA representation of all storage entities in the program.

[^1]: The name _Hashed_ SSA comes from the use of hashing in value numbering.

## 16.1 SSA and Aliasing: $\mu$- and $\chi$-Functions

Aliasing occurs in a program when a storage location (that contains a value) referred to in the program code can potentially be accessed through a different means; this can occur under one of the following four conditions:

* Two or more storage locations partially overlap. For example, in the C _union_ construct, a storage location can be accessed via different field names.
* A variable is pointed to by a pointer. In this case, the variable can be accessed in two ways: _directly_, through the variable name, and _indirectly_, through the pointer that holds its address.
* The address of a variable is passed in a procedure call. This enables the called procedure to access the variable indirectly.
* The variable is declared in the global scope. This allows the variable to potentially be accessed in any function call.

We seek to model the _effects_ of aliasing on a program in SSA form based on the results of the alias analysis performed. To characterize the effects of aliasing, we distinguish between two types of definitions of a variable: _MustDef_ and _MayDef_. A MustDef must redefine the variable and thus blocks the references to its previous definitions from that point on. A MayDef only potentially redefines the variable and so does not prevent previous definitions of the same variable from being referenced later in the program.[^2] We represent MayDef through the use of $\chi$-functions. On the use side, in addition to real uses of the variable, which are _MustUses_, there are _MayUses_ that arise in places in the program where there are potential references to the variable. We represent MayUse through the use of $\mu$-functions. The semantics of $\mu$ and $\chi$ operators can be illustrated through the C-like example in Fig. 16.1, where $*p$ represents an indirect access through pointer $p$. The argument of the $\mu$-operator is the potentially used variable. The argument of the $\chi$-operator is the potentially assigned variable itself, to express the fact that the variable's original value will _flow through_ if the MayDef does not modify the variable.

[^2]: MustDefs are often referred to as Killing Defs and MayDefs as Preserving or Non-killing Defs in the literature.

The use of $\mu$- and $\chi$-functions does not alter the complexity of transforming a program into SSA form. All that is necessary is a pre-pass that inserts them in the program. Ideally, $\mu$- and $\chi$-functions should be placed _parallel_ to the instruction that led to their insertion. Parallel instructions are represented in Fig. 16.1 using the notation introduced in Sect. 13.1.4. Nonetheless, practical implementations may choose to insert $\mu$- and $\chi$-functions before or after the instructions that involve aliasing. In particular, $\mu$-functions can be inserted immediately _before_ the involved statement or expression and $\chi$-operators immediately _after_ the statement. This distinction allows us to model call effects correctly: the called function appears to potentially use the values of variables before the call, and the potentially modified values appear after the call.

![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310092140758.png)

Thanks to the systematic insertion of $\mu$-functions and $\chi$-functions, an assignment to any scalar variable can be safely considered dead if it is not marked live by a standard SSA-based dead-code elimination. In our running example of Fig. 16.1c, the potential use of the assigned value of $i$ outside the function, represented through the $\mu$-function at the return, allows us to conclude that the assignment to $i_{4}$ is not dead.

## 16.2 Introducing "Zero Versions" to Limit Version Explosion

While it is true that $\mu$ and $\chi$ insertion does not alter the complexity of SSA construction, applying it to a production compiler as described in the previous section may make working with code in SSA form inefficient in programs that exhibit a lot of aliasing. Each $\chi$-function introduces a new version, and it may in turn cause new $\phi$-functions to be inserted. This can make the number of distinct variable versions needlessly large.

The major issue is that the SSA versions introduced by $\chi$-operators are useless for most optimizations that deal with variable values. $\chi$ definitions add uncertainty to the analysis of variable values: the actual value of a variable after a $\chi$ definition could be its original value, or it could be the one indirectly assigned by the $\chi$.

Our solution to this problem is to factor all variable versions that are considered _useless_ together, so that SSA versions are not wasted. We assign number 0 to this special variable version and call it _zero version_.

Our notion of useless versions relies on the concept of _real occurrence_ of a variable, which is an actual definition or use of a variable in the _original_ program. From this point of view, in the SSA form, variable occurrences in $\mu$-, $\chi$-, and $\phi$-functions are not regarded as real occurrences. In our example in Fig. 16.1, $i_{2}$ has no real occurrence, while $i_{1}$, $i_{3}$, and $i_{4}$ do have. The idea is that variable versions that have no real occurrence do not play important roles in the optimization of the program. Once the program is converted back from SSA form, these variables will no longer show up. Since they do not directly appear in the code and their values are usually unknown, we can dispense with the cost of distinguishing among them.

For these reasons, we assign the zero version to versions of variables that have no real occurrence and whose values are derived from at least one $\chi$-function through zero or more intervening $\phi$-functions. An equivalent recursive definition is as follows:

* The result of a $\chi$ has zero version if it has no real occurrence.
* If the operand of a $\phi$ is zero version, then the result of the $\phi$ is zero version if it has no real occurrence.

For a program in full SSA form, Algorithm 16 determines which variable versions are to be made zero version under the above definition. The algorithm assumes that only use-def edges (and _not_ def-use edges) are available. A _HasRealOcc_ flag is associated with each original SSA version, being set to true whenever it has a real occurrence in the program. This can be done during the initial SSA construction. A list _NonZeroPhiList_, initially empty, is also associated with each original program variable.

![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310092141795.png)

The loop from lines 2 to 12 of Algorithm 16.1 can be regarded as the initialization pass, processing each variable version once. The while loop from lines 14 to 25 is the propagation pass, with time bound by the length of the longest chain of contiguous $\phi$ assignments. This bound can easily be reduced to the deepest loop nesting depth of the program by traversing the versions based on a topological order of the forward control flow graph. All in all, zero-version detection in the presence of $\mu$- and $\chi$-functions does not significantly change the complexity of SSA construction, while the corresponding reduction in the number of variable versions can reduce the overhead in the subsequent SSA-based optimization phases.

Because zero versions can have multiple static assignments, they do not have fixed or known values. Thus, two zero versions of the same variable cannot be assumed to have identical values. The occurrence of a zero version breaks use-def chains. But since the results of $\chi$-functions have unknown values, zero versioning does not affect the performance of optimizations that propagate known values, such as constant propagation, because they cannot be propagated across points of MayDefs in any case. Optimizations that operate on real occurrences, such as equivalencing and redundancy detection, are also unaffected. In performing dead store elimination, zero versions have to be assumed live. Since zero versions can only have uses represented by $\mu$-functions, the chances of the deletion of stores associated with $\chi$-functions with zero versions are small. But if some later optimizations delete the code that contains a $\mu$-function, zero versioning could prevent its defining $\chi$-function from being recognized as dead. Overall, zero versioning should only cause a small loss of effectiveness in the subsequent SSA-based optimization phases.

## 16.3  SSA for Indirect Memory Operations: Virtual Variables

The techniques described in the previous sections only apply to scalar variables in the program and not to arbitrary memory locations accessed indirectly. As an example, in Fig. 16.1, $\mu$-, $\chi$-, and $\phi$-functions have been introduced to keep track of $i$'s use-defs, but $*p$ is not considered as an SSA variable. Thus, even though we can apply SSA-based optimizations to scalar variables when they are affected by aliasing, indirect memory access operations are not targeted in our optimizations.

This situation is far from ideal, because code written in current mainstream imperative languages (like C++, Java, or C#) typically contains many operations on data stored in unfixed memory locations. For instance, in C, a two-dimensional vector could be represented as a struct declared as "typedef struct {double x; double y;} point;." Then, we can have a piece of code that computes the modulus of the vector $p$ written as "m = (p->x * p->x) + (p->y * p->y) ;." As $x$ is accessed twice with both accesses yielding the same value, the second access could be optimized away, and the same goes for $y$. The problem is that $x$ and $y$ are not scalar variables: $p$ is a pointer variable, while "p->x" and "p->y"are indirect memory operations. Representing this code snippet in SSA form tells us that the value of $p$ never changes, but it reveals nothing about the values stored in the locations "p->x" and "p->y." It is worth noting that operations on array elements suffer from the same problem.

The purpose of HSSA is to put indirect memory operations in SSA form just like scalar variables, so we can apply all SSA-based optimizations to them uniformly. In our discussion, we use the C dereferencing operator _dereference_ to denote indirection from a pointer. This operator can be placed on either the left- or the right-hand side of the assignment operator. Appearances on the right-hand side represent indirect loads, while those on the left-hand side represent indirect stores. Examples include:

* $*p$: read memory at address $p$
* $*(p+4)$ : read memory at address $p+4$ (as in reading the field of an object at offset 4)
* $**p$: double indirection
* $*p$ =: indirect store

As noted above, indirect memory operations cannot be handled by the regular SSA construction algorithm because they are _operations_, while SSA construction works on _variables_ only. In HSSA, we represent the target locations of indirect memory operations using _virtual variables_. A virtual variable is an abstraction of a memory area and appears under HSSA thanks to the insertion of $\mu$-, $\chi$-, and $\phi$-functions. Indeed, like any other variable, they can also have aliases. For the same initial C code shown in Fig. 16.2a, we show two different scenarios after $\mu$-function and $\chi$-function insertion. In Fig. 16.2b, the two virtual variables introduced, $v^{*}$ and $w^{*}$, are associated with the memory locations pointed to by $*p$ and $*q$, respectively. As a result, $v^{*}$ and $w^{*}$ both alias with all the indirect memory operations (of lines 3, 5, 7, and 8). In Fig. 16.2c, $x^{*}$ is associated with the memory location pointed to by $b$ and $y^{*}$ is associated with the memory location pointed to by $b+1$. Assuming alias analysis determines that there is no alias between $b$ and $b+1$, only $x^{*}$ aliases with the indirect memory operations of line 3, and only $y^{*}$ aliases with the indirect memory operations of lines 5, 7, and 8.

![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310092142672.png)

It can be seen that the behaviour of a virtual variable in annotating the SSA representation is dictated by its definition. The only discipline imposed by HSSA is that each indirect memory operand must be associated with at least one virtual variable. At one extreme, there could be one virtual variable for each indirect memory operation. On the other hand, it may be desirable to cut down the number of virtual variables by making each virtual variable represent more forms of indirect memory operations. Called _assignment factoring_, this has the effect of replacing multiple use-def chains belonging to different virtual variables with one use-def chain that encompasses more nodes and thus more versions. At the other extreme, the most factored HSSA form would define only one single virtual variable that represents all indirect memory operations in the program.

In practice, it is best not to use assignment factoring among memory operations that do not alias among themselves. This provides high accuracy in the SSA representation without incurring additional representation overhead, because the total number of SSA versions is unaffected. On the other hand, among memory operations that alias among themselves, using multiple virtual variables would result in greater representation overhead. Zero versioning can also be applied to virtual variables to help reduce the number of versions. Appearances of virtual variables in the $\mu$- and $\chi$-functions next to their defined memory operations are regarded as _real_ occurrences in the algorithm that determines zero versions for them.

Virtual variables can be instrumented into the program during $\mu$-function and $\chi$-function insertion, in the same pass as for scalar variables. During SSA construction, virtual variables are handled just like scalar variables. In the resulting SSA form, the use-def relationships of the virtual variables will represent the use-def relationships among the memory access operations in the program. At this point, we are ready to complete the construction of the HSSA form by applying global value numbering (GVN).

## 16.4 Using GVN to Form HSSA

In the previous sections, we have laid the foundations for dealing with aliasing and indirect memory operations in SSA form: we introduced $\mu$- and $\chi$-operators to model aliases, applied zero versioning to keep the number of SSA versions acceptable, and defined virtual variables as a way to apply SSA to storage locations accessed indirectly. However, HSSA is incomplete unless _global value numbering_ is applied to handle scalar variables and indirect storage locations uniformly (see Chap. 11).

Value numbering works by assigning a unique number to every expression in the program with the idea that expressions identified by the same number are guaranteed to compute to the same value. The value number is obtained using a hash function applied to each node in an expression tree. For an internal node in the tree, the value number is a hash function of the operator and the value numbers of all its immediate operands. The SSA form enables value numbering to be applied on the global scope,taking advantage of the property that the same SSA version of a variable must store the same value regardless of where it appears.

GVN enables us to easily determine when two address expressions compute the same address when building HSSA. We can then construct the SSA form among indirect memory operations whose address expressions have the same value number. Because a memory location accessed indirectly may be assigned different values at different points in the program, having the same value number for address expressions is not a sufficient condition for the read of the memory location to have the same value number. This is where we make use of virtual variables. In HSSA, for two reads of indirect memory locations to have the same value number, apart from their address expressions having identical value numbers, an additional condition is that they must have the same SSA version for their virtual variable. If they are associated with different versions of their virtual variable, they will be assigned different value numbers, because their reads may return different values. This enables the GVN in HSSA to maintain the consistency whereby nodes with the same value number must yield the same value. Thus, in HSSA, indirect memory operations can be handled in the same rank as scalar variables, and they can benefit transparently from any SSA-based optimizations applied to the program. For instance, in the vector modulus computation described above, every occurrence of the expression "p ->x" will always have the same GVN and is therefore guaranteed to return the same value, allowing the compiler to emit code that avoids redundant memory reads (the same holds for "p ->y").

Using the same code example from Fig. 16.2, we can see in Fig. 16.3 that $p_{2}$ and $q_{2}$ have the same value number $h_{7}$, while $p_{1}$'s value number is $h_{1}$ and is different. The loads at lines 5 and 8 cannot be considered redundant between each other because the versions for $v$ ($v_{1}^{*}$ then $v_{2}^{*}$) are different. On the other hand, the load at line 8 can be safely avoided by reusing the value computed in line 7, as both their versions for $v$ ($v_{2}^{*}$) and their address expressions' value numbers are identical. As a last example, if all the associated virtual variable versions for an indirect memory store (defined in parallel by $\chi$-functions) are found to be dead, then it can be safely eliminated.[^3] In other words, HSSA transparently extends SSA's dead store elimination algorithm to indirect memory stores.

[^3]: Note that any virtual variable that aliases with a memory region live out of the compiled procedure is considered to alias with the return instruction of the procedure and as a consequence will lead to a live $\mu$-function.

Another advantage of HSSA in this context is that it enables uniform treatment of indirect memory operations regardless of the levels of indirections (as in the "$\star\star$p" expression in C which represents a double indirection). This happens naturally because each node of the expression is identified by its value number, and the fact that it is used as an address in another expression does not cause any additional complications.

![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310092146320.png)

Having explained why HSSA uses GVN, we are ready to explain how the HSSA intermediate representation is structured. A program in HSSA keeps its CFGstructure, with basic blocks made up of a sequence of statements (assignments, procedure calls, etc.), but the expression operands of each statement and the left-hand side of assignments are replaced by their corresponding entries in the hash table. Constants, variables (both scalar and virtual), and expressions all find their entries in the hash table. As the variables are in SSA form, each SSA version is assigned one and only one value number. An expression is hashed using its operator and the value numbers of its operands. An indirect memory operation is regarded as both expression and variable. Because it is not a leaf, it is hashed based on its operator and the value number of its address operand. Because it has the property of a variable, the value number of its associated virtual variable version is also included in the hashing. This is illustrated in lines 3, 5, 7, and 8 of the example code in Fig. 16.3. Such entries are referred to as _ivar_ nodes, for _indirect variables_, to denote their operational semantics and to distinguish them from the regular scalar variables.

From the code generation point of view, ivar nodes are operations and not variables. The compiler back end, when processing them, will emit indirect memory

accesses to the addresses specified as their operand. On the other hand, the virtual variables have no real counterpart in the program's code generation. In this sense, virtual variables can be regarded as a tool to associate aliasing effects with indirect memory operations and construct use-def relationships in the program that involves the indirect memory operations. Using virtual variables, indirect memory operations are no longer relegated to second-class citizens in SSA-based optimization phases.

As a supplementary note, in HSSA, because values and variables in particular can be uniquely identified using value numbers, the reference to SSA versions has been rendered redundant. In fact, to further ensure uniform treatment among scalar and indirect variables in the implementation, the use of SSA version numbers should be omitted in the representation.

## 16.5 Building HSSA

We now put together all the topics discussed in this chapter by detailing the steps to build HSSA starting from some non-SSA representations of the program. The first task is to construct the SSA form for the scalar and virtual variables, and this is displayed in Algorithm 16.2.

![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310092146537.png)

At the end of Algorithm 16.2, all scalar and virtual variables have SSA information, but the indirect memory operations are only "annotated" with virtual variables and cannot be regarded as being in SSA form.

Next, we perform a round of dead store elimination based on the constructed SSA form and then run our zero-version detection algorithm to detect zero versions in the scalar and virtual variables. These correspond to Steps 5 and 6 in Algorithm 16.3, respectively.

![](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310092147705.png)

At this point, however, the number of unique SSA versions have diminished because of the application of zero versioning. The final task is to build the HSSA representation of the program by applying GVN. The steps are displayed in Algorithm 16.4.

![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310092147151.png)

At the end of Algorithm 16.4, HSSA form is complete, and every value in the program code is represented by a reference to a node in the hash table.

The purpose of the preorder traversal processing order in Algorithm 16.4 is not strictly required to ensure the correctness of the HSSA, because we already have SSA version information for the scalar and virtual variables. Because this processing order ensures that we always visit definitions before their uses, it streamlines the implementation and also makes it easier to perform additional optimizations like copy propagation during the program traversal. It is also possible to go up the use-def chains for virtual variables and analyse the address expressions of their associated indirect memory operations to determine more accurate alias relations among _ivar_ nodes that share the same virtual variable.

In HSSA form, expression trees are converted to directed acyclic graphs (DAGs), which is more memory efficient than ordinary program representation because of node sharing. Many optimizations can run faster on HSSA because they only need to be applied once on the shared use nodes. Optimization implementations can also take advantage of the fact that it is trivial to check if two expressions compute the same value in HSSA.

## 16.6 Further Reading

The earliest attempts at accommodating may-alias information into SSA form are represented by the work of Cytron and Gershbein [88], where they defined may-alias sets of the form $MayAlias(p,\,S)$, which gives the variable names aliased with $*p$ at statement $S$ in the program. Calls to an $IsAlias(p,\,v)$ function are then inserted into the program at points in the program where modifications due to aliasing occur. The $\mathit{IsAlias}(p,v)$ function contains code that models runtime logic and returns appropriate values based on the pointer values. To address the high cost of this representation, Cytron and Gershbein proposed an incremental approach for including may-alias information into SSA form in their work.

The use of assignment factoring to represent MayDefs was first proposed by Choi et al. [68]. The same factoring was referred to as _location factored SSA form_ by Steensgaard [272]. He also used assignment factoring to reduce the number of SSA variables in his SSA form.

The idea of value numbering can be traced to Cocke and Schwartz [77]. While the application of global value numbering (GVN) in this chapter is more for the purpose of representation than optimization, GVN has mostly been discussed in the context of redundancy elimination [76; 249]. Chapter 11 covers redundancy elimination in depth and also contains a discussion of GVN.

HSSA was first proposed by Chow et al. [72] and first implemented in the Open64 compiler [7; 64; 65]. All the SSA-based optimizations in the Open64 compiler were implemented based on HSSA. Their copy propagation and dead store elimination work uniformly on both direct and indirect memory operations. Their redundancy elimination covers indirect memory references as well as other expressions. Other SSA-based optimizations in the Open64 compiler include loop induction variable canonicalization [185], strength reduction [163], and register promotion [187]. Induction variable recognition is discussed in Chap. 10. Strength reduction and register promotion are also discussed in Chap. 11.

The most effective way to optimize indirect memory operations is to promote them to scalars when possible. This optimization is called _indirection removal_. In the Open64 compiler, it depends on the ability of copy propagation to convert the address expressions of indirect memory operations to address constants. The indirect memory operations can then be folded to direct loads and stores. Subsequent register promotion will promote the scalars to registers. If the scalars are free of aliasing, they will not be allocated storage in memory.

Lapkowski and Hendren proposed the use of Extended SSA Numbering to capture the semantics of aliases and pointer accesses [175]. Their SSA number idea borrows from SSA's version numbering, in the sense that a new number is used to annotate the variable whenever it could assume a new value. $\phi$-nodes are not represented, and not all SSA numbers need to have explicit definitions. SSA numbers for pointer references are "extended" to two numbers, the primary one for the pointer variable and the secondary one for the pointed-to memory location. But because it is not SSA form, it does not exhibit many of the nice properties of SSA, like single definitions and built-in use-defs. It cannot benefit from the many SSA-based optimization algorithms either.

The GCC compiler originally uses different representation schemes between unaliased scalar variables and aliased memory-residing objects. To avoid compromising the compiler's optimization when dealing with memory operations, Novillo proposed a unified approach for representing both scalars and arbitrary memory expressions in their SSA form [211]. They do not use Hashed SSA, but their approach to representing aliasing in its SSA form is very similar to ours. They define the virtual operators VDEF and VUSE, which correspond to our $\chi$- and $\mu$-functions. They started out by creating symbol names for any memory-residing program entities. This resulted in an explosion in the number of VDEFs and VUSEs inserted. They then used assignment factoring to cut down the number of these virtual operators, in which memory symbols are partitioned into groups. The virtual operators were then inserted on a per-group basis, thus reducing compilation time. Since the reduced representation precision has a negative effect on optimizations, they provided different partitioning schemes to reduce the impact on optimizations.

One class of indirectly accessed memory objects is array, in which each element is addressed via an index or subscript expression. HSSA distinguishes among different elements of an array only to the extent of determining if the address expressions compute to the same value or not. When two address expressions have the same value, the two indirect memory references are definitely the same. But when the two address expressions cannot be determined to be the same, they may still be the same. Thus, HSSA cannot provide _definitely different_ information. In contrast, the array SSA form can enable more accurate program analysis among accesses to array elements by incorporating the indices into the SSA representation. The array SSA form can capture element-level use-defs, whereas HSSA cannot. In addition, the heap memory storage area can be modelled using abstract arrays that represent disjoint subsets of the heap, with pointers to the heap being treated like indices. Array SSA is covered in detail in Chap. 17. When array references are affine expressions of loop indices, the appropriate representation for dependencies is the so-called _dependence relations_ in the polyhedral model [116].