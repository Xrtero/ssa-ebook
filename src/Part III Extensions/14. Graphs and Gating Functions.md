# Chapter 14. Graphs and Gating Functions
**James Stanier and Fabrice Rastello**

Many compilers represent the input program as some form of graph in order to support analysis and transformation. Over time a cornucopia of program graphs have been presented in the literature and subsequentially implemented in real compilers. Many of these graphs use SSA concepts as the core principle of their representation, ranging from literal translations of SSA into graph form to more abstract graphs which are implicitly in SSA form. We aim to introduce a selection of program graphs that use these SSA concepts, and examine how they may be useful to a compiler writer.

A well-known graph representation is the control-flow graph (CFG) which we encountered at the beginning of the book while being introduced to the core concept of SSA. The CFG models control flow in a program, but the graphs that we will study instead model _data flow_. This is useful as a large number of compiler optimizations are based on data-flow analysis. In fact, all graphs that we consider in this chapter are all data-flow graphs.

In this chapter, we will look at a number of SSA-based graph representations. An introduction to each graph will be given, along with diagrams to show how sample programs look when translated into that particular graph. Additionally, we will describe the techniques that each graph was created to solve, with references to the literature for further research.

For this chapter, we assume that the reader already has familiarity with SSA (see Chap. 1) and the applications that it is used for.

## 14.1 Data-Flow Graphs

Since all of the graphs in this chapter are data-flow graphs, let us define them. A data-flow graph (DFG) is a directed graph $G=(V,\,E)$ where the edges $E$ represent the flow of data from the result of one instruction to the input of another. An instruction executes once all of its input data values have been computed. When an instruction executes, it produces a new data value which is propagated to other connected instructions.

Whereas the CFG imposes a total ordering on instructions--the same ordering that the programmer wrote them in--the DFG has no such concept of ordering; it just models the flow of data. This means that it typically needs a companion representation such as the CFG to ensure that optimized programs are still correct. However, with access to both the CFG and DFG, optimizations such as dead code elimination, constant folding, and common subexpression elimination can be performed effectively. But this comes at a price: keeping both graphs updated during optimization can be costly and complicated.

## 14.2 The SSA Graph

We begin our exploration with a graph that is a literal representation of SSA: the SSA graph. The SSA graph can be constructed from a program in SSA form by explicitly adding use-def chains. To demonstrate what the graph looks like, we present some sample code in Fig. 14.1 which is then translated into an SSA graph.

An SSA graph consists of vertices that represent instructions (such as + and print) or $\phi$-functions, and directed edges that connect uses to definitions of values. The outgoing edges of a vertex represent the arguments required for that instruction, and the ingoing edge(s) to a vertex represent the propagation of the instruction's result(s) after they have been computed. We call these types of graphs _demand-based_ representations. This is because in order to compute an instruction, we must first _demand_ the results of the operands.

Although the textual representation of SSA is much easier for a human to read, the primary benefit of representing the input program in graph form is that the compiler writer is able to apply a wide array of graph-based optimizations by using standard graph traversal and transformation techniques.

In the literature, the SSA graph has been used to detect induction variables in loops, for performing instruction selection (see Chap. 19), operator strength reduction, rematerialization, and has been combined with an extended SSA language to support compilation in a parallelizing compiler. The reader should note that the exact specification of what constitutes an SSA graph changes from paper to paper. The essence of the intermediate representation (IR) has been presented here, as each author tends to make small modifications for their particular implementation.

![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310092220482.png)
### 14.2.1 Finding Induction Variables with the SSA Graph

We illustrate the usefulness of the SSA graph through a basic induction variable (IV) recognition technique. A more sophisticated technique is developed in Chap. 10. Given that a program is represented as an SSA graph, the task of finding induction variables is simplified. A _basic linear induction variable_$i$ is a variable that appears only in the form:
![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310092220040.png)
where $k$ is a constant or loop-invariant. A simple IV recognition algorithm is based on the observation that each basic linear induction variable will belong to a non-trivial strongly connected component (SCC) in the SSA graph. SCCs can be easily discovered in linear time using any depth-first search traversal. Each such SCC must conform to the following constraints:

* The SCC contains only one $\phi$-function at the header of the loop.
* Every component is either $i=\phi(i_{0},\,...,\,i_{n})$ or $i=i_{k}\oplus n$, where $\oplus$ is addition or subtraction, and $n$ is loop-invariant.

This technique can be expanded to detect a variety of other classes of induction variables, such as wraparound variables, non-linear induction variables, and nested induction variables. Scans and reductions also show a similar SSA graph pattern and can be detected using the same approach.

## 14.3 Program Dependence Graph

The program dependence graph (PDG) represents both control and data dependencies together in one graph. The PDG was developed to support optimizations requiring reordering of instructions and graph rewriting for parallelism, as the strict ordering of the CFG is relaxed and complemented by the presence of data dependence information. The PDG is a directed graph $G=(V,\,E)$ where nodes $V$ are statements, predicate expressions, or region nodes, and edges $E$ represent either control or data dependencies. Thus, the set of all edges $E$ has two distinct subsets: the control dependence subgraph $E_{C}$ and the data dependence subgraph $E_{D}$.

Statement nodes represent instructions in the program. Predicate nodes test a conditional statement and have true and false edges to represent the choice taken on evaluation of the predicate. Region nodes group control dependencies with identical source and label together. If the control dependence for a region node is satisfied, then it follows that all of its children can be executed. Thus, if a region node has three different control-independent statements as immediate children, then those statements could potentially be executed in parallel. Diagrammatically, rectangular nodes represent statements, diamond nodes predicates, and circular nodes are region nodes. Dashed edges represent control dependence, and solid edges represent data dependence. Loops in the PDG are represented by back edges in the control dependence subgraph. We show example code translated into a PDG in Fig. 14.2.
Building a PDG is a multi-stage process involving:
* Construction of the post-dominator tree
* Use of the post-dominator tree to generate the control dependence subgraph
* Insertion of region nodes
* Construction of DAGs for each basic block which are then joined to create the data dependence subgraph

Let us explore this construction process in more detail.
An ENTRY node is added with one edge labeled true pointing to the CFG entry node, and another labeled false going to the CFG exit node.

![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310092221767.png)

Before constructing the rest of the control dependence subgraph $E_{C}$, let us define control dependence. A node $w$ is said to be control dependent on edge $(u,\,v)$ if $w$ post-dominates $v$ and $w$ does not strictly post-dominate $u$. Control dependence between nodes is equivalent to the post-dominance frontier on the reversed CFG. To compute the control dependence subgraph, the post-dominator tree is constructed for the CFG. Then, the control dependence edges from $u$ to $w$ are labeled with the boolean value taken by the predicate computed in $u$ when branching on edge $(u,\,v)$. Then, let $S$ consist of the set of all edges $(A,\,B)$ in the CFG such that $B$ is not an ancestor of $A$ in the post-dominator tree. Each of these edges has an associated label true or false. Then, each edge in $S$ is considered in turn. Given $(A,\,B)$, the post-dominator tree is traversed backwards from $B$ until we reach $A$'s parent, marking all nodes visited (including $B$) as control dependent on $A$ with the label of $S$.

Next, region nodes are added to the PDG. Each region node summarizes a set of control conditions and "groups" all nodes with the same set of control conditions together. Region nodes are also inserted so that predicate nodes will only have two direct successors. To begin with, an unpruned PDG is created by checking, for each node of the CFG, which control region it depends on. This is done by traversing the post-dominator tree in post-order and mapping sets of control dependencies to region nodes. For each node $N$ visited in the post-dominator tree, the map is checked for an existing region node with the same set $CD$ of control dependencies. If none exists, a new region node $R$ is created with these control dependencies and entered into the map. $R$ is made to be the only control dependence direct predecessor of $N$. Next, the intersection $INT$ of $CD$ is computed for each immediate child of $N$ in the post-dominator tree. If $INT=CD$, then the corresponding dependencies are removed from the child and replaced with a single dependence on the child's control direct predecessor. Then, a pass over the graph is made to make sure that each predicate node has a unique direct successor for each boolean value. If more than one exists, the corresponding edges are replaced by a single edge to a freshly created region node that itself points to the direct successor nodes.

Finally, the data dependence subgraph is generated. This begins with the construction of DAGs for each basic block where each upwards reaching leaf is called a _merge node_. Data-flow analysis is used to compute reaching definitions. All individual DAGs are then connected together: edges are added from definition nodes to the corresponding merge nodes that may be reached. The resulting graph is the data dependence subgraph, and PDG construction is complete.

The PDG has been used for generating code for parallel architectures and has also been used in order to perform accurate program slicing and testing.

### 14.3.1 Detecting Parallelism with the PDG

An instruction scheduling algorithm running on a CFG lacks the necessary data-flow information to make decisions about parallelization. It requires additional code transformations such as loop unrolling or if-conversion (see Chap. 20) in order to expose any instruction-level parallelism.

However, the structure of the PDG can give the instruction scheduler this information for free. Any node of a CFG loop that is not contained in a strongly connected component of the PDG (using _both_ control and data dependence edges) can be parallelized.

In the example in Fig. 14.2, since the instruction A[i]=a in the loop does not form a strongly connected component in the PDG, it can be vectorized provided that variable $a$ has a private scope. On the other hand, because of the circuit involving the test on $a$, the instruction a=2*B[i] cannot.

## 14.4 Gating Functions and GSA

In SSA form, $\phi$-functions are used to identify points where variable definitions converge. However, they cannot be directly _interpreted_, as they do not specify the condition that determines which of the variable definitions to choose. By this logic,we cannot directly interpret the SSA graph. Being able to interpret our IR is a useful property as it gives the compiler writer more information when implementing optimizations and also reduces the complexity of performing code generation. Gated single assignment form (GSA--sometimes called gated SSA) is an extension of SSA with _gating functions_. These gating functions are directly interpretable versions of $\phi$-nodes and replace $\phi$-nodes in the representation. We usually distinguish the three following forms of gating functions:

* The $\phi_{if}$ function explicitly represents the condition that determines which $\phi$ value to select. A $\phi_{if}$ function of the form $\phi_{if}(p,\,v_{1},\,v_{2})$ has $p$ as a predicate, and $v_{1}$ and $v_{2}$ as the values to be selected if the predicate evaluates to true or false, respectively. This can be read simply as _if-then-else_.
* The $\phi_{entry}$ function is inserted at loop headers to select the initial and loop carried values. A $\phi_{entry}$ function of the form $\phi_{entry}(v_{init},\,v_{iter})$, has $v_{init}$ as the initial input value for the loop, and $v_{iter}$ as the iterative input. We replace $\phi$-functions at loop headers with $\phi_{entry}$ functions.
* The $\phi_{exit}$ function determines the value of a variable when a loop terminates. A $\phi_{exit}$ function of the form $\phi_{exit}(p,\,v_{exit})$ has $P$ as predicate and $v_{exit}$ as the definition reaching beyond the loop.

It is easiest to understand these gating functions by means of an example. Figure 14.3 shows how our earlier code in Fig. 14.2 translates into GSA form. Here,
![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310092222113.png)
![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310092222760.png)

> Figure 14: **(a)** A structured code; **(b)** the PDG (with region nodes omitted); **(c)** the DAG representation of the nested gated $\phi_{if}$

we can see the use of both $\phi_{entry}$ and $\phi_{exit}$ gating functions. At the header of our sample loop, the $\phi$-function has been replaced by a $\phi_{entry}$ function which determines between the initial and iterative value of $i$. After the loop has finished executing, the nested $\phi_{exit}$ function selects the correct live-out version of $a$.

This example shows several interesting points. First, the semantics of both the $\phi_{exit}$ and $\phi_{if}$ are strict in their gate: here $a_{1}$ or $\phi_{exit}(q,a_{2})$ is not evaluated before $p$ is known.[^1] Similarly, a $\phi_{if}$ function that results from the nested if-then-else code of Fig. 14 would be itself nested as $a=\phi_{if}(p,\phi_{if}(q,a_{2},a_{3}),a_{1})$. Second, this representation of the program does not allow for an interpreter to decide whether an instruction with a side effect (such as $A[i_{1}]=a_{2}$ in our running example) has to be executed or not. Finally, computing the values of gates is highly related to the simplification of path expressions: in our running example, $a_{2}$ should be selected when the path $\neg p$ followed by $q$ (denoted $\neg p.q$) is taken, while $a_{1}$ should be selected when the path $p$ is taken; for our nested if-then-else example, $a_{1}$ should be selected either when the path $\neg p.r$ is taken or when the path $\neg p.\neg r$ is taken, which simplifies to $\neg p$. Diverse approaches can be used to generate the correct nested $\phi_{if}$ or $\phi_{exit}$ gating functions.

[^1]: As opposed to the $\psi$–function described in Chap. 15 that would use syntax such as $a_{3}=\phi((p\land\neg q)?a_{1},(\neg p\land q)?a_{2})$ instead.

The most natural way uses a data-flow analysis that computes, for each program point and each variable, its unique reaching definition and the associated set of reaching paths. This set of paths is abstracted using a _path expression_. If the code is not already under SSA, and if at a merge point of the CFG, its direct predecessor basic blocks are reached by different variables, a $\phi$-function is inserted. The gate of each operand is set to the path expression of its corresponding incoming edge. If a unique variable reaches all the direct predecessor basic blocks, the corresponding path expressions are merged. Of course, a classical path compression technique can be used to minimize the number of visited edges. One can observe the similarities with the $\phi$-function placement algorithm described in Sect. 4.4.

There also exists a relationship between the control dependencies and the gates: from a code already under strict and conventional SSA form, one can derive the gates of a $\phi_{\mathit{if}}$ function from the control dependencies of its operands. This relationship is illustrated by Fig. 14.4 in the simple case of a structured code.

These gating functions are important as the concept will form components of the value state dependence graph later. GSA has seen a number of uses in the literature including analysis and transformations based on data flow. With the diversity of applications (see Chaps. 10 and 23), many variants of GSA have been proposed. Those variations concern the correct handling of loops in addition to the computation and representation of gates.

By using gating functions, it becomes possible to construct IRs based solely on data dependencies. These IRs are sparse in nature compared to the CFG, making them good for analysis and transformation. This is also a more attractive proposition than generating and maintaining both a CFG and DFG, which can be complex and prone to human error. One approach has been to combine both of these into one representation, as is done in the PDG. Alternatively, we can utilize gating functions along with a data-flow graph for an effective way of representing whole program information using data-flow information.

### 14.4.1 Backward Symbolic Analysis with GSA

GSA is useful for performing symbolic analysis. Traditionally, symbolic analysis is performed by forward propagation of expressions through a program. However, complete forward substitution is expensive and can result in a large quantity of unused information and complicated expressions. Instead, _backward_, demand-driven substitutions can be performed using GSA which only substitutes _needed_ information. Consider the following program:

![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310092224402.png)

If forward substitutions were to be used in order to determine whether the assertion is correct, then the symbolic value of $J$ must be discovered, starting at the top of the program in statement at line 1. Forward propagation through this program results in statement at line 6 being
![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310092224490.png)
and thus the **assert** () statement evaluates to true. In real, non-trivial programs, these expressions can get unnecessarily long and complicated.

Using GSA instead allows for backwards, demand-driven substitutions. The program above has the following GSA form:

![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310092224156.png)


Using this backward substitution technique, we start at statement on line 7 and follow the SSA links of the variables from $J_{3}$. This allows for skipping of any intermediate statements that do not affect variables in this statement. Thus, the substitution steps are
![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310092224056.png)
The backward substitution then stops because enough information has been found, avoiding the redundant substitution of $JMAX_{1}$ by $EXPR$. In non-trivial programs, this can greatly reduce the number of redundant substitutions, making symbolic analysis significantly cheaper.

## 14.5 Value State Dependence Graph

The gating functions defined in the previous section were used in the development of a sparse data-flow graph IR called the value state dependence graph (VSDG). The VSDG is a directed graph consisting of operation nodes, loop, and merge nodes together with value and state dependency edges. Cycles are permitted but must satisfy various restrictions. A VSDG represents a single procedure: this matches the classical CFG. An example VSDG is shown in Fig. 14.5.

### 14.5.1 Definition of the VSDG

A VSDG is a labeled directed graph $G=(N,\,E_{V},\,E_{S},\,\ell,\,N_{0},\,N_{\infty})$ consisting of nodes $N$ (with unique entry node $N_{0}$ and exit node $N_{\infty}$), value dependency edges $E_{V}\subseteq N\times N$, and state dependency edges $E_{S}\subseteq N\times N$. The labelling function $\ell$ associates each node with an operator.
![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310092225838.png)

> Fig 14.5: A recursive factorial function, whose VSDG illustrates the key graph components—value dependency edges (solid lines), state dependency edges (dashed lines), a const node, a call node, a $\gamma$-node, a conditional node, and the function entry and exit nodes


The VSDG corresponds to a reducible program, e.g., there are no cycles in the VSDG except those mediated by $\theta$-nodes (loop).

Value dependency ($E_{V}$) indicates the flow of values between nodes. State dependency ($E_{S}$) represents two things; the first is essentially a sequential dependency required by the original program, e.g., a given load instruction may be required to follow a given store instruction without being re-ordered, and a return node in general must wait for an earlier loop to terminate even though there might be no value dependency between the loop and the return node. The second purpose is that state dependency edges can be added incrementally until the VSDG corresponds to a unique CFG. Such state dependency edges are called _serializing_ edges.

The VSDG is implicitly represented in SSA form: a given operator node, $n$, will have zero or more $E_{V}$-consumers using its value. Note that, in implementation terms, a single register can hold the produced value for consumption at all consumers; it is therefore useful to talk about the idea of an output _port_ for $n$ being allocated a specific register, $r$, to abbreviate the idea of $r$ being used for each edge ($n_{1},n_{2}$), where $n_{2}\in\text{directsucc}(n_{1})$.

There are four main classes of VSDG nodes: value nodes (representing pure arithmetic),$\gamma$-nodes (conditionals), $\theta$-nodes (loops), and state nodes (side effects). The majority of nodes in a VSDG generates a value based on some computation (add, subtract, etc.) applied to their dependent values (constant nodes, which have no dependent nodes, are a special case).

#### $\gamma$-Nodes

The $\gamma$-node is similar to the $\phi_{if}$ gating function in being dependent on a control predicate, rather than the control-independent nature of SSA $\phi$-functions. A $\gamma$-node $\gamma(C:p,\ T:v_{\textsf{true}},\ F:v_{\textsf{false}})$ evaluates the condition dependency $p$ and returns the value of $v_{\textsf{true}}$ if $p$ is true, otherwise $v_{\textsf{false}}$. We generally treat $\gamma$-nodes as single-valued nodes (contrast $\theta$-nodes, which are treated as tuples), with the effect that two separate $\gamma$-nodes with the same condition can be later combined into a tuple using a single test. Figure 14 illustrates two $\gamma$-nodes that can be combined in this way. Here, we use a pair of values (2-tuple) for prots $T$ and $F$. We also see how two syntactically different programs can map to the same structure in the VSDG.

#### $\theta$-Nodes

The $\theta$-node models the iterative behaviour of loops, modelling loop state with the notion of an _internal value_ which may be updated on each iteration of the loop. It has five specific elements that represent dependencies at various stages of computation.

![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310092226446.png)

> Figure 14.6: Two different code schemes map to the same $\gamma$-node structure


![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310092226138.png)

> Figure 14.7: An example showing a for loop. Evaluating $\mathbf{X}$ triggers it to evaluate the $\mathbf{I}$ value (outputting the value $\mathbf{L}$). While $\mathbf{C}$ evaluates to true, it evaluates the $\mathbf{R}$ value (which in this case also uses the $\theta$-node’s $\mathbf{L}$ value). When $\mathbf{C}$ is false, it returns the final internal value through $\mathbf{X}$. As $\mathbf{i}$ is not used after the loop, there is no dependency on $\mathbf{i}$ at $\mathbf{X}$


The $\theta$-node corresponds to a merge of the $\phi_{entry}$ and $\phi_{exit}$ nodes in gated SSA. A $\theta$-node $\theta(C:p,\ I:v_{init},\ R:v_{return},\ L:v_{iter},\ X:v_{exit})$ sets its internal value to initial value $v_{init}$ and then, while condition value $p$ holds true, sets $v_{iter}$ to the current internal value and updates the internal value with the repeat value $v_{return}$. When $p$ evaluates to false, computation ceases, and the last internal value is returned through $v_{exit}$.

A loop that updates $k$ variables will have: a single condition $p$, initial values $v^{1}_{init}$,..., $v^{k}_{init}$, loop iterations $v^{1}_{iter}$,..., $v^{k}_{iter}$, loop returns $v_{return^{1}}$,..., $v^{k}_{return}$, and loop exits $v^{1}_{exit}$,..., $v^{k}_{exit}$. The example in Fig. 14.7 also shows a pair (2-tuple) of values being used on ports $I$, $R$, $L$, $X$, one for each loop-variant value.

The $\theta$-node directly implements pre-test loops (while, for); post-test loops (do...while, repeat...until) are synthesized from a pre-test loop preceded by a duplicate of the loop body. At first, this may seem to cause unnecessary duplication of code, but it has two important benefits: (1) it exposes the first loop body iteration to optimization in post-test loops (cf. loop-peeling) and (2) it normalizes all loops to one loop structure, which both reduces the cost of optimization and increases the likelihood of two schematically dissimilar loops being isomorphic in the VSDG.

#### State Nodes

Loads and stores compute a value and state. The call node takes both the name of the function to call and a list of arguments and returns a list of results; it is treated as a state node as the function body may read or update state.

We maintain the simplicity of the VSDG by imposing the restriction that _all_ functions have _one_ return node (the exit node $N_{\infty}$), which returns at least one result



(which will be a state value in the case of void functions). To ensure that function calls and definitions are able to be allocated registers easily, we suppose that the number of arguments to, and results from, a function is smaller than the number of physical registers--further arguments can be passed via a stack as usual.

Note also that the VSDG does not force loop-invariant code into or out of loop bodies, but rather allows later phases to determine, by adding serializing edges, such placement of loop-invariant nodes for later phases.

#### 14.5.2 Dead Node Elimination with the VSDG

By representing a program as a VSDG, many optimizations become trivial. For example, consider dead node elimination (Fig. 14.1). This combines both dead code elimination and unreachable code elimination. Dead code generates VSDG nodes for which there is no value or state dependency path from the return node, i.e., the result of the function does not in any way depend on the results of the dead nodes. Unreachable code generates VSDG nodes that are either dead or become dead after some other optimization. Thus, a _dead node_ is a node that is not post-dominated by the exit node $N_{\infty}$. To perform dead node elimination, only two passes are required over the VSDG resulting in linear runtime complexity: one pass to identify all of the live nodes, and a second pass to delete the unmarked (i.e., dead) nodes. It is safe because all nodes that are deleted are guaranteed never to be reachable from the return node.

![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310092227335.png)
## 14.6 Further Reading![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310092227335.png)

A compiler's intermediate representation can be a graph, and many different graphs exist in the literature. We can represent the control flow of a program as a control-flow graph (CFG) [3], where straight-line instructions are contained within basic blocks and edges show where the flow of control may be transferred to once leaving that block. A CFG is traditionally used to convert a program into SSA form [90]. We can also represent programs as a type of data-flow graph (DFG) [96, 97], and SSA can be represented in this way as an SSA graph [84]. An example was given that used the SSA graph to detect a variety of induction variables in loops [127, 306]. It has also been used for performing instruction selection techniques [108, 254], operator strength reduction [84], and rematerialization [48] and has been combined with an extended SSA language to aid compilation in a parallelizing compiler [274].

The program dependence graph (PDG) as defined by Ferrante et al. [118] represents control and data dependencies in one graph. We choose to report the definition of Bilardi and Pingali [29]. Section 14.3 mentions possible abstractions to represent data dependencies for dynamically allocated objects. Among others, the book of Darte et al. [93] provides a good overview of such representations. The PDG has been used for program slicing [214], testing [20] and widely for parallelization [21, 117, 119, 259].

Gating functions can be used to create directly interpretable $\phi$-functions. These are used in gated single assignment form. Alpern et al. [6] presented a precursor of GSA for structured code, to detect equality of variables. This chapter adopts their notations, i.e., a $\phi_{\mathit{if}}$ for an if-then-else construction, a $\phi_{\mathit{entry}}$ for the entry of a loop, and a $\phi_{\mathit{exit}}$ for its exit. The original usage of GSA was by Ballance et al. [215] as an intermediate stage in the construction of the Program Dependence Web IR. Further, GSA papers replaced $\phi_{\mathit{if}}$ by $\gamma$, $\phi_{\mathit{entry}}$ by $\mu$, and $\phi_{\mathit{exit}}$ by $\eta$. Havlak [141] presented an algorithm for construction of a simpler version of GSA--Thinned GSA--which is constructed from a CFG in SSA form. The construction technique sketched in this chapter is developed in more detail in [289]. GSA has been used for a number of analyses and transformations based on data flow. The example given of how to perform backward demand-driven symbolic analysis using GSA has been borrowed from [290]. If-conversion (see Chap. 20) converts control dependencies into data dependencies. To avoid the potential loss of information related to the lowering of $\phi$-functions into conditional moves or select instructions, gating $\psi$-functions (see Chap. 15) can be used.

We then described the value state dependence graph (VSDG) [151], which is an improvement on a previous, unmentioned graph, the Value Dependence Graph [304]. It uses the concept of gating functions, data dependencies, and state to model a program. We gave an example of how to perform dead node elimination on the VSDG. Detailed semantics of the VSDG are available [151], as well as semantics of a related IR: the gated data dependence graph [292]. Further study has taken place on the problem of generating code from the VSDG [269, 178, 291], and it has also been used to perform a combined register allocation and code motion algorithm [152].