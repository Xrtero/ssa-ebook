# Chapter 12. Introduction
**Vivek Sarkar and Fabrice Rastello**

So far, we have introduced the foundations of SSA form and its use in different program analyses. We now explain the need for extensions to SSA form to enable a larger class of program analyses. The extensions arise from the fact that many analyses need to make finer-grained distinctions between program points and data accesses than what can be achieved by vanilla SSA form. However, these richer flavours of extended SSA-based analyses still retain many of the benefits of SSA form (e.g., sparse data-flow propagation) which distinguish them from classical data-flow frameworks for the same analysis problems.

## 12.1 Static Single Information Form

The sparseness in vanilla SSA form arises from the observation that information for an unaliased scalar variable can be safely propagated from its (unique) definition to all its reaching uses without examining any intervening program points. As an example, SSA-based constant propagation aims to compute for each single assignment variable, the (usually over-approximated) set of possible values carried by the definition of that variable. For instance, consider an instruction that defines a variable $a$ and uses two variables, $b$ and $c$. An example is $a=b+c$. In an SSA-form program, constant propagation will determine if $a$ is a constant by looking directly at the definition point of $b$ and at the definition point of $c$. We say that informationis propagated from these two definition points directly to the instruction $a=b+c$. However, there are many analyses for which definition points are not the only source of new information. For example, consider an if-then-else statement that uses $a$ in both the then and else parts. If the branch condition involves $a$, say $a$ == 0, we now have additional information that can distinguish the value of $a$ in the then and else parts, even though both uses have the same reaching definition. Likewise, the use of a reference variable $p$ in certain contexts can be the source of new information for subsequent uses, e.g., the fact that $p$ is non-null because no exception was thrown at the first use.

The goal of Chap. 13 is to present a systematic approach to deal with these additional sources of information while still retaining the space and time benefits of vanilla SSA form. This approach is called static single information (SSI) form, and it involves additional renaming of variables to capture new sources of information. For example, SSI form can provide distinct names for the two uses of $a$ in the then and else parts of the if-then-else statement mentioned above. This additional renaming is also referred to as _live range splitting_, akin to the idea behind splitting live ranges in optimized register allocation. The sparseness of SSI form follows from formalization of the Partitioned Variable Lattice (PVL) problem and the Partitioned Variable Problem (PVP), both of which establish orthogonality among transfer functions for renamed variables. The $\phi$-functions inserted at join nodes in vanilla SSA form are complemented by $\sigma$-functions in SSI form that can perform additional renaming at branch nodes and interior (instruction) nodes. Information can be propagated in the forward and backward directions using SSI form, enabling analyses such as range analysis (leveraging information from branch conditions) and null-pointer analysis (leveraging information from prior uses) to be performed more precisely than with vanilla SSA form.

## 12.2 Control Dependencies

So as to expose parallelism and locality, one needs to get rid of the CFG at some points. For loop transformations, software pipelining, there is a need to manipulate a higher degree of abstraction to represent the iteration space of nested loops and to extend data-flow information to this abstraction. One can expose even more parallelism (at the level of instructions) by replacing control flow by control dependencies: the goal is either to express a predicate expression under which a given basic block is to be executed or to select afterwards (using similar predicate expressions) the correct value among a set of eagerly computed ones.

1. Technically, we say that SSA provides data flow (data dependencies). The goal is to enrich it with control dependencies. The program dependence graph (PDG) constitutes the basis of such IR extensions. Gated single assignment (gated SSA, GSA) mentioned below provides an interpretable (data- or demand-driven) IRthat uses this concept. Psi-SSA ($\psi$-SSA) also mentioned below is a very similar IR but more appropriate to code generation for architectures with predication.
2. Note that such extensions sometimes face difficulties handling loops correctly (need to avoid deadlock between the loop predicate and the computation of the loop body, replicate the behaviour of infinite loops, etc.). However, we believe that, as we will illustrate further, loop carried control dependencies complicate the recognition of possible loop transformations: it is usually better to represent loops and their corresponding iteration space using a dedicated abstraction.

## 12.3 Gated SSA Forms

As already mentioned, one of the strengths of SSA form is its associated data-flow graph (DFG), the SSA graph that is used to propagate directly the information along the def-use chains. This is what makes data-flow analysis sparse. By combining the SSA graph with the control-flow graph, static analysis can be made context-sensitive. This can be done in a more natural and powerful way by incorporating in a unified representation both the data-flow and control-flow information.

The program dependence graph (PDG) adds to the data dependence edges (SSA graph as the data dependence graph--DDG) the control dependence edges (control dependence graph--CDG). As already mentioned, one of the main motivations for the development of the PDG was to aid automatic parallelization of instructions across multiple basic blocks. However, in practice, it also exposes the relationship between the control predicates and their related control-dependent instructions, thus allowing us to propagate the associated information. A natural way to represent this relationship is through the use of gating functions that are used in some extensions such as the gated SSA (GSA) or the value state dependence graph (VSDG). Gating functions are directly interpretable versions of $\phi$-nodes. As an example, $\phi_{\mathit{if}}(\mathit{P},\mathit{v}_{1},\mathit{v}_{2})$ can be interpreted as a function that selects the value $\mathit{v}_{1}$ if predicated $\mathit{P}$ evaluates to true and the value $\mathit{v}_{2}$ otherwise. PDG, GSA, and VSDG are described in Chap. 14.

## 12.4 Psi-SSA Form

$\psi$-SSA form (Chap. 15) addresses the need for modelling Static Single Assignment form in predicated operations. A predicated operation is an alternate representation of a fine-grained control-flow structure, often obtained by using the well-known _if-conversion_ transformation (see Chap. 20). A key advantage of using predicated operations in a compiler's intermediate representation is that it can often enable more optimizations by creating larger basic blocks compared to approaches in which predicated operations are modelled as explicit control-flow graphs. From an SSA-form perspective, the challenge is that a predicated operation may or may not update its definition operand, depending on the value of the predicate guarding that assignment. This challenge is addressed in $\psi$-SSA form by introducing $\psi$-functions that perform merge functions for predicated operations, analogous to the merge performed by $\phi$-functions at join points in vanilla SSA form.

In general, a $\psi$-function has the form, $a_{0}=\psi$ ($p_{1}?a_{1}$,..., $p_{i}?a_{i}$,..., $p_{n}?a_{n}$), where each input argument $a_{i}$ is associated with a predicate $p_{i}$ as in a nested if-then-else expression for which the value returned is the rightmost argument whose predicate is true. Observe that the semantic of a $\psi$-function imposes the logical disjunction of its predicates to evaluate to true. A number of algebraic transformations can be performed on $\psi$-functions, including $\psi$-inlining, $\psi$-reduction, $\psi$-projection, $\psi$-permutation, and $\psi$-promotion. Chapter 15 also includes an algorithm for transforming a program out of Psi-SSA form that extends the standard algorithm for destruction of vanilla SSA form.

## 12.5 Hashed SSA Form

The motivation for SSI form arose from the need to perform additional renaming to distinguish among different uses of an SSA variable. Hashed SSA (HSSA) form introduced in Chap. 16 addresses another important requirement, viz., the need to model aliasing among variables. For example, a static use or definition of indirect memory access $*p$ in the C language could represent the use or definition of multiple local variables whose addresses can be taken and may potentially be assigned to $p$ along different control-flow paths. To represent aliasing of local variables, HSSA form extends vanilla SSA form with _MayUse_ ($\mu$) and _MayDef_ ($\chi$) functions to capture the fact that a single static use or definition could potentially impact multiple variables. Note that MayDef functions can result in the creation of new names (versions) of variables, compared to vanilla SSA form. HSSA form does not take a position on the accuracy of alias analysis that it represents. It is capable of representing the output of any alias analysis performed as a pre-pass to HSSA construction. As summarized above, a major concern with HSSA form is that its size could be quadratic in the size of the vanilla SSA form, since each use or definition can now be augmented by a set of MayUse's and MayDef's, respectively. A heuristic approach to dealing with this problem is to group together all variable versions that have no "real" occurrence in the program, i.e., do not appear in a real instruction outside of a $\phi$, $\mu$, or $\chi$ function. These versions are grouped together into a single version called the _zero version_ of the variable.

In addition to aliasing of locals, it is important to handle the possibility of aliasing among heap-allocated variables. For example, $*p$ and $*q$ may refer to the same location in the heap, even if no aliasing occurs among local variables. HSSA form addresses this possibility by introducing a _virtual variable_ for each address expression used in an indirect memory operation and renaming virtual variables with $\phi$-functions as in SSA form. Further, the alias analysis pre-pass is expected to provide information on which virtual variables may potentially be aliased, thereby leading to the insertion of $\mu$ or $\chi$ functions for virtual variables as well. Global value numbering is used to increase the effectiveness of the virtual variable approach, since all indirect memory accesses with the same address expression can be merged into a single virtual variable (with SSA renaming as usual). In fact, the Hashed SSA name in HSSA form comes from the use of hashing in most value-numbering algorithms.

## 12.6 Array SSA Form

In contrast to HSSA form, Array SSA form (Chap. 17) takes an alternate approach to modelling aliasing of indirect memory operations by focusing on aliasing in arrays as its foundation. The aliasing problem for arrays is manifest in the fact that accesses to elements $A[i]$ and $A[j]$ of array $A$ refer to the same location when $i=j$. This aliasing can occur with just local array variables, even in the absence of pointers and heap-allocated data structures. Consider a program with a definition of $A[i]$ followed by a definition of $A[j]$. The vanilla SSA approach can be used to rename these two definitions to (say) $A_{1}[i]$ and $A_{2}[j]$. The challenge with arrays arises when there is a subsequent use of $A[k]$. For scalar variables, the reaching definition for this use can be uniquely identified in vanilla SSA form. However, for array variables, the reaching definition depends on the subscript values. In this example, the reaching definition for $A[k]$ will be $A_{2}$ or $A_{1}$ if $k==j$ or $k==i$ (or a prior definition $A_{0}$ if $k\neq j$ and $k\neq i$). To provide $A[k]$ with a single reaching definition, Array SSA form introduces a definition-$\Phi$ ($d\Phi$) operator that represents the merge of $A_{2}[j]$ with the prevailing value of array $A$ prior to $A_{2}$. The result of this $d\Phi$ operator is given a new name, $A_{3}$ (say), which serves as the single definition that reaches use $A[k]$ (which can then be renamed to $A_{3}[k]$). This extension enables sparse data-flow propagation algorithms developed for vanilla SSA form to be applied to array variables, as illustrated by the algorithm for _sparse constant propagation of array elements_ presented in this chapter. The accuracy of analyses for Array SSA form depends on the accuracy with which pairs of array subscripts can be recognized as being _definitely same_ ($\mathcal{DS}$) or _definitely different_ ($\mathcal{DS}$).

To model heap-allocated objects, Array SSA form builds on the observation that all indirect memory operations can be modelled as accesses to elements of abstract arrays that represent disjoint subsets of the heap. For modern object-oriented languages such as Java, type information can be used to obtain a partitioning of the heap into disjoint subsets, e.g., instances of field $x$ are guaranteed to be disjoint from instances of field $y$. In such cases, the set of instances of field $x$ can be modelled as a logical array (map) $\mathcal{H}^{x}$ that is indexed by the object reference (key). The problem of resolving aliases among field accesses $p\!\cdot\!x$ and $q\!\cdot\!x$ then becomes equivalent to the problem of resolving aliases among array accesses $\mathcal{H}^{x}[p]$ and $\mathcal{H}^{x}[q]$, thereby enabling Array SSA form to be used for analysis of objects as in the algorithm for _redundant load elimination_ among object fields presented in this chapter. For weakly typed languages such as C, the entire heap can be modelled as a single heap array. As in HSSA form, an alias analysis pre-pass can be performed to improve the accuracy of _definitely same_ and _definitely different_ information. In particular, global value numbering is used to establish _definitely same_ relationships, analogous to its use in establishing equality of address expressions in HSSA form. In contrast to HSSA form, the size of Array SSA form is guaranteed to be linearly proportional to the size of a comparable scalar SSA form for the original program (if all array variables were treated as scalars for the purpose of the size comparison).

## 12.7 Memory-Based Data Flow

SSA provides data flow/dependencies between _scalar_ variables. Execution order of side effect instructions must also be respected. Indirect memory access can be considered very conservatively as such and lead to (sometimes called state, see Chap. 14) dependence edges. Too conservative dependencies annihilate the potential of optimizations. Alias analysis is the first step towards more precise dependence information. Representing this information efficiently in the IR is important. One could simply add a dependence (or a flow arc) between two "consecutive" instructions that may or must alias. Then, in the same spirit as SSA $\phi$-nodes aim at combining the information as early as possible (as opposed to standard def-use chains, see the discussion in Chap. 2), similar nodes can be used for memory dependencies. Consider the following sequential C-like code involving pointers, where p and q may alias:
![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310092138140.png)
Without the use of $\phi$-nodes, the amount of def-use chains required to link the assignments to their uses would be quadratic (4 here). Hence, the usefulness of generalizing SSA and its $\phi$-node for scalars to handle memory access for sparse analyses. HSSA (see Chap. 16) and Array-SSA (see the Chap. 17) are two different implementations of this idea. One has to admit that if this early combination is well suited for analysis or interpretation, the introduction of a $\phi$-function might add a control dependence to an instruction that would not exist otherwise. In other words, only simple loop carried dependencies can be expressed this way. Let us illustrate this point using a simple example:
![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310092138487.png)
Here, the computation at iteration indexed by $i+2$ accesses the value computed at iteration indexed by $i$. Suppose we know that $f(x)>x$ and that prior to entering the loop $A[*]\geq 0$. Consider the following SSA-like representation:
![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310092139418.png)
This would easily allow for the propagation of information that $A_{2}\geq 0$. On the other hand, by adding this $\phi$-node, it becomes difficult to devise that iteration $i$ and $i$$+$ 1 can be executed in parallel: the $\phi$-node adds a loop carried dependence. If you are interested in performing loop transformations that are more sophisticated than just exposing fully parallel loops (such as loop interchange, loop tiling, or multidimensional software pipelining), then (dynamic) single assignment forms should be your friend. There exist many formalisms including Kahn process networks (KPN) or Fuzzy Data-flow Analysis (FADA) that implement this idea. But each time restrictions apply. This is part of the huge research area of automatic parallelization outside of the scope of this book. For further details, we refer the reader to the corresponding Encyclopedia of Parallel Computing [216].