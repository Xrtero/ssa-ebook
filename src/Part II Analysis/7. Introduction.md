# Chapter 7. Introduction

**Markus Schordan and Fabrice Rastello**

The objective of Part II is to describe several program analyses and optimizations that benefit from SSA form.

In particular, we illustrate how SSA form makes an analysis more convenient because of its similarity to _functional programs_. Technically, the def-use chains explicitly expressed through the SSA graph, but also the static-single information property, are what make SSA so convenient.

We also illustrate how SSA form can be used to design _sparse_ analyses that are faster and more efficient than their dense counterparts. This is especially important for just-in-time compilation but also for complex inter-procedural ahead-of-time analysis.

Finally, as already mentioned in Chap. 2, SSA form can come in different flavours. The vanilla flavour is strict SSA, or in equivalent terms, SSA form with the dominance property. The most common SSA construction algorithm exploits this dominance property by two means. First, it serves to compute join sets for $\phi$-placement in a very efficient way using the dominance frontier. Second, it allows variable renaming using a folding scheme along the dominance tree. The notion of dominance and dominance frontier are two _structural properties_ that make SSA form singular for compiler analysis and transformations.

The following paragraphs provide a short overview of the chapters that constitute this part.

**Propagating Information Using SSA**

There are several analyses that propagate information through the SSA graph. Chapter 8 gives a general description of the mechanism. It shows how SSA form facilitates the design and implementation of analyses equivalent to traditional data-flow analyses and how the SSA property serves to reduce analysis time and memory consumption. The presented _Propagation Engine_ is an extension of the well-known approach by Wegman and Zadeck [303] for sparse conditional constant propagation. The basic algorithm is not limited to constant propagation and can be used to solve a broad class of data-flow problems more efficiently than the iterative work list algorithm for solving data-flow equations. The basic idea is to directly propagate information computed at the unique definition of a variable to all its uses.

**Liveness**

A data-flow analysis potentially iterates up to as many times as the maximum loop depth of a given program until it stabilizes and terminates. In contrast, the properties of SSA form mean liveness analysis can be accelerated without the requirement of any iteration to reach a fixed point: it only requires at most two passes over the CFG. Also, an extremely simple liveness check is possible by providing a query system to answer questions such as "is variable $v$ live at location $q$?". Chapter 9 shows how the loop nesting forest and dominance property can be exploited to devise very efficient liveness analyses.

**Loop Tree and Induction Variables**

Chapter 10 illustrates how capturing properties of the SSA graph itself (circuits) can be used to determine a very specific subset of program variables: induction variables. The induction variable analysis is based on the detection of self-references in the SSA representation and the extraction of the loop tree, which can be performed on the SSA graph as well. The presented algorithm translates the SSA representation into a representation of polynomial functions, describing the sequence of values that SSA variables hold during the execution of loops. The number of iterations is computed as the minimum solution of a polynomial inequality with integer solutions, also called Diophantine inequality.

**Redundancy Elimination**

The elimination of redundant computations is an important compiler optimization. A computation is _fully redundant_ if it has also occurred earlier regardless of control flow, and a computation is _partially redundant_ if it has occurred earlier only on certain paths. Following the program flow, once we are past the dominance frontiers, any further occurrence of a redundant expression is partially redundant, whereas any occurrence before the dominance frontier is fully redundant. The difference for the optimization is that fully redundant expressions can be deleted, whereas for (strictly) partially redundant expressions insertions are required to eliminate the redundancy. Thus, since partial redundancies start at dominance frontiers they are related to SSA's $\phi$ statements. Chapter 11 shows how the dominance frontier that is used to insert a minimal number of $\phi$-function for SSA construction can also be used to minimize redundant computations. It presents the SSAPRE algorithmand its variant, the speculative PRE, which (possibly speculatively) perform partial redundancy elimination (PRE). It also discusses a direct and very useful application of the presented technique, register promotion via PRE. Unfortunately, the SSAPRE algorithm is not capable of recognizing redundant computations among lexically different expressions that yield the same value. Therefore, redun

dancy elimination based on value analysis (GVN) is also discussed, although a description of the value-based partial redundancy elimination (GVN-PRE) algorithm of VanDrunen [295], which subsumes both PRE and GVN, is missing.

**Alias Analysis**

The book is lacking an extremely important compiler analysis, namely alias analysis. Disambiguating pointers improves the precision and performance of many other analyses and optimizations. To be effective, flow-sensitivity and inter-procedurality are required but, with standard iterative data-flow analysis, lead to serious scalability issues. The main difficulty with making alias analysis sparse is that it is a chicken and egg problem. For each may-alias between two variables, the associated information interferes. Assuming the extreme scenario where any pair of variables is a pair of aliases, one necessarily needs to assume that the modification of one potentially modifies the other. In other words, the def-use chains are completely dense and there is no benefit in using any extended SSA form such as HSSA form (see Chap. 16). The idea is thus to decompose the analysis into phases where sophistication increases with sparsity. This is precisely what the _staged flow-sensitive analysis_ of Hardekopf and Lin [138] achieves with two "stages." The first stage, performing a not-so-precise auxiliary pointer analysis, creates def-use chains used to enable sparsity in the second stage. The second stage, the primary analysis, is then a flow-sensitive pointer analysis. An important aspect is that as long as the auxiliary pointer analysis in the first stage is sound, the primary flow-sensitive analysis in the second stage will also be sound and will be at least as precise as a traditional "non-sparse" flow-sensitive analysis. The sparsity improves the runtime of the analysis, but it does not reduce precision.

Another way to deal with pointers in SSA form is to use a variant of SSA, called partial SSA, which requires variables to be divided into two classes: one class that contains only variables that are never referenced by pointers, and another class containing all those variables that can be referenced by pointers (address-taken variables). To avoid complications involving pointers in SSA form, only variables in the first class are put into SSA form. This technique is used in modern compilers such as GCC and LLVM.