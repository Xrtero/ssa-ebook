# Chapter 10. Loop Tree and Induction Variables

**Sebastian Pop and Albert Cohen**

This chapter presents an extension of SSA whereby the extraction of the reducible loop tree can be done only on the SSA graph itself. This extension also captures reducible loops in the CFG. This chapter first illustrates this property and then shows its usefulness through the problem of induction variable recognition.

## 10.1 Part of the CFG and Loop Tree Can Be Exposed Through the SSA

During the construction of the SSA representation based on a CFG representation, a large part of the CFG information is translated into the SSA representation. As the construction of the SSA has precise rules to place the $\phi$-nodes in special points of the CFG (i.e., at the merge of control-flow branches), by identifying patterns of uses and definitions, it is possible to expose a part of the CFG structure through the SSA representation.

Furthermore, it is possible to identify higher-level constructs inherent to the CFG representation, such as strongly connected components of basic blocks (or reducible loops), based only on the patterns of the SSA definitions and uses. The induction variable analysis presented in this chapter is based on the detection of self-references in the SSA representation and on its characterization.

This first section shows that the classical SSA representation is not sufficient to represent the semantics of the original program. We will see the minimal amount of information that has to be added to the classical SSA representation in order to represent the loop information: similar to the $\phi_{exit}$-function used in the gated SSA presented in Chap. 14, the loop-closed SSA form adds an extra variable at the end of a loop for each variable defined in a loop and used after the loop.

### 10.1.1 _An SSA Representation Without the CFG_

In the classical definition of SSA, the CFG provides the skeleton of the program: basic blocks contain assignment statements defining SSA variable names, and the basic blocks with multiple direct predecessors contain $\phi$-nodes. Let us look at what happens when, starting from a classical SSA representation, we remove the CFG.

In order to remove the CFG, imagine a pretty printer function that dumps only the arithmetic instructions of each basic block and skips the control instructions of an imperative program by traversing the CFG structure in any order. Does the representation obtained from this pretty printer contain enough information to enable us to compute the same thing as the original program?[^1] Let us see what happens with an example in its CFG-based SSA representation:

![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310092102439.png)

[^1]: To simplify the discussion, we consider the original program to be free of side effect instructions.

After removing the CFG structure, listing the definitions in an arbitrary order, we could obtain this:

![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310092103684.png)

And this SSA code is sufficient, in the absence of side effects, to recover an order of computation that leads to the same result as in the original program. For example, the evaluation of this sequence of statements would produce the same result:
![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310092103546.png)

### 10.1.2 Natural Loop Structures on the SSA

We will now see how to represent the natural loops in the SSA form by systematically adding extra $\phi$-nodes at the end of loops, together with extra information about the loop-exit predicate. Supposing that the original program contains a loop:
![](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310092104265.png)
By pretty printing with a random order traversal, we could obtain this SSA code:
![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310092104877.png)
Note that some information is lost in this pretty printing: the exit condition of the loop has been lost. We will have to record this information in the extension of the SSA representation. However, the loop structure still appears through the cyclic definition of the induction variable $i$. To expose it, we can rewrite this SSA code using simple substitutions, such as:
![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310092106659.png)
Thus, we have the definition of the SSA name $i$ defined as a function of itself. This pattern is characteristic of the existence of a loop. We remark that there are two kinds of $\phi$-nodes used in this example:

* **Loop-$\phi$ nodes** "$i=\phi(x,\,j)$" (also denoted $i=\phi_{entry}(x,\,j)$ as in Chap. 14) have an argument that contains a self-reference $j$ and an invariant argument $x$: here the defining expression "$j=i+1$" contains a reference to the same loop-$\phi$ definition $i$, while $x$ (here 3) is not part of the circuit of dependencies that involves $i$ and $j$. Note that it is possible to define a canonical SSA form by limiting the number of arguments of loop-$\phi$ nodes to two.
* **Close-$\phi$ nodes** "$k=\phi_{exit}(i)$" (also denoted $k=\phi_{exit}(i)$ as in Chap. 14) capture the last value of a name defined in a loop. Names defined in a loop can only be used within that loop or in the arguments of a close-$\phi$ node (which is "closing" the set of uses of the names defined in that loop). In a canonical SSA form, it is possible to limit the number of arguments of close-$\phi$ nodes to one.

### 10.1.3 Improving the SSA Pretty Printer for Loops

As we have seen in the example above, the exit condition of the loop disappeared during the basic pretty printing of the SSA. To capture the semantics of the computation of the loop, we have to specify this condition in the close-$\phi$-node when we exit the loop, so as to be able to derive which value will be available at the end of the loop. With our extension, which adds the loop-exit condition to the syntax of the close-$\phi$, the SSA pretty printing of the above example would be:
![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310092107626.png)
So $k$ is defined as the "first value" of $i$ satisfying the loop- exit condition, "$i\geq N$." In the case of finite loops, this is well defined as being the first element satisfying the loop-exit condition of the sequence defined by the corresponding loop-$\phi$ node.

Below we will look at an algorithm that translates the SSA representation into a representation of polynomial functions, describing the sequence of values that SSA names take during the execution of a loop. The algorithm is restricted to the loops that are reducible. All such loops are labeled. Note that irreducible control flow is not forbidden: only the loops carrying self-definitions must be reducible.

## 10.2 Analysis of Induction Variables

The purpose of the induction variable analysis is to provide a characterization of the sequences of values taken by a variable during the execution of a loop. This characterization can be an exact function of the canonical induction variable of the loop (i.e., a loop counter that starts at zero with a step of one for each iteration of the loop) or an approximation of the values taken during the execution of the loop represented by values in an abstract domain. In this section, we will see a possible characterization of induction variables in terms of sequences. The domain of sequences will be represented by _chains of recurrences_: as an example, a canonical induction variable with an initial value 0 and a stride 1 that would occur in the loop with label $x$ will be represented by the chain of recurrences $\{0$, $+$, $1\}_{x}$.

### 10.2.1 _Stride Detection_

The first phase of the induction variable analysis is the detection of the strongly connected components of the SSA. This can be performed by traversing the use-def SSA chains and detecting that some definitions are visited twice. For a self-referring use-def chain, it is possible to derive the step of the corresponding induction variable as the overall effect of one iteration of the loop on the value of the loop-$\phi$ node. When the step of an induction variable depends on another cyclic definition, one has to further analyse the inner cycle. The analysis of the induction variable ends when all the inner cyclic definitions used for the computation of the step have been analysed. Note that it is possible to construct SSA graphs with strongly connected components that are impossible to characterize with the chains of recurrences. This is precisely the case of the following example which shows two inter-dependent circuits, the first involving $a$ and $b$ with step $c+2$, and the second involving $c$ and $d$ with step $a+2$. This leads to an endless loop, which must be detected.

![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310092108273.png)
![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310092108234.png)
Let us now look at an example, presented in Fig. 10.1, to see how the stride detection works. The arguments of a $\phi$-node are analysed to determine whether they contain self-references or are pointing towards the initial value of the induction variable. In this example, (a) represents the use-def edge that points towards the invariant definition. When the argument to be analysed points towards a longer use-def chain, the full chain is traversed, as shown in (b), until a $\phi$-node is reached. In this example, the $\phi$-node that is reached in (b) is different to the $\phi$-node from which the analysis started, and so in (c) a search starts on the uses that have not yet been analysed. When the original $\phi$-node is found, as in (c), the cyclic def-use chain provides the step of the induction variable: the step is "$+e$" in this example. Knowing the symbolic expression for the step of the induction variable may not be enough, as we will see next; one has to instantiate all the symbols ("$e$" in the current example) defined in the varying loop to precisely characterize the induction variable.

### 10.2.2 Translation to Chains of Recurrences

Once the def-use circuit and its corresponding overall loop update expression have been identified, it is possible to translate the sequence of values of the induction variable to a chain of recurrences. The syntax of a polynomial chain of recurrence is $\{\mathit{base},\,+,\,\mathit{step}\}_{x}$, where _base_ and _step_ may be arbitrary expressions or constants, and $x$ is the loop with which the sequence is associated. As a chain of recurrences represents the sequence of values taken by a variable during the execution of a loop, the associated expression of a chain of recurrences is given by $\{\mathit{base},\,+,\,\mathit{step}\}_{x}(\ell_{x})=\mathit{base}+\mathit{ step}\times\ell_{x}$, which is a function of $\ell_{x}$, the number of times the body of loop $x$ has been executed.

When _base_ or _step_ translates to sequences varying in outer loops, the resulting sequence is represented by a multivariate chain of recurrences. For example, $\{\{0,\,+,\,1\}_{x},\,+,\,2\}_{y}$ defines a multivariate chain of recurrences with a step of 1 in loop $x$ and a step of 2 in loop $y$, where loop $y$ is enclosed in loop $x$. When _step_ translates into a sequence varying in the same loop, the chain of recurrences represents a polynomial of a higher degree. For example, $\{3,\,+,\,\{8,\,+,5\}_{x}\}_{x}$ represents a polynomial evolution of degree 2 in loop $x$. In this case, the chain of recurrences is also written omitting the extra braces: $\{3,\,+,\,8,\,+,\,5\}_{x}$. The semantics of a chain of recurrences is defined using the binomial coefficient $\binom{n}{p}=\frac{n!}{p!(n-p)!}$, by the equation:

![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310092109697.png)

with $\vec{\ell}$ the iteration domain vector (the iteration loop counters of all the loops in which the chain of recurrences variates), and $\ell_{x}$ the iteration counter of loop $x$. This semantics is very useful in the analysis of induction variables, as it makes it possible to split the analysis into two phases, with a symbolic representation as a partial intermediate result:

1. First, the analysis leads to an expression where the step part "s" is left in a symbolic form, i.e., $\{c_{0},\,+,\,s\}_{x}$.
2. Then, by instantiating the step, i.e., $s=\{c_{1},\,+,\,c_{2}\}_{x}$, the chain of recurrences is that of a higher-degree polynomial, i.e., $\{c_{0},\,+,\,\{c_{1},\,+,\,c_{2}\}_{x}\}_{x}=\{c_{0},\,+,\,c_{1},\,+,\,c_{2} \}_{x}$.

### 10.2.3 _Instantiation of Symbols and Region Parameters

The last phase of the induction variable analysis consists in the instantiation (or further analysis) of symbolic expressions left from the previous phase. This includes the analysis of induction variables in outer loops, computing the last value of the counter of a preceding loop, and the propagation of closed-form expressions for loop invariants defined earlier. In some cases, it becomes necessary to leave in a symbolic form every definition outside a given region, and these symbols are then called parameters of the region.

Let us look again at the example of Fig. 10.1 to see how the sequence of values of the induction variable $c$ is characterized with the notation of the chains of recurrences. The first step, after the cyclic definition is detected, is the translation of this information into a chain of recurrences: in this example, the initial value (or base of the induction variable) is $a$ and the step is $e$, and so $c$ is represented by a chain of recurrences $\{a,\,+,\,e\}_{1}$ that is varying in loop number $1$. The symbols are then instantiated: $a$ is trivially replaced by its definition, leading to $\{3,\,+,\,e\}_{1}$. The analysis of $e$ leads to the chain of recurrences $\{8,\,+,\,5\}_{1}$, which is then used in the chain of recurrences of $c$, $\{3,\,+,\,\{8,\,+,\,5\}_{1}\}_{1}$, and is equivalent to $\{3,\,+,\,8,\,+,\,5\}_{1}$, a polynomial of degree two:

![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310092110681.png)

### 10.2.4 Number of Iterations and Computation of the End-of-Loop Value

One of the important static analyses for loops is to evaluate their trip count, i.e., the number of times the loop body is executed before the exit condition becomes true. In common cases, the loop-exit condition is a comparison of an induction variable against some constant, parameter, or other induction variables. The number of iterations is then computed as the minimum solution of a polynomial inequality with integer solutions, also called a Diophantine inequality. When one or more coefficients of the Diophantine inequality are parameters, the solution is left in parametric form. The number of iterations can also be an expression varying in an outer loop, in which case it can be characterized using a chain of recurrences.

Consider a scalar variable varying in an outer loop with strides dependent on the value computed in an inner loop. The expression representing the number of iterations in the inner loop can then be used to express the evolution function of the scalar variable varying in the outer loop.

For example, the following code:
![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310092110426.png)
would be written in loop-closed SSA form as

![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310092110967.png)

$x_{3}$ represents the value of variable $x$ at the end of the original imperative program. The analysis of scalar evolutions for variable $x_{4}$ would trigger the analysis of scalar evolutions for all the other variables defined in the loop-closed SSA form, as follows:

* First, the analysis of variable $x_{4}$ would trigger the analysis of $i$, $N$, and $x_{1}$.
	* The analysis of $i$ leads to $i=\{0,\,+,\,1\}_{1}$, i.e., the canonical loop counter $l_{1}$ of $loop_{1}$.
	* $N$ is a parameter and is left in its symbolic form.
	* The analysis of $x_{1}$ triggers the analysis of $x_{0}$ and $x_{2}$:
		* The analysis of $x_{0}$ leads to $x_{0}=0$.
			* Analysing $x_{2}$ triggers the analysis of $j$, $M$, and $x_{3}$:
			* $j=\{0,\,+,\,1\}_{2}$, i.e., the canonical loop counter $l_{2}$ of $loop_{2}$.
			* $M$ is a parameter.
			* $x_{3}=\phi^{2}_{entry}(x_{1},\,x_{3}+1)=\{x_{1},\,+,\,1\}_{2}$. $x_{2}=\phi^{2}_{entry}(j<M,\,x_{3})$ is then computed as the last value of $x_{3}$ after $loop_{2}$, i.e., it is the chain of recurrences of $x_{3}$ applied to the first iteration of $loop_{2}$ that does not satisfy $j<M$ or equivalently $l_{2}<M$. The corresponding Diophantine inequality $l_{2}\geq M$ has a minimum solution $l_{2}=M$. So, to finish the computation of the scalar evolution of $x_{2}$, we apply $M$ to the scalar evolution of $x_{3}$, leading to $x_{2}=\{x_{1},\,+,\,1\}_{2}(M)=x_{1}+M$.
	* The scalar evolution analysis of $x_{1}$ then leads to $x_{1}=\phi^{1}_{entry}(x_{0},\,x_{2})=\phi^{1}_{entry}(x_{0},\,x_{1}+M)=\{x_ {0},\,+,\,M\}_{1}=\{0,\,+,\,M\}_{1}$.
* Finally, the analysis of $x_{4}$ ends with $x_{4}=\phi^{1}_{exit}(i<N,\,x_{1})=\{0,\,+,\,M\}_{1}(N)=M\times N$.

## 10.3 Further Reading

Induction variable detection has been studied extensively in the past because of its central role in loop optimizations. Wolfe [306] designed the first SSA-based induction variable recognition technique. It abstracts the SSA graph and classifies inductions according to a wide spectrum of patterns.

When operating on a low-level intermediate representation with arbitrary gotos, detecting the natural loops is the first step in the analysis of induction variables. In general, and when operating on low-level code in particular, it is preferable to use analyses that are more robust to complex control flows that do not resort to an early classification into predefined patterns. Chains of recurrences [15; 167; 318] have been proposed to characterize the sequence of values taken by a variable during the execution of a loop [293], and it has proven to be more robust to the presence of complex, unstructured control flows, to the characterization of induction variables over modulo-arithmetic such as unsigned wrap-around types in C, and to implementation in a production compiler [232].

The formalism and presentation of this chapter are derived from the thesis work of Sebastian Pop. The manuscript [231] contains pseudo-code and links to the implementation of scalar evolutions in GCC since version 4.0. The same approach has also influenced the design of LLVM's scalar evolution, but the implementation is different.

Induction variable analysis is used in dependence tests for scheduling and parallelization [308] and more recently, the extraction of short vector to SIMD instructions [212].[^2] The Omega test [235] and parametric integer linear programming [115] have typically been used to reason about system parametric affine Diophantine inequalities. But in many cases, simulations and approximations can lead to polynomial decision procedures [16]. Modern parallelizing compilers tend to implement both kinds, depending on the context and aggressiveness of the optimization.

[^2]: Note, however, that the computation of closed-form expressions is not required for dependence testing itself [309].

Substituting an induction variable with a closed-form expression is also useful for the removal of the cyclic dependencies associated with the computation of the induction variable itself [127]. Other applications include enhancements to strength reduction and loop-invariant code motion [127], and induction variable canonicalization (reducing induction variables to a single one in a given loop) [185].

The number of iterations of loops can also be computed based on the characterization of induction variables. This information is essential to advanced loop analyses, such as value-range propagation [210], and enhanced dependence tests for scheduling and parallelization [16; 235]. It also enables more opportunities for scalar optimization when the induction variable is used after its defining loop. Looptransformations also benefit from the replacement of the end-of-loop value as this removes scalar dependencies between consecutive loops. Another interesting use of the end-of-loop value is the estimation of the worst-case execution time (WCET) where an attempt is made to obtain an upper bound approximation of the time necessary for a program to terminate.