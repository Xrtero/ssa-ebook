# Chapter 8 Propagating Information Using SSA

**Florian Brandner and Diego Novillo**

A central task of compilers is to _optimize_ a given input program such that the resulting code is more efficient in terms of execution time, code size, or some other metric of interest. However, in order to perform these optimizations, typically some form of _program analysis_ is required to determine if a given program transformation is applicable, to estimate its profitability, and to guarantee its correctness.

_Data-flow analysis_ is a simple yet powerful approach to program analysis that is utilized by many compiler frameworks and program analysis tools today. We will introduce the basic concepts of traditional data-flow analysis in this chapter and will show how the _Static Single Assignment_ form (SSA) facilitates the design and implementation of equivalent analyses. We will also show how the SSA property allows us to reduce the compilation time and memory consumption of the data-flow analyses that this program representation supports.

Traditionally, data-flow analysis is performed on a _control-flow graph_ representation (CFG) of the input program. Nodes in the graph represent operations, and edges represent the potential flow of program execution. Information on certain _program properties_ is propagated among the nodes along the control-flow edges until the computed information stabilizes, i.e., no _new_ information can be inferred from the program.

The _propagation engine_ presented in the following sections is an extension of the well-known approach by Wegman and Zadeck for _sparse conditional constant propagation_ (also known as SSA-CCP). Instead of using the CFG, they represent the input program as an _SSA graph_ as defined in Chap. 14: operations are againrepresented as nodes in this graph; however, the edges represent _data dependencies_ instead of control flow. This representation allows selective propagation of program properties among data-dependent graph nodes only. As before, the processing stops when the information associated with the graph nodes stabilizes. The basic algorithm is not limited to constant propagation and can also be applied to solve a large class of other data-flow problems efficiently. However, not all data-flow analyses can be modelled. Chapter 13 addresses some of the limitations of the SSA-based approach.

The remainder of this chapter is organized as follows. First, the basic concepts of (traditional) data-flow analysis are presented in Sect. 8.1. This will provide the theoretical foundation and background for the discussion of the SSA-based propagation engine in Sect. 8.2. We then provide an example of a data-flow analysis that can be performed efficiently by the aforementioned engine, namely copy propagation in Sect. 8.3.

## 8.1 Preliminaries

Data-flow analysis is at the heart of many compiler transformations and optimizations but also finds application in a broad spectrum of analysis and verification tasks in program analysis tools such as program checkers, profiling tools, and timing analysis tools. This section gives a brief introduction to the basics of data-flow analysis.

As noted before, data-flow analysis derives information from certain interesting program properties that may help to optimize the program. Typical examples of interesting properties are: the set of _live_ variables at a given program point, the particular constant value a variable may take, or the set of program points that are reachable at runtime. Liveness information, for example, is critical during register allocation, while the two latter properties help to simplify computations and to identify dead code.

The analysis results are gathered from the input program by propagating information among its operations considering all potential execution paths. The propagation is typically performed iteratively until the computed results stabilize. Formally, a data-flow problem can be specified using a _monotone framework_ that consists of:

* A _complete lattice_ representing the property space
* A _flow graph_ resembling the control flow of the input program
* A set of _transfer functions_ modelling the effect of individual operations on the property space

**Property Space** A key concept for data-flow analysis is the representation of the property space via _partially ordered sets_ ($L,\sqsubseteq$), where $L$ represents some interesting program property and $\sqsubseteq$ represents a reflexive, transitive, and anti symmetric relation. Using the $\sqsubseteq$ relation, _upper_ and _lower bounds_, as well as _least upper_ and _greatest lower bounds_, can be defined for subsets of $L$.

A particularly interesting class of partially ordered sets are _complete lattices_, where all subsets have a least upper bound as well as a greatest lower bound. Those bounds are unique and are denoted by $\sqcup$ and $\sqcap$, respectively. In the context of program analysis, the former is often referred to as the _join operator_, while the latter is termed the _meet operator_. Complete lattices have two distinguished elements, the _least element_ and the _greatest element_, often denoted by $\bot$ and $\top$, respectively.

An _ascending chain_ is a totally ordered subset $\{l_{1},\ldots,l_{n}\}$ of a complete lattice. A chain is said to _stabilize_ if there exists an index $m$, where $\forall i>m:l_{i}=l_{m}$. An analogous definition can be given for _descending chains_.

**Program Representation** The functions of the input program are represented as control-flow graphs, where the nodes represent operations, or instructions, and edges denote the potential flow of execution at runtime. Data-flow information is then propagated from one node to another adjacent node along the respective graph edge using _in_ and _out_ sets associated with every node. If there exists only one edge connecting two nodes, data can be simply copied from one set to the other. However, if a node has multiple incoming edges, the information from those edges has to be combined using the meet or join operator.

Sometimes, it is helpful to reverse the flow graph to propagate information, i.e., reverse the direction of the edges in the control-flow graph. Such analyses are termed _backward analyses_, while those using the regular flow graph are _forward analyses_.

**Transfer Functions** Aside from the control flow, the operations of the program need to be accounted for during analysis. Usually, these operations change the way data is propagated from one control-flow node to the other. Every operation is thus mapped to a _transfer function_, which transforms the information available from the _in_ set of the flow graph node of the operation and stores the result in the corresponding _out_ set.

### 8.1.1 Solving Data-Flow Problems

Putting all those elements together--a complete lattice, a flow graph, and a set of transfer functions--yields an instance of a monotone framework. This framework describes a set of _data-flow equations_ whose solution will ultimately converge to the solution of the data-flow analysis. A very popular and intuitive way to solve these equations is to compute the _maximal (minimal) fixed point_ (MFP) using an iterative work list algorithm. The work list contains edges of the flow graph that have to be revisited. Visiting an edge consists of first combining the information from the _out_ set of the source node with the _in_ set of the target node, using the meet or join operator, and then applying the transfer function of the target node. The obtained information is then propagated to all direct successors of the target node by appending the corresponding edges to the work list. The algorithm terminates when the data-flow information stabilizes, as the work list then becomes empty.

A single flow edge can be appended several times to the work list in the course of the analysis. It may even happen that an infinite feedback loop prevents the algorithm from terminating. We are thus interested in bounding the number of times a flow edge is processed. Recalling the definition of chains from before (see Sect. 8.1), the _height_ of a lattice is defined by the length of its longest chain. We can ensure termination for lattices fulfiling the _ascending chain condition_, which ensures that the lattice has finite height. Given a lattice with finite height $h$ and a flow graph $G~{}=~{}(V,~{}E)$, it is easy to see that the MFP solution can be computed in $O(|E|\cdot h)$ time, where $|E|$ represents the number of edges. Since the number of edges is bounded by the number of graph nodes $|V|$, more precisely, $|E|\leq|V|^{2}$, this gives a $O(|V|^{2}\cdot h)$ general algorithm to solve data-flow analyses. Note that the height of the lattice often depends on properties of the input program, which might ultimately yield bounds worse than cubic in the number of graph nodes. For instance, the lattice for copy propagation consists of the cross product of many smaller lattices, each representing the potential values of a variable occurring in the program. The total height of the lattice thus directly depends on the number of variables in the program.

In terms of memory consumption, we have to propagate data-flow to all relevant program points. Nodes are required to hold information even when it is not directly related to the node; hence, each node must store complete _in_ and _out_ sets.

## 8.2 Data-Flow Propagation Under SSA Form

SSA form allows us to solve a large class of data-flow problems more efficiently than the iterative work list algorithm presented previously. The basic idea is to directly propagate information computed at the unique definition of a variable to all its uses. In this way, intermediate program points that neither define nor use the variable of interest do not have to be taken into consideration, thus reducing memory consumption and compilation time.

### 8.2.1 Program Representation

Data-flow analyses under SSA form rely on a specialized program representation based on _SSA graphs_ (see Chap. 14). Besides the data dependencies, the SSA graph captures the _relevant_ join nodes of the CFG of the program. A join node is relevant for the analysis whenever the value of two or more definitions may reach a use by passing through that join. The SSA form properties ensure that a $\phi$-function is placed at the join node and that any use of the variable that the $\phi$-function defines has been properly updated to refer to the correct name.

![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310092040371.png)

Consider, for example, the code excerpt shown in Fig. 8.1, along with its corresponding SSA graph and CFG. Assume we are interested in propagating information from the assignment of variable $y_{1}$, at the beginning of the code, down to its unique use at the end. The traditional CFG representation causes the propagation to pass through several intermediate program points. These program points are concerned only with computations of the variables $x_{1}$, $x_{2}$, and $x_{3}$ and are thus irrelevant for $y_{1}$. The SSA graph representation, on the other hand, propagates the desired information directly from definition to use sites, without any intermediate step. At the same time, we also find that the control-flow join following the conditional is properly represented by the $\phi$-function defining the variable $x_{3}$ in the SSA graph.

Even though the SSA graph captures data dependencies and the relevant join nodes in the CFG, it lacks information on other _control dependencies_. However, analysis results can often be improved significantly by considering the additional information that is available from the control dependencies in the CFG. As an example, consider once more the code in Fig. 8.1, and assume that the condition associated with the if-statement is known to be false for all possible program executions. Consequently, the $\phi$-function will select the value of $x_{2}$ in all cases, which is known to be of constant value 5. However, due to the shortcomings of the SSA graph, this information cannot be derived. It is thus important to use both the control-flow graph and the SSA graph during data-flow analysis in order to obtain the best possible results.

Figure 8.1: Example program and its SSA graph

### 8.2.2 Sparse Data-Flow Propagation

Similar to monotone frameworks for traditional data-flow analysis, frameworks for _sparse data-flow propagation_ under SSA form can be defined using:

* A _complete lattice_ representing the property space
* A set of _transfer functions_ for the operations of the program
* A _control-flow graph_ capturing the execution flow of the program
* An _SSA graph_ representing data dependencies

We again seek a maximal (minimal) fixed-point solution (MFP) using an iterative work list algorithm. However, in contrast to the algorithm described previously, data-flow information is not propagated along the edges of the control-flow graph but along the edges of the SSA graph. For regular uses, the propagation is straightforward due to the fact that every use receives its value from a unique definition. Special care has to be taken only for $\phi$-functions, which select a value among their operands depending on the incoming control-flow edges. The data-flow information of the incoming operands has to be combined using the meet or join operator of the lattice. As data-flow information is propagated along SSA edges that have a single source, it is sufficient to store the data-flow information with the SSA graph node. The _in_ and _out_ sets used by the traditional approach--see Sect. 8.1--are obsolete, since $\phi$-functions already provide the required buffering. In addition, the control-flow graph is used to track which operations are not reachable under any program execution and thus can be safely ignored during the computation of the fixed-point solution.

The algorithm is shown in Algorithm 8.1 and processes two work lists, the _CFGWorkList_, which contains edges of the control-flow graph, and the _SSAWorkList_, which contains edges from the SSA graph. It proceeds by removing the top element of either of those lists and processing the respective edge. Throughout the main algorithm, operations of the program are visited to update the work lists and propagate information using Algorithm 8.2.

The _CFGWorkList_ is used to track edges of the CFG that were encountered to be _executable_, i.e., where the data-flow analysis cannot rule out that a program execution traversing the edge exists. Once the algorithm has determined that a CFG edge is executable, it will be processed by Step 3 of the main algorithm. First, all $\phi$-functions of its target node need to be reevaluated due to the fact that Algorithm 8.2 discarded the respective operands of the $\phi$-functions so far--because the control-flow edge was not yet marked executable. Similarly, the operation of the target node has to be evaluated when the target node is encountered to be executable for the first time, i.e., the currently processed control-flow edge is the first of its incoming edges that is marked executable. Note that this is only required the _first_ time the node is encountered to be executable, due to the processing of operations in Step 4b, which thereafter triggers the reevaluation automatically when necessary through the SSA graph.

![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310092040024.png)
![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310092041487.png)

Regular operations as well as $\phi$-functions are visited by Algorithm 8 when the corresponding control-flow graph node has become executable, or whenever the data-flow information of one of their direct predecessors in the SSA graph has changed. At $\phi$-functions, the information from multiple control-flow paths is combined using the usual meet or join operator. However, only those operands where the associated control-flow edge is marked executable are considered. Conditional branches are handled by examining their conditions based on the data-flow information computed so far. Depending on whether those conditions are satisfiable or not, control-flow edges are appended to the _CFGWorkList_ to ensure that all reachable operations are considered during the analysis. Finally, all regular operations are processed by applying the relevant transfer function and possibly propagating the updated information to all uses by appending the respective SSA graph edges to the _SSAWorkList_.

![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310092041066.png)

As an example, consider the program shown in Fig. 8.1 and the constant propagation problem. First, assume that the condition of the if-statement cannot be statically evaluated, we thus have to assume that all its successors in the CFG are reachable. Consequently, all control-flow edges in the program will eventually be marked executable. This will trigger the evaluation of the constant assignments to the variables $x_{1}$, $x_{2}$, and $y_{1}$. The transfer functions immediately yield that the variables are all constant, holding the values 4, 5, and 6, respectively. This new information will trigger the reevaluation of the $\phi$-function of variable $x_{3}$. As both of its incoming control-flow edges are marked executable, the combined information yields $4\big{\lceil}\big{\rceil}5=\bot$, i.e., the value is known not to be a particular constant value. Finally, also the assignment to variable $z_{1}$ is reevaluated, but the analysis shows that its value is not a constant as depicted in Fig. 8.2a. If, however, the if-condition is known to be false for all possible program executions, a more precise result can be computed, as shown in Fig. 8.2b. Neither the control-flow edge leading to the assignment of variable $x_{1}$ nor its outgoing edge leading to the $\phi$-function of variable $x_{3}$ is marked executable. Consequently, the reevaluation of the $\phi$-function considers the data-flow information of its second operand $x_{2}$ only, which is known to be constant. This enables the analysis to show that the assignment to variable $z_{1}$ is, in fact, constant as well.

### 8.2.3 Discussion

During the course of the propagation algorithm, every edge of the SSA graph is processed at least once, whenever the operation corresponding to its definition is found to be executable. Afterwards, an edge can be revisited several times depending on the height $h$ of the lattice representing the property space of the analysis. On the other hand, edges of the control-flow graph are processed at most

Figure 8.2: Sparse conditional data-flow propagation using SSA graphs

once. This leads to an upper bound in execution time of $O(|E_{SSA}|\cdot h+|E_{CFG}|)$, where $E_{SSA}$ and $E_{CFG}$ represent the edges of the SSA graph and the control-flow graph, respectively. The size of the SSA graph increases with respect to the original non-SSA program. Measurements indicate that this growth is linear, yielding a bound that is comparable to the bound of traditional data-flow analysis. However, in practice, the SSA-based propagation engine outperforms the traditional approach. This is due to the direct propagation from the definition of a variable to its uses, without the costly intermediate steps that have to be performed on the CFG. The overhead is also reduced in terms of memory consumption: instead of storing the _in_ and _out_ sets capturing the complete property space on every program point, it is sufficient to associate every node in the SSA graph with the data-flow information of the corresponding variable only, leading to considerable savings in practice.

## 8.3 Example--Copy Propagation

Even though data-flow analysis based on SSA graphs has its limitations, it is still a useful and effective solution for interesting problems, as we will show in the following example. Copy propagation under SSA form is, in principle, very simple. Given the assignment $x\gets y$, all we need to do is to traverse the immediate uses of $x$ and replace them with $y$, thereby effectively eliminating the original copy operation. However, such an approach will not be able to propagate copies past $\phi$-functions, particularly those in loops. A more powerful approach is to split copy propagation into two phases: First, a data-flow analysis is performed to find copy-related variables throughout the program. Second, a rewrite phase eliminates spurious copies and renames variables.

![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310092041424.png)

The analysis for copy propagation can be described as the problem of propagating the _copy of value_ of variables. Given a sequence of copies as shown in Fig. 8.3a, we say that $y_{1}$ is a _copy of_$x_{1}$ and $z_{1}$ is a _copy of_$y_{1}$. The problem with this representation is that there is no apparent link from $z_{1}$ to $x_{1}$. In order to handle transitive copy relations, all transfer functions operate on copy of values instead of the direct source of the copy. If a variable is not found to be a copy of anything else, its copy of value is the variable itself. For the above example, this yields that both $y_{1}$ and $z_{1}$ are copies of $x_{1}$, which in turn is a copy of itself. The lattice of this data-flow problem is thus similar to the lattice used previously for constant propagation. The lattice elements correspond to variables of the program instead of integer numbers. The least element of the lattice represents the fact that a variable is a copy of itself.

![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310092042130.png)


Similarly, we would like to obtain the result that $x_{3}$ is a copy of $y_{1}$ for the example of Fig. 8.3b. This is accomplished by choosing the join operator such that a copy relation is propagated whenever the copy of values of all the operands of the $\phi$-function matches. When visiting the $\phi$-function for $x_{3}$, the analysis finds that $x_{1}$ and $x_{2}$ are both copies of $y_{1}$, and consequently propagates that $x_{3}$ is also a copy of $y_{1}$.

The next example shows a more complex situation where copy relations are obfuscated by loops--see Fig. 8.4. Note that the actual visiting order depends on the shape of the CFG and immediate uses; in other words, the ordering used here is meant for illustration only. Processing starts at the operation labeled 1, with both work lists empty and the data-flow information $\top$ associated with all variables:

1. Assuming that the value assigned to variable $x_{1}$ is not a copy, the data- flow information for this variable is lowered to $\bot$, the SSA edges leading to operations 2 and 3 are appended to the _SSAWorkList_, and the control-flow graph edge $e_{1}$ is appended to the _CFGWorkList_.
2. Processing the control-flow edge $e_{1}$ from the work list causes the edge to be marked executable and the operations labeled 2 and 3 to be visited. Since edge $e_{5}$ is not yet known to be executable, the processing of the $\phi$-function yields a copy relation between $x_{2}$ and $x_{1}$. This information is utilized in order to determine which outgoing control-flow graph edges are executable for the conditional branch. Examining the condition shows that only edge $e_{3}$ is executable and thus needs to be added to the work list.

Figure 8.4: $\phi$-functions in loops often obfuscate copy relations

3. Control-flow edge $e_{3}$ is processed next and marked executable for the first time. Furthermore, the $\phi$-function labeled 5 is visited. Due to the fact that edge $e_{4}$ is not known to be executable, this yields a copy relation between $x_{4}$ and $x_{1}$ (via $x_{2}$). The condition of the branch labeled 6 cannot be analysed and thus causes its outgoing control-flow edges $e_{5}$ and $e_{6}$ to be added to the work list.
4. Now, control-flow edge $e_{5}$ is processed and marked executable. Since the target operations are already known to be executable, only the $\phi$-function is revisited. However, variables $x_{1}$ and $x_{4}$ have the same copy of value $x_{1}$, which is identical to the previous result computed in Step 2. Thus, neither of the two work lists is modified.
5. Assuming that the control-flow edge $e_{6}$ leads to the exit node of the control-flow graph, the algorithm stops after processing the edge without modifications to the data-flow information computed so far.

The straightforward implementation of copy propagation would have needed multiple passes to discover that $x_{4}$ is a copy of $x_{1}$. The iterative nature of the propagation, along with the ability to discover non-executable code, allows one to handle even obfuscated copy relations. Moreover, this kind of propagation will only reevaluate the subset of operations affected by newly computed data-flow information instead of the complete control-flow graph once the set of executable operations has been discovered.

## 8.4 Further Reading

Traditional data-flow analysis is well established and well described in numerous papers. The book by Nielsen, Nielsen, and Hankin [208] gives an excellent introduction to the theoretical foundations and practical applications. For reducible flow graphs, the order in which operations are processed by the work list algorithm can be optimized [144, 157, 208], allowing one to derive tighter complexity bounds. However, relying on reducibility is problematic because the flow graphs are often _not_ reducible even for proper structured languages. For instance, reversed control-flow graphs for backward problems can be--and in fact almost always are--irreducible even for programs with reducible control-flow graphs, for instance because of loops with multiple exits. Furthermore, experiments have shown that the tighter bounds do not necessarily lead to improved compilation times [85].

Apart from computing a fixed-point (MFP) solution, traditional data-flow equations can also be solved using a more powerful approach called the _meet over all paths_ (MOP) solution, which computes the _in_ data-flow information for a basic block by examining _all_ possible paths from the start node of the control-flow graph. Even though more powerful, computing the MOP solution is often harder or even undecidable [208]. Consequently, the MFP solution is preferred in practice.

The sparse propagation engine [210, 303], as presented in the chapter, is based on the underlying properties of the SSA form. Other intermediate representationsoffer similar properties. _Static Single Information_ form (SSI) first introduced by Ananian [8] and then later fixed by Singer [261] has been designed with the objective of allowing both backward and forward problems. It uses $\sigma$-functions, which are placed at program points where data-flow information for backward problems needs to be merged [260]. But as illustrated by dead-code elimination (backward sparse data-flow analysis on SSA), the fact that $\sigma$-functions are necessary for backward propagation problems (or $\phi$-functions for forward) is a misconception. This subtle point is further explained in Sect. 13.2. Bodik also uses an extended SSA form, $e$-SSA, his goal being to eliminate array bounds checks [32]. $e$-SSA is a simple extension that allows one to also propagate information from conditionals. Chapter 13 revisits the concepts of static single information and proposes a generalization that subsumes all those extensions to SSA form. Ruf [250] introduces the _value dependence graph_, which captures both control and data dependencies. He derives a sparse representation of the input program, which is suited for data-flow analysis, using a set of transformations and simplifications.

The _sparse evaluation graph_ by Choi et al. [67] is based on the same basic idea as the approach presented in this chapter: intermediate steps are eliminated by bypassing irrelevant CFG nodes and merging the data-flow information only when necessary. Their approach is closely related to the placement of $\phi$-functions and similarly relies on the dominance frontier during construction. A similar approach, presented by Johnson and Pingali [153], is based on single-entry/single-exit regions. The resulting graph is usually less sparse but is also less complex to compute. Ramalingam [237] further extends those ideas and introduces the _compact evaluation graph_, which is constructed from the initial CFG using two basic transformations. The approach is superior to the sparse representations by Choi et al. as well as the approach presented by Johnson and Pingali.

The previous approaches derive a sparse graph suited for data-flow analysis using graph transformations applied to the CFG. Duesterwald et al. instead examine the data-flow equations, eliminate redundancies, and apply simplifications to them [107].