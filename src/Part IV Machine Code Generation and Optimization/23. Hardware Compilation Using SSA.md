# Chapter 23. Hardware Compilation Using SSA

**Pedro C. Diniz and Philip Brisk**




This chapter describes the use of SSA-based high-level program representations for the realization of the corresponding computations using hardware digital circuits. We begin by highlighting the benefits of using a compiler SSA-based intermediate representation in this hardware mapping process, using an illustrative example. The subsequent sections describe hardware translation schemes for discrete hardware logic structures or datapaths of hardware circuits and outline several compiler transformations that benefit from SSA. We conclude with a brief survey of various hardware compilation efforts from both academia and industry that have adopted SSA-based internal representations.

## 23.1 Brief History and Overview

Hardware compilation is the process by which a high-level behavioural description of a computation is translated into a hardware-based implementation, that is, a circuit expressed in a hardware design language such as VHDL, or Verilog which can be directly realized as an electrical (often digital) circuit.

Hardware-oriented languages such as VHDL or Verilog allow programmers to develop such digital circuits either by structural composition of blocks using abstractions such as wires and ports or behaviourally by definition of the input-output relations of signals in these blocks. A mix of both design approaches is often found in medium to large designs. Using a structural approach, a circuitdescription will typically include discrete elements such as registers (flip-flops) that capture the state of the computation at specific events, such as clock edges, and combinatorial elements that transform the values carried by wires. The composition of these elements allows programmers to build finite-state machines (FSMs) that orchestrate the flow and the processing of data stored in internal registers or RAM structures. These architectures support an execution model with operations akin to assembly instructions found in common processors that support the execution of high-level programs.

A common vehicle for the realization of hardware designs is an FPGA or field-programmable gate array. These devices include a large number of configurable logic blocks (or CLBs), each of which can be individually programmed to realize an arbitrary combinatorial function of k inputs whose outputs can be latched in flip-flops and connected via an internal interconnection network to any subset of the CLBs in the device. Given their design regularity and simple structure, these devices, popularized in the 1980s as fast hardware prototyping vehicles, have taken advantage of Moore's law to grow to large sizes with which programmers can define custom architectures capable of TFlops/Watt performance, thus making them the vehicle of choice for very power-efficient custom computing machines.

While developers were initially forced to design hardware circuits exclusively using schematic capture tools, over the years, high-level behavioural synthesis allowed them to leverage a wealth of hardware mapping and design exploration techniques to realize substantial productivity gains.

As an example, Fig. 23 illustrates these concepts of hardware mapping for the computation expressed as $x\,\leftarrow\,(a\times b)-(c\times d)+f$. Figure 23 depicts a graphical representation of a circuit that directly implements this computation. Here, there is a direct mapping between hardware operators such as adders and multipliers and the operations in the computation. Input values are stored in the registers at the top of the diagram, and the entire computation is carried out during a single (albeit long) clock cycle, at the end of which the results propagated through the various hardware operators are captured (or latched) in the registers at the bottom of the diagram. In all, this direct implementation uses two multipliers, two adders/subtractors, and six registers, five registers to hold the computation's input values and one to capture the computation's output result. This hardware implementation requires a simple control scheme, as it just needs to record the input values, and wait for a single clock cycle at the end of which it stores the outputs of the operations in the output register. Figure 23 depicts a different implementation variant of the same computation, this time using nine registers and the same number of adders and subtractors.1 The increased number of registers allows the circuit to be clocked at a higher frequency as well as to be executed in a pipelined fashion. Lastly, Fig. 23 depicts yet another possible implementation of the same computation,Figure 23: Different variants of mapping a computation to hardware

but using a single multiplier operator. This last version allows for the reuse in time of the multiplier operator and the required thirteen registers as well as multiplexers to route the inputs to the multiplier in two distinct control steps. As is apparent, the reduction in the number of operators, in this particular case the multipliers, carries a penalty in the form of an increased number of registers and multiplexers.[^2] It can be viewed as a hardware implementation of the C programming language selection operator: $out=q$? $in_{1}$ : $in_{2}$ and increased complexity of the control scheme.

[^2]: A 2 $\times$ 1 multiplexer is a combinatorial circuit with two data inputs, a single output and a control input, where the control input selects which of the two data inputs is transmitted to the output.


![Fig.23.1.svg](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310110253604.svg)

This example illustrates the many degrees of freedom in high-level behavioural hardware synthesis. Synthesis techniques perform the classical tasks of allocation, binding, and scheduling of the various operations in a computation given specific target hardware resources. For instance, a designer can use a behavioural synthesis tool (e.g., Xilinx's Vivado) to automatically derive a hardware implementation for a computation, as expressed in the example in Fig. 23.1a. This high-level description suggests a simple, direct hardware implementation using a single adder and two multipliers, as depicted in Fig. 23.1b. Alternative hardware implementations taking advantage of pipelined execution and the sharing of a multiplier to reduce hardware resources are respectively illustrated in Fig. 23.1c and Fig. 23.1d. The tool then derives the control scheme required to route the data from registers to the selected units so as to meet the designers' goals.

Despite the introduction of high-level behavioural synthesis techniques into commercially available tools, hardware synthesis, and thus hardware compilation, has never enjoyed the same level of success as traditional software compilation. Sequential programming paradigms popularized by programming languages such as C/C++ and, more recently, by Java, allow programmers to easily reason about program behaviour as a sequence of program memory state transitions. The underlying processors and the corresponding system-level implementations present a number of simple unified abstractions--such as a unified memory model, a stack, and a heap that do not exist (and often do not make sense) in customized hardware designs.

Hardware compilation, in contrast, has faced numerous obstacles that have hampered its wide adoption. When developing hardware solutions, designers must understand the concept of spatial concurrency that hardware circuits offer. Precise timing and synchronization between distinct hardware components are key abstractions in hardware. Solid and robust hardware design implies a detailed understanding of the precise timing of specific operations, including I/O, that simply cannot be expressed in languages such as C, C++, or Java. Alternatives such as SystemC have emerged in recent years, giving the programmer considerably more control over these issues. The inherent complexity of hardware designs has hampered the development of robust synthesis tools that can offer high-level programming abstractions enjoyed by tools that target traditional architecture and software systems, thus substantially raising the barrier of entry for hardware design ers in terms of productivity and robustness of the generated hardware solutions. At best, hardware compilers today can only handle certain subsets of mainstream high-level languages and at worst are limited to purely arithmetic sequences of operations with strong restrictions on control flow.

Nevertheless, the emergence of multi-core processing has led to the introduction of new parallel programming languages and parallel programming constructs that are more amenable to hardware compilation than traditional languages. For example, MapReduce, originally introduced by Google to spread parallel jobs across clusters of servers, has been an effective programming model for FPGAs as it naturally exposes task-level concurrency with data independence. Similarly, high-level languages based on parallel models of computation such as synchronous data flow, or functional single-assignment languages, have also been shown to be good choices for hardware compilation, as not only do they make data independence obvious, but in many cases, the natural data partitioning they expose is a natural match for the spatial concurrency of FPGAs.

Although in the remainder of this chapter we will focus primarily on the use of SSA representations for hardware compilation of imperative high-level programming languages, many of the emerging parallel languages, while including sequential constructs (such as control-flow graphs), also support true concurrent constructs. These languages can be a natural fit to exploit the spatial and customization opportunities of FPGA-based computing architectures. While the extension of SSA form to these emerging languages is an open area of research, the fundamental uses of SSA for hardware compilation, as discussed in this chapter, are likely to remain a solid foundation for the mapping of these parallel constructs to hardware.

## 23.2 Why Use SSA for Hardware Compilation?

Hardware compilation, unlike its software counterpart, offers a spatially oriented computational infrastructure that presents opportunities to leverage information exposed by the SSA representation. We illustrate the direct connection between SSA representation form and hardware compilation using the example of computation mapping in Fig. 23.2a. Here the value of a variable $v$ depends on the control flow of the computation, as the temporary variable $t$ can be assigned different values depending on the value of the $p$ predicate. The representation of this computation is depicted in Fig. 23.2b, where a $\phi$-function is introduced to capture the two possible assignments to the temporary variable $t$ in both control branches of the if-then-else construct. Lastly, in Fig. 23.2c, we illustrate the corresponding mapping to hardware.

The basic observation is that the confluence of values for a given program variable leads to the use of a $\phi$-function. This $\phi$-function abstraction thus corresponds in terms of hardware implementation to the insertion of a multiplexer logic circuit. This logic circuit uses the boolean value of a control input to select which of its input's values is to be propagated to its output. The selection or control input of a multiplexer thus acts as a gated transfer of value that parallels the actions of an if-then-else construct in software. Note also that in the case of a backward control flow (e.g., associated with a back edge of a loop), the possible indefinition of one of the $\phi$-function's inputs is transparently ignored by the fact that in a correct execution the predicate associated with the true control-flow path will yield the value associated with a defined input of the SSA representation.

![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310110253237.png)

Equally important in this mapping is the notion that the computation in hardware can now take a spatial dimension. In the hardware circuit in Fig. 23.2c, the computation derived from the statement in both branches of the if-then-else construct can be evaluated concurrently by distinct logic circuits. After the evaluation of both circuits, the multiplexer will define which set of values is used based on the value of its control input, in this case of the value of the computation associated with $p$.

In a sequential software execution environment, the predicate $p$ would be evaluated first, and then either branches of the if-then-else construct would be evaluated, based on the value of $p$; as long as the register allocator is able to assign $t_{1}$, $t_{2}$, and $t_{3}$ to the same register, then the $\phi$-function is executed implicitly; if not, it is executed as a register-to-register copy.

There have been some efforts to automatically convert the sequential program above into a semi-spatial representation that could obtain some speedup if executed on a VIW (very long instruction word) type of processor. For example, if-conversion (see Chap. 20) would convert the control dependency into a data dependency: statements from the if and else blocks could be interleaved, as long as they do not overwrite one another's values, and the proper result (the $\phi$-function) could be selected using a conditional move instruction. In the worst case, however, this approach would effectively require the computation of both branch sides, rather than one, so it could increase the computation time. In contrast, in a spatial representation, the correct result can be output as soon as two of the three inputs to the multiplexer are known ($p$, and one of $t_{1}$ or $t_{2}$, depending on the value of $p$).

While the Electronic Design Automation (EDA), community has for decades now exploited similar information regarding data and control dependencies for the generation of hardware circuits from increasingly higher-level representations (e.g., Behavioural HDL), SSA-based representations make these dependencies explicit in the intermediate representation itself. Similarly, the more classical compiler representation, using three-address instructions augmented with the def-use chains, already exposes the data-flow information as for the SSA-based representation. However, as we will explore in the next section, the latter facilitates the mapping and selection of hardware resources.

##  23.3 Mapping a Control-Flow Graph to Hardware

In this section, we focus on hardware implementations of circuits that are spatial in nature. We therefore do not address the mapping to architectures such as VLIW or Systolic Arrays. While these architectures raise interesting and challenging issues, namely scheduling and resource usage, we are more interested in exploring and highlighting the benefits of SSA representation which, we believe, are more naturally (although not exclusively) exposed in the context of spatial hardware computations.

### 23.3.1 Basic Block Mapping

As a basic block is a straight-line sequence of three-address instructions, a simple hardware mapping approach consists in composing or evaluating the operations in each instruction as a data-flow graph. The inputs and outputs of the instructions are transformed into registers[^3] connected by nodes in the graph that represent the operators.

[^3]: As a first approach, these registers are virtual, and then, after synthesis, some of them are materialized to physical registers in a process similar to register allocation in software-oriented compilation.


As a result of the "evaluation" of the instructions in the basic block, this algorithm constructs a hardware circuit that has as input registers that will hold the values of the input variables to the various instructions and will have as outputs registers that hold only variables that are live outside the basic block.

### 23.3.2 Basic Control-Flow Graph Mapping

One can combine the various hardware circuits corresponding to a control-flow graph in two basic approaches, respectively, _spatial_ and _temporal_. The spatial form of combining hardware circuits consists in laying out the various circuits spatially by connecting variables that are live at the output of a basic block, and therefore the output registers of the corresponding hardware circuit, to the registers that will hold the values of those same variables in subsequent hardware circuits of the basic blocks that execute in sequence.

In the temporal approach, the hardware circuits corresponding to the various CFG basic blocks are not directly interconnected. Instead, their input and output registers are connected via dedicated buses to a local storage module. An execution controller "activates" a basic block or a set of basic blocks by transferring data between the storage and the input registers of the hardware circuits to be activated. Upon execution completion, the controller transfers the data from the output registers of each hardware circuit to the storage module. These data transfers do not necessarily need to be carried out sequentially, but instead can leverage the aggregation of the outputs of each hardware circuit to reduce transfer time to and from the storage module via dedicated wide buses.

The temporal approach described above is well suited to the scenario where the target hardware architecture does not have sufficient resources to simultaneously implement the hardware circuits corresponding to all basic blocks of interest, as it trades off execution time for hardware resources.

In such a scenario, where hardware resources are very limited or the hardware circuit corresponding to a set of basic blocks is exceedingly large, one could opt for partitioning a basic block or a set of basic blocks into smaller blocks until the space constraints for the realization of each hardware circuit are met. In reality, this is the common approach in every processor today. It limits the hardware resources to those required for each of the ISA instructions and schedules them in time at each step, saving the states (registers) that were the output of the previous instruction. The computation thus proceeds as described above by saving the values of the output registers of the hardware circuit corresponding to each smaller block.

These two approaches, illustrated in Fig. 23.3, can obviously be merged in a hybrid implementation. As they lead to distinct control schemes for the orchestration of the execution of computation in hardware, their choice depends heavily on the nature and granularity of the target hardware architecture. For fine-grain hardware architectures such as FPGAs, a spatial mapping can be favoured, while for coarse-grain architectures a temporal mapping is common.

![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310110256906.png)
While the overall execution control for the temporal mapping approach is simpler, as the transfers to and from the storage module are done upon the transfer of control between hardware circuits, a spatial mapping approach makes it easier totake advantage of pipelining execution techniques and speculation.4 The temporal mapping approach can, however, be area-inefficient, as often only one basic block will execute at any point in time. This issue can nevertheless be mitigated by exposing additional amounts of instruction-level parallelism by merging multiple basic blocks into a single _hyper-block_ and combining this aggregation with loop unrolling. Still, as these transformations and their combination can lead to a substantial increase in the required hardware resources, a compiler can exploit resource sharing between the hardware units corresponding to distinct basic blocks to reduce the pressure on resource requirements and thus lead to feasible hardware implementation designs. As these optimizations are not specific to the SSA representation, we will not discuss them further here.

### 23.3.3 Control-Flow Graph Mapping Using SSA

In the spatial mapping approach, the SSA form plays an important role in the minimization of multiplexers and thus in the simplification of the corresponding data-path logic and execution control.

Consider the illustrative example in Fig. 23.4a. Here, basic block BB0 defines a value for the variables $x$ and $y$. One of the two subsequent basic blocks BB1 redefines the value of $x$, whereas the other basic block BB2 only reads $x$.

A naive implementation based exclusively on liveness analysis (see Chap. 9) would use multiplexers for both variables $x$ and $y$ to merge their values as inputs to the hardware circuit implementing basic block BB3, as depicted in Fig. 23.4b. As can be observed, however, the SSA form representation captures the fact that such a multiplexer is only required for variable $x$. The value for variable $y$ can be propagated either from the output value in the hardware circuit for basic block BB0 (as shown in Fig. 23.4c) or from any other register that has a valid copy of the $y$ variable. The direct flow of the single definition point to all its uses, across the hardware circuits corresponding to the various basic blocks in the SSA form, thus allows a compiler to use the minimal number of multiplexers strictly required.[^5]

[^5]: Under the scenarios of a spatial mapping and with the common disclaimers about static control-flow analysis.

An important aspect of the implementation of a multiplexer associated with a $\phi$-function is the definition and evaluation of the predicate associated with each multiplexer's control (or selection) input signal. In the basic SSA representation, the selection predicates are not explicitly defined, as the execution of each $\phi$-function is implicit when the control flow reaches it. When mapping a computation to hardware, however, a $\phi$-function clearly elicits the need to define a predicate to be included as part of the hardware logic circuit that defines the value of the multiplexer circuit's selection input signal. To this effect, hardware mapping must rely on a variant of SSA, named gated SSA (see Chap. 14), which explicitly captures the symbolic predicate information in the representation.[^6] The generation of the hardware circuit simply uses the register that holds the corresponding variable's version value of the predicate. Figure 23.5 illustrates an example of a mapping using the information provided by the gated SSA form.

[^6]: As with any SSA representation, variable names fulfil the referential transparency.
![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310110256161.png)

When combining multiple predicates in the gated SSA form, it is often desirable to leverage the control-flow representation in the form of the program dependence graph (PDG) described in Chap. 14. In the PDG representation, basic blocks that share common execution predicates (i.e., both execute under the same predicate conditions) are linked to the same _region_ nodes. Nested execution conditions are easily recognized as the corresponding nodes are hierarchically organized in the PDG representation. As such, when generating code for a given basic block, an algorithm will examine the various region nodes associated with a given basic block and compose (using AND operators) the outputs of the logic circuits that implement the predicates associated with these nodes. If a hardware circuit already exists that evaluates a given predicate that corresponds to a given region, the implementation can simply reuse its output signal. This lazy code generation and predicate composition achieves the goal of hardware circuit sharing, as illustrated by the example in Fig. 23.6 where some of the details were omitted for simplicity. When using the PDG representation, however, care must be taken regarding the

![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310110301954.png)

potential lack of referential transparency. To this effect, it is often desirable to combine the SSA information with the PDG's regions to ensure correct reuse of the hardware that evaluates the predicates associated with each control-dependence region.

#### 23.3.4 $\phi$-Function and Multiplexer Optimizations

We now describe a set of hardware-oriented transformations that can be applied to potentially reduce the amount of hardware resources devoted to multiplexer implementation or to use multiplexers to change the temporal features of the execution and thus enable other aspects of hardware execution to be more effective (e.g., scheduling). Although these transformations are not specific to the mapping of computations to hardware, the explicit representation of the selection constructs in SSA makes it very natural to map and therefore manipulate/transform the resulting hardware circuit using multiplexers. Other operations in the intermediate representation (e.g., predicated instructions) can also yield multiplexers in hardware without the explicit use of SSA form.

A first transformation is motivated by a well-known result in computer arithmetic: integer addition scales with the number of operands. Building a large, unified
![Fig.23.5.svg](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310110305030.svg)
k-input integer addition circuit is more efficient than adding k integers two at a time. Moreover, hardware multipliers naturally contain multi-operand adders as building blocks: a partial product generator (a layer of AND gates) is followed by a multi-operand adder called a _partial product reduction tree_. For these reasons, there have been several efforts in recent years to apply high-level algebraic transformations to source code with the goal of merging multiple addition operations with partial product reduction trees of multiplication operations. The basic flavour of these transformations is to push the addition operators towards the outputs of a data-flow graph, so that they can be merged at the bottom. Examples of these transformations that use multiplexers are depicted in Fig. 23.7a and b. In the case of Fig. 23, the transformation leads to the fact that an addition is always executed, unlike in the original hardware design. This can lead to more predictable timing or more uniform power draw signatures.7 Figure 23 depicts a similar transformation that

![Fig. 23.7.svg](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310110305462.svg)

merges two multiplexers sharing a common input, while exploiting the commutative property of the addition operator. The SSA-based representation facilitates these transformations as it explicitly indicates which values (by tracing backward in the representation) are involved in the computation of the corresponding values. For the example in Fig. 23.7b, a compiler could quickly detect the variable a to be common to the two expressions associated with the $\phi$-function.

A second transformation that can be applied to multiplexers is specific to FPGA, whose basic building block consists of a k-input lookup table (LUT) logic element which can be programmed to implement any k-input logic function.[^8] For example, a 3-input LUT (3-LUT) can be programmed to implement a multiplexer with two data inputs and one selection bit. Similarly, a 6-LUT can be programmed to implement a multiplexer with four data inputs and two selection bits, thus enabling the implementation of a tree of 2-input multiplexers.

[^8]: A typical k-input LUT will include an arbitrary combinatorial functional block of those k inputs followed by an optional register element (e.g., Flip-Flop).

### 23.3.5 Implications of Using SSA Form in Floor Planning

For spatial oriented hardware circuits, moving a $\phi$-function from one basic block to another can alter the length of the wires that are required to transmit data from the hardware circuits corresponding to the various basic blocks. As the boundaries of basic blocks are natural synchronization points, where values are captured in hardware registers, the length of wires dictates the maximum allowed hardware clock rate for synchronous designs. We illustrate this effect via an example as depicted in Fig. 23.8. In this figure, each basic block is mapped to a distinct hardware unit, whose spatial implementation is approximated by a rectangle. A floor planning algorithm must place each of the units in a two-dimensional plane while ensuring that no two units overlap. As can be seen in Fig. 23.8a, placing block 4 on the right-hand side of the plane will result in several mid-range and one long-range wire connections. However, placing block 4 at the centre of the design would virtually eliminate all mid-range connections as all connections corresponding to the transmission of the values for variable $x$ are now next-neighbouring connections.

The same result could be obtained by moving the $\phi$-function to block 5, which is illustrated in Fig. 23.8b. But moving a multiplexer from one hardware unit to another can significantly change the dimensions of the resulting unit, which is not under the control of the compiler. Changing the dimensions of the hardware units fundamentally changes the placement of modules, so it is very difficult to predict whether moving a $\phi$-function will actually be beneficial. For this reason, compiler optimizations that attempt to improve the physical layout must be performed using a feedback loop so that the results of the lower-level CAD tools that produce the layout can be reported back to the compiler.


![Fig. 23.8.svg](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310110306556.svg)


## 23.4 Existing SSA-Based Hardware Compilation Efforts

Several research projects have relied on SSA-based intermediate representations that leverage control- and data-flow information to exploit fine-grain parallelism. Often, but not always, these efforts have been geared towards mapping computations to fine-grain hardware structure, such as the ones offered by FPGAs.

The standard approach followed in these compilers has been to translate a high-level programming language such as Java, in the case of the Sea Cucumber [288] compiler, or C, in the case of the DEFACTO [105] and ROCCC [133] compilers that translate C code to sequences of intermediate instructions. These sequences are then organized in basic blocks that compose the control-flow graph (CFG). For each basic block a data-flow graph (DFG) is typically extracted followed by a conversion into SSA representation, possibly using predicates associated with the control flow in the CFG, thus explicitly using Predicated SSA representation (see the gated SSA representation [290] and the Predicate SSA [58; 104; 275]).

As an approach to increase the potential amount of exploitable ILP at the instruction level, many of these efforts (as well as others such as the earlier Garp compiler [55]) restructure the CFG into hyper-blocks [191]. A hyper-block consists in single-entry multi-exit regions derived from the aggregation of multiple basic blocks, thus serializing longer sequences of instruction. As not all instructions are executed in a hyper-block (due to early exit of a block), hardware circuit implementation must rely on predication to exploit the potential for additional ILP.

Figure 23.8: Example of the impact of $\phi$-function movement in reducing hardware wire lengthThe CASH compiler [54] uses an augmented predicated SSA representation with tokens to explicitly express synchronization and handle _may-dependencies_, thus supporting speculative execution. This fine-grain synchronization mechanism is also used to serialize the execution of consecutive hyper-blocks, thus greatly simplifying the code generation. Other efforts also exploit instruction-level optimizations or algebraic properties of the operators for minimization of expensive hardware resources such as multipliers, adders, and in some cases even multiplexers [198, 298]. For a comprehensive description of a wide variety of hardware-oriented high-level program transformations, the reader is referred to [57].

## Further Reading

Despite their promise in terms of high performance and high computational efficiency, hardware devices such as FPGAs have long been beyond the reach of the "average" software programmer. To effectively program them using hardware-oriented programming languages such as VHDL [12], Verilog [283], OpenCL [173], or SystemC [218], the developers must assume the role of both software and hardware designs.

To address the semantic gap between a hardware-oriented programming model and high-level software programming models, various research projects, first in academia and later in industry, developed prototype tools that could bridge this gap and make the promising technology of configurable logic approachable among a wider audience of programmers. In these efforts, loosely labeled C-to-Gates, compilers performed the traditional phases of program data- and control-dependence analysis to uncover opportunities for concurrent and/or pipelined execution and directly translated the underlying data flow to Verilog/VHDL description alongside the corresponding control logic. In practice, these compilers, of which Vivado HLS [310] and LegUp [56] are notable examples, focus on loop constructs with significant execution time weights (the so-called hot-spots) for which they automatically derive hardware pipelined implementations (often guided by user-provided compilation directives) that efficiently executed them. When the target architecture is a "raw" FPGA (rather than an overlay architecture), this approach invariably leads to long compilation and synthesis times.

The inherent difficulties and limitations in extracting enough instruction-level Parallelism in these approaches, coupled with the increase in the devices' capacities (e.g. Intel's Arria [147] and Xilinx's Virtex UltraScale+ [311]), have prompted a search for programming models with a more natural concurrency that would facilitate the mapping of high-level computation to hardware. One such example is MapReduce[95], originally introduced by Google to naturally distribute concurrent jobs across clusters of servers [95], which has been an effective programming model for FPGAs [315]. Similarly, high-level languages based on parallel models of computation such as synchronous data flow [179] or functional single-assignment languages have also been shown to be good choices for hardware compilation [137, 145, 33].
