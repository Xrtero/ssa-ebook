# Chapter 22. Register Allocation
**Florent Bouchez Tichadou and Fabrice Rastello**




Register allocation maps the variables of a program to physical memory locations: usually either CPU registers or the main memory. The compiler determines the location for each variable and each program point. Ideally, as many operations as possible should draw their operands from processor registers without loading them from memory beforehand. As there is only a small number of registers available in a CPU (with usual values ranging from 8 to 128), it is usually not possible to only use registers, and the task of register allocation is also to decide which variables should be evicted from registers and at which program points to store and load them from memory (spilling).

Furthermore, register allocation has to remove spurious copy instructions (copy coalescing) inserted by previous phases in the compilation process and to deal with allocation restrictions that the instruction set architecture and the run time system impose (register targeting). Classical register allocation algorithms address those different issues with either complex and sometimes expensive schemes (usually graph-based) or simpler and faster (but less efficient) algorithms such as linear scan.

The goal of this chapter is to illustrate how SSA form can help in designing both simpler and faster schemes with similar or even better quality than the most complex existing ones.


## 22.1 Introduction

Let us first review the basics of register allocation, to help us understand the choices made by graph-based and linear scan style allocations.

Register allocation is usually performed per procedure. In each procedure, a liveness analysis (see Chap. 9) determines for each variable the program points where the variable is live. The set of all program points where a variable is live is called the _live range_ of the variable, and all along this live range, storage needs to be allocated for that variable, ideally a register. When two variables "exist" at the same time, they are conflicting for resources, i.e., they cannot reside in the same location.

This resource conflict of two variables is called _interference_ and is usually defined via liveness: two variables interfere if (and only if) there exists a program point where they are simultaneously live, i.e., their live ranges intersect.[^1] It represents the fact that those two variables cannot share the same register. For instance, in Fig. 22.1, variables $a$ and $b$ interfere as $a$ is live at the definition of $b$.

[^1]: This definition of interference by liveness is an over-approximation (see Sect. 2.6 of Chap. 2), and there are refined definitions that create less interferences (see Chap. 21). However, in this chapter, we will restrict ourselves to this definition and assume that two variables whose live ranges intersect cannot be assigned the same register.

### 22.1.1 Questions for Register Allocators

There are multiple questions that arise at that point that a register allocator has to answer to:

* Are there enough registers for all my variables? (_spill test_)
* If yes, how do I choose which register to assign to which variable? (_assignment_)
* If no, how do I choose which variables to store in memory? (_spilling_)

Without going into the details, let us see how linear scan and graph-based allocators handle these questions. Figure 22.1 will be used in the next paragraphs to illustrate how these allocators work.

#### Linear Scan

The linear scan principle is to consider that a procedure is a long basic block and, hence, live ranges are approximated as intervals. For instance, the procedure shown in Fig. 22.1b is viewed as the straight-line code of Fig. 22.1a. The algorithm then proceeds in scanning the block from top to bottom. When encountering the definition of a variable (i.e., the beginning of a live range), we check if some registers are free (_spill test_). If yes, we pick one to assign the variable to (_assignment_). If no, we choose from the set of currently live variables the one thathas the farthest use and spill it (_spilling_). When we encounter the end of its live range (e.g., a last use), we free the register it was assigned to. On our example, we would thus greedily colour along the following order: $p$, $a$, $b$, $x$, and finally $y$. When the scan encounters the first definition of $y$ in the second basic block, four other variables are live and a fifth colour is required to avoid spilling.

![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310110229357.png)

#### **Graph-Based**

Graph-based allocators, such as the "Iterated Register Coalescing" allocator (IRC), represent interferences of variables as an undirected _interference graph_: the nodes are the variables of the program, and two nodes are connected by an edge if they interfere, i.e., if their live ranges intersect. For instance, Fig. 22.1c shows the interference graph of the code example presented in Fig. 22.1b. In this model, two neighbouring nodes must be assigned different registers, so the assignment of variables to registers amounts to _colouring_ the graph--two neighbouring nodes must have a different colour--using at most $R$ colours, the number of registers.[^2]

[^2]: Hence, the terms “register” and “colour” will be used interchangeably in this chapter.

Here, the allocator will try to colour the graph. If it succeeds (_spill test_), then the colouration represents a valid assignment of registers to variables (_assignment_). If not, the allocator will choose some nodes (usually the ones with the highest number of neighbours) and remove them from the graph by storing the corresponding variables in memory (_spilling_).

On our example, one would need to use four different colours for $a$, $b$, $p$, and $x$, but $y$ could use the same colour as $a$ or $b$.

Figure 22: Linear scan makes an over-approximation of live ranges as intervals, while graph-based allocator creates an interference graph capturing the exact interferences. Linear scan requires 5 registers in this case, while colouring the interference graph can be done with 4 registers

#### Comparison

Linear scan is a very fast allocator where a procedure is modelled as a straight-line code. In this model, the colouring scheme is considered to be optimal. However, the model itself is very imprecise: Procedures generally are not just straight-line codes but involve complex flow structures such as if-conditions and loops. The live ranges are artificially longer and produce more interferences than there actually are. If we look again at the example Fig. 22.1, we have a simple code with an if-condition. Linear scan would decide that, because four variables are live at the definition $y\gets b$, it needs five registers (spill test). But one can observe that $a$ is actually not live at that program point: Modelling the procedure as a straight-line code artificially increases the live ranges of variables.

On the other hand, a graph-based allocator has a much more precise notion of interference. Unfortunately, graph $k$-colouring is known to be an NP-complete problem. Control-flow structures create cycles in the interference graph that can get arbitrarily complex. The allocator uses a heuristic for colouring the graph and will base its spill decisions on this heuristic.

In our example, IRC would create the graph depicted in Fig. 22.1c, which includes a 4-clique (i.e., a complete sub-graph of size 4, here with variables $a$, $b$, $p$, and $x$), and, hence, would require at least 4 colours. This simple graph would actually be easily 4-colourable with a heuristic; hence, the spill test would succeed with four registers.

Still, one could observe that at each point of the procedure, no more than three variables are simultaneously live. However, since $x$ interferes with $b$ on the left branch and with $a$ on the right branch, with the model used by IRC, it is indeed impossible to use only three registers.

The question we raise here is: can't we do better? The answer is yes, as depicted in Fig. 22.2a. If it was possible for $x$ to temporarily use the same register as $b$ in the right branch as long as $a$ lives (short live range denoted $x^{\prime}$ in the figure), then $x$ could use the same colour as $a$ (freed after $a$'s last use). In this context, three registers are enough. In fact, one should expect that, for a code where only three variables are live at any program point, it should be possible to register allocate without spilling with only three registers and proper reshuffling of variables in registers from time to time.

So what is wrong with the graph colouring based scheme we just described? We will develop below that its limitation is mostly due to the fact that it arbitrarily enforces all variables to be assigned to only one register for their entire live range.

In conclusion, linear scan allocators are faster, and graph colouring ones have better results in practice, but both approaches have an inexact spill test: linear scan has artificial interferences, and graph colouring uses a colouring heuristic. Moreover, both require variables to be assigned to _exactly one register_ for all their live ranges. This means both allocators will potentially spill more variables than strictly necessary. We will see how SSA can help with this problem.

![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310110236273.png)

### 22.1.2 Maxlive and Variable Splitting

The number of simultaneously live variables at a program point is called the _register pressure_ at that program point.[^3] The maximum register pressure over all program points in a procedure is called the register pressure of that procedure, or "_Maxlive._" One can observe that _Maxlive_ expresses the _minimum_ number of required registers for a spill-free register assignment, i.e., an allocation that does not require memory. For instance, in Fig. 22.1b, $\textit{Maxlive}=3$, so a minimum number of 3 registers is required. If a procedure is restricted to a single basic block (straight-line code), then _Maxlive_ also constitutes a _sufficient number of registers_ for a spill-free assignment. But in general, a procedure is made of several basic blocks, and under the standard model described so far, an allocation might require _more than Maxlive registers_ to be spill-free.

[^3]: See Fig. 13.1 of Chap. 13 for a visualization of program points.

This situation changes if we permit _live range splitting_. This means inserting a variable-to-variable copy instruction at a program point that creates a new version of a variable. Thus, the value of a variable is allowed to reside in different registers at different times. For instance, in Fig. 22.1b, we can split $x$ in the right branch by changing the definition to $x^{\prime}$ and using a copy $x\leftarrow x^{\prime}$ at the end of the block, producing the code shown in Fig. 22.2a. It means the node $x$ in the graph is split into two nodes, $x$ and $x^{\prime}$. Those nodes do not interfere, and also interfere differently with the other variables: Now, $x$ can use the same register as $a$ because only $x^{\prime}$ interferes with $a$. Conversely, $x^{\prime}$ can use the same register as $b$, because only $x$ interferes with $b$. In this version, we only need $\textit{Maxlive}=3$ registers, which means that, if the

Figure 22.2: Splitting variable $x$ in the previous example breaks the interference between $x$ and $a$. By using copies between $x/x^{\prime}$ and $y/a$ in parallel, now only 3 registers are required on the left. SSA introduces splitting that guarantees _Maxlive_ registers are enoughnumber of registers was tight (i.e., $R=3$), we have traded a spill (here one store and one load) for one copy, which is an excellent bargain.

This interplay between live range splitting and colourability is one of the key issues in register allocation, and we will see in the remainder of this chapter how SSA, which creates live range splitting at particular locations, can play a role in register allocation.

### 22.1.3 The Spill Test Under SSA

As said above, a register allocation scheme needs to address the following three sub-problems: spill test, assignment, and spilling. We focus here on the spill test, that is, verify whether there are enough registers for all variables without having to store any of them in memory.

As already mentioned in Sect. 2.3 of Chap. 2, the live ranges in an SSA-form program with dominance property have interesting structural properties: In that flavour, SSA requires that all uses of a variable are dominated by its definition. Hence, the whole live range is dominated by the definition of the variable. Dominance, however, induces a tree on the control-flow graph (see for instance the dominance edges of Fig. 4.1 in Chap. 4). Thus, the live ranges of SSA variables are all tree-shaped. They can branch downward on the dominance tree but have a single root: the program point where the variable is defined. Hence, a situation like in Fig. 22.1 can no longer occur: $x$ and $y$ had two "roots" because they were defined twice. Under SSA form, the live ranges of those variables are split by $\phi$-functions, which creates the code shown in Fig. 22.2b, where we can see that live ranges form a "tree." The argument and result variables of the $\phi$-functions constitute new live ranges, giving more freedom to the register allocator since they can be assigned to different registers.

This structural property is interesting as we can now perform exact polynomial colouring schemes that work both for graph-based and linear-style allocators.

#### Graph-Based

Graph-based allocators such as the IRC mentioned above use a _simplification scheme_ that works quite well in practice but is a heuristic for colouring general graphs. We will explain it in more detail in Sect. 22.3.1, but the general idea is to remove from the interference graph nodes that have strictly less than $R$ neighbours, as there will always be a colour available for them. If the whole graph can be simplified, nodes are given a colour in the reverse order of their simplification. We also call this method the _greedy colouring scheme_. On our running example, the interference graph of Fig. 22.1c, candidates for simplification with $R=4$ would be nodes with strictly less than 4 neighbours, that is, $a$, $b$, or $y$. As soon as one of them is simplified (removed from the graph), $p$ and $x$ now have only 3 neighbours and can also be simplified. So a possible order would be to simplify $a$, then $y$, $p$, $b$, and finally $x$. Colours can be greedily assigned during the reinsertion of nodes in the graph in reverse order, starting from $x$, and ending with $a$: When we colour $a$, all its 3 neighbours already have a colour and we assign it to the fourth colour.

Interestingly, under SSA, the interference graph becomes a _chordal graph_.[^4] The important nice property about chordal graphs is that they can be coloured minimally in polynomial time. Even more interesting is the fact that the simplification scheme used in IRC is optimal for such graphs and will always manage to colour it with _Maxlive_ colours! Thus the same colouring algorithm can be used, _without any modification_, and now becomes an exact spill test: Spilling is required if and only if the simplification scheme fails to completely simplify (hence, colour) the graph.
![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310110237272.png)

[^4]: In a chordal graph, also called a triangulated graph, every cycle of length 4 or more has (at least) one chord (i.e., an edge joining two non-consecutive edges in the cycle).

Under SSA, the live ranges are intervals that can "branch" but never "join." This allows for a simple generalization of the linear scan mentioned above that we call the _tree scan_. As we will see, the tree scan always succeeds in colouring the tree-shaped live ranges with _Maxlive_ colours. This greedy assignment scans the dominance tree, colouring the variables from the root to the leaves in a top-down order. This means the variables are simply coloured in the order of appearance of their respective definition. On our example (Fig. 22.2b), tree scan would colour the variables in the following order: $p_{1}$, $a_{1}$, $b_{1}$, $x_{1}$, $y_{1}$, $x_{2}$, $y_{2}$, $x_{3}$, $y_{3}$. This works because branches of the tree are independent, so colouring one will not add constraints on other parts of the tree, contrary to the general non-SSA case that may expose cycles.

The pseudo-code of the tree scan is shown in Algorithm 22.1. In this pseudo-code, the program points $p$ are processed using a depth-first search traversal of the dominance tree $T$. For a colour $c$, available[$c$] is a boolean that expresses if $c$ is available for the currently processed point. Intuitively, when the scanning arrives at the definition of a variable, the only already coloured variables are "above" it, and since there is at most $\mathit{Maxlive}-1$, other variables live at this program point, and there is always a free colour. As an example, when colouring $x_{1}$, the variables live at its definition point are $p_{1}$ and $b$ and are already coloured. So a third colour, different from the ones assigned to $p_{1}$ and $b$, can be given to $x_{1}$.

#### Conclusion

The direct consequence is that, as opposed to general form programs, and whether we consider graph-based or scan-based allocators, the only case where spilling is required is when $\mathit{Maxlive}>R$, i.e., when the maximum number of simultaneously live variables is greater than the number of available registers. In other words, there is no need to try to colour the interference graph to check if spilling is necessary or not: the spill test for SSA-form programs simplifies to simply computing $\mathit{Maxlive}$ and then comparing it to $R$.

This allows to design a register allocation scheme where spilling is decoupled from colouring: First, lower the register pressure to at most $R$ everywhere in the program. Then, colour the interference graph with $R$ colours in polynomial time.

## 22.2 Spilling

We have seen previously that, under SSA, it is easy to decide in polynomial time whether there is enough registers or not, simply by checking if $\mathit{Maxlive}\leq R$, the number of registers. The goal of this section is to present algorithms that will lower the register pressure when it is too high, i.e., when $\mathit{Maxlive}>R$, by _spilling_ (assigning) some variables to memory.

Spilling is handled differently depending on the allocator used. For a scan-based allocator, the spilling decision happens when we are at a particular program point. Although it is actually a bit more complex, the idea when spilling a variable $v$ is to insert a store at that point, and a load just before its next use. This process leads to spilling only a part of the live range. On the other end, a graph-based allocator has no notion of program points since the interferences have been combined in an abstract structure: the interference graph. In the graph colouring setting, spilling means removing a node of the interference graph and thus the _entire_ live range of a variable. This is a called a _spill-everywhere_ strategy, which implies inserting load instructions in front of every use and storing instructions after each definition of the variables. These loads and stores require temporary variables that were not present in the initial graph. These temporary variables also need to be assigned to registers. This implies that whenever the spilling/colouring is done, the interference graph has to be rebuilt and a new pass of allocation is triggered, until no variable is spilled anymore: this is where the "Iterated" comes from in the IRC name.

In this section, we will consider the two approaches: the graph-based approach with a spill-everywhere scheme, and the scan-based approach that allows partial live range spilling. In both cases, we will assume that the program was in SSA before spilling. This is important to notice that there are pros and cons of assuming so. In particular, the inability to coalesce or move the shuffle code associated to $\phi$-functions can lead to spurious load and store instructions on CFG edges. Luckily, these can be handled by a post-pass of partial redundancy elimination (PRE, see Chap. 11), and we will consider here the spilling phase as a full-fledged SSA program transformation.

Suppose we have $R$ registers, the objective is to establish $\mathit{Maxlive}\leq R$ ($\mathit{Maxlive}$ lowering) by inserting loads and stores into the program. Indeed, as stated above, lowering $\mathit{Maxlive}$ to $R$ ensures that a register allocation with $R$ registers can be found in polynomial time for SSA programs. Thus, spilling should take place before registers are assigned $\mathit{and}$ yield a program in SSA form. In such a decoupled register allocation scheme, the spilling phase is an optimization problem for which we define the following constraints and objective function:

* The _constraints_ that describe the universe of possible solutions express that the resulting code should be $R$-colourable.
* The _objective function_ expresses the fact that the (weighted) amount of inserted loads and stores should be minimized.

The constraints directly reflect the "spill test" that expresses whether more spilling is necessary or not. The objective is expressed with the profitability test: among all variables, which one is more profitable to spill? The main implication of spilling in SSA programs is that the spill test--which amounts to checking whether $\mathit{Maxlive}$ has been lowered to $R$ or not--becomes precise.

The other related implication of the use of SSA form follows from this observation: consider a variable such that for any program point in its entire live range the register pressure is at most $R$, and then spilling this variable is useless with regard to the colourability of the code. In other words, spilling such a variable will never be profitable. We will call this yes-or-no criteria, enabled by the use of SSA form, the "usefulness test."

We will see now how to choose, among all "useful" variables (with regard to the colourability), the ones that seem most profitable. In this regard, we present in the next section how SSA allows to better account for the program structure in the spilling decision even in a graph-based allocator, thanks to the enabled capability to decouple spilling (allocation) to colouring (assignment). However, register allocation under SSA shines the most in a scan-based setting, and we present guidelines to help the spill decisions in such a scheme in Sect. 22.2.2.

### 22.2.1 Graph-Based Approach

In a graph-based allocator such as the IRC, a _spill-everywhere_ strategy is used: a variable is either spilled completely or not at all, and loads are placed directly in front of uses and stores directly after the definition. When spilled, the live range of a variable then degenerates into small intervals: one from the definition and the store, and one from each load to its subsequent use. However, even in this simplistic setting, it is NP-complete to find the minimum number of nodes to establish $\mathit{Maxlive}\leq R$. In practice, heuristics such as the one in IRC spill variables (graph nodes) greedily using a weight that takes into account its node degree (the number of interfering uncoloured variables) and an estimated spill cost (estimated execution frequency of inserted loads and stores). Good candidates are high-degree nodes of low spill cost, as this means they will lessen colouring constraints on many nodes--their neighbours--while inserting few spill codes.

The node degree represents the profitability of spilling the node in terms of colourability. It is not very precise as it is only a graph property, independent of the control-flow graph. We can improve this criteria by using SSA to add a usefulness tag. We will now show how to build this criteria and how to update it.

We attach to each variable $v$ a "useful" tag, an integer representing the number of program points that would benefit from spilling $v$, i.e., $v.\mathit{useful}$ expresses the number of program points that belong to the live range of $v$ and for which the register pressure is strictly greater than $R$.

#### Building the "Useful" Criteria

We will now explain how the "useful" tag can be built at the same time as the interference graph. Under SSA, the interference graph can be built through a simple _bottom-up traversal_ of the CFG. When encountering the last use of a variable $p$, $p$ is added to the set of currently live variables (live set) and a corresponding node (that we also call $p$) is added in the graph. Arriving at the definition of $p$, it is removed from the current live set, and edges (interferences) are added to the graph: for all variables $v$ in the live set, there is an edge ($v\rightarrow\,p$). Note that, as opposed to standard interference graphs, we consider directed edges here, where the direction represents the dominance. At that point, we also associate the following fields to node $p$ and its interferences:

* $p.\mathit{pressure}$ that corresponds to the number of variables live at the definition point of $p$, that is, $|\mathrm{live\text{-}set}|+1$
* ($v\rightarrow\,p$).$\mathit{high}$, a boolean set to true if and only if $p.\mathit{pressure}>R$, meaning this interference belongs to a clique of size more than $R$

We then create the "usefulness" field of $p$. If $p.\mathit{pressure}\leq R$, then $p.\mathit{useful}$ is set to 0. Otherwise, we do the following:

* $p.\mathit{useful}$ is set to 1.
* For all ($v\rightarrow\,p$), $v.\mathit{useful}$ gets incremented by 1.

At the end of the build process, $v.\mathit{useful}$ expresses the number of program points that belong to the live range of $v.\mathit{var}$ and for which the register pressure is greater than $R$. More precisely,
![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310110240138.png)
With this definition, if $v.\mathit{useful}=0$, then it can be considered to be useless to spill $v$, as it will not help in reducing the register pressure. If not, it means that $v$ belongs to this number of cliques of size greater than $R$. Since at least one of the nodes of those cliques must be spilled, spilling $v$ is useful as it would reduce the size of each of those cliques by one. Existing colouring scheme such as the IRC can be modified to use the _useful_ tag when making spill decisions, the higher the better.

#### Updating the "useful" Criteria

Whenever a node $n$ is spilled (assume only useful nodes are spilled), those additional fields of the interference graph must be updated as follows:

1. If $n.\mathit{pressure}>R$, then for all its incoming edges ($v\to n$), $v.\mathit{useful}$ is decremented by one.
2. For all its outgoing edges ($n\to p$) such that $(n\to p).\mathit{high}=\mathsf{true}$, $p.\mathit{pressure}$ is decremented by one; if, following this decrement, $p.\mathit{pressure}\leq R$; then for all the incoming edges ($v\to p$) of $p$, $v.\mathit{useful}$ is decremented by one.

#### Thoughts on Graph-Based Spilling

In an existing graph-based allocation, SSA can bring information to better help the allocator in making its spill decisions. However, with the spilling and colouring fully decoupled, encoding the information using a graph does not seem as pertinent. Moreover, formulations like _spill everywhere_ are often not appropriate for practical purposes, as putting the whole live range to memory is too simplistic. A variable might be spilled because at some program point the pressure is too high; however, if that same variable is later used in a loop where the register pressure is low, a spill everywhere will place a superfluous (and costly!) load in that loop. Spill-everywhere approaches try to minimize this behaviour by adding costs to variables, to make the spilling of such a variable less likely. These bring in a flow-insensitive information that a variable resides in a frequently executed area, but such approximations are often too coarse to give good performances. Hence, it is imperative to cleverly split the live range of the variable according to the _program structure_ and spill only parts of it, which is why we prefer to recommend a scan-based approach of the register allocation under SSA.

### 22.2.2 Scan-Based Approach

In the context of a basic block, a simple algorithm that gives good results is the "furthest first" algorithm that is presented in Algorithm 22.2. The idea is to scan the block from top to bottom: whenever the register pressure is too high, we will spill the variable whose next use is the furthest away, and it is spilled only _up to this next use_. In the _evict_ function of Algorithm 22.2, this corresponds to maximizing distance_to_next_use_after($p$). Spilling this variable frees a register for the longest time, hence diminishing the chances to have to spill other variables later. This algorithm is not optimal because it does not take into account the fact that the first time we spill a variable is more costly than subsequent spills of the same variable (the first time, a store and a load are added, but only a load must be added afterwards). However, the general problem is NP-complete, and this heuristic, although it may produce more stores than necessary, gives good results on "straight-line codes," i.e., basic blocks.

![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310110241025.png)

We now present an algorithm that extends this "furthest first" algorithm to general control-flow graphs. The idea is to scan the CFG using a topological order and greedily evict sub-live ranges whenever the register pressure is too high. There are two main issues:

1. Generalize the priority function distance_to_next_use_after to a general CFG.
2. Find a way to initialize the "in_regs" set when starting to scan a basic block, in the situation where direct predecessor basic blocks have not been processed yet (e.g., at the entry of a loop).

![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310110242112.png)

#### Profitability to Spill

To illustrate the generalization of the further-first priority strategy to a CFG, let us consider the example of Fig. 22.3. In this figure, the $\dot{\iota}+n$ sign denotes regions with (high) register pressure of $R+n$. At program point $p_{0}$, register pressure is too high by one variable (suppose there are other hidden variables that we do not want to spill). We have two candidates for spilling: $x$ and $y$, and the classical furthest first criteria would be different depending on the chosen branch:

* If the left branch is taken, considering an execution trace ($AB^{100}C$): In this branch, the next use of $y$ appears in a loop, while the next use of $x$ appears way further, after the loop has fully executed. It is more profitable to evict variable $x$ (at distance 101).
* If the right branch is taken, the execution trace is ($AD$). In that case, this is variable $y$ that has the further use (at distance 2) so we would evict variable $y$.

We have two opposing viewpoints, but looking at the example as a whole, we see that the left branch is not under pressure, so spilling $x$ would only help for program point $p_{0}$, and one would need to spill another variable in block $D$ ($x$ is used at the beginning of $D$); hence, it would be preferable to evict variable $y$.

On the other hand, if we modify a little bit the example by assuming a high register pressure within the loop at program point $p_{1}$ (by introducing other variables), then evicting variable $x$ would be preferred in order to avoid a load and store in a loop!

This dictates the following remarks:

1. Program points with low register pressure can be ignored.
2. Program points within loops, or more generally with higher execution frequency, should account in the computation of the "distance" more than program points with lower execution frequency.

Figure 22.3: Generalization of distance_to_next_use_after for a CFG. Illustrating exampleWe will then replace the notion of "distance" in the furthest first algorithm with a notion of "profitability," that is, a measure of the number of program points (weighted by frequency) that would benefit from the spilling of a variable $v$.

**Definition 22.1 (Spill Profitability from $p$)**: Let $p$ be a program point and $v$ a variable live at $p$. Let $v.\text{HP}(p)$ (high pressure) be the set of all program points $q$ such that: 1. Register pressure at $q$ is strictly greater than $R$. 2. $v$ is live at $q$. 3. There exists a path from $p$ to $q$ that does not contain any use or definition of $v$. Then,

\[v.\text{spill\_profitability}(p)=\sum_{q\in v.\textsc{HP}(p)}q.\text{frequency}.\]

There is an important subtlety concerning the set of points in $v.\text{HP}(p)$ that is dependent on the instruction set architecture and the way spill code is inserted. Consider in our example the set $x.\text{HP}(p_{0})$, that is, the set of high-pressure points that would benefit from the spilling of $x$ at point $p_{0}$. The question is the following: "does the point $p_{3}$ (just before the instruction using $x$) belong to this set?" If the architecture can have an operand in memory, $p_{3}$ belongs to the set. However, if the architecture requires an operand to be loaded in a register before its use, then $x$ would be reloaded at $p_{3}$ so this point would not benefit from the spilling of $x$.

Let us see how the profitability applies to our running example with two scenarios: low pressure and high pressure in the $B$ loop at program point $p_{1}$. We also assume that an operand should be loaded in a register before its use. In particular, $p_{3}$ must be excluded from $x.\text{HP}(p_{0})$.

![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310110243651.png)

In the first scenario, we would evict $y$ with a profitability of 1.5. In the second, we would evict $x$ with a profitability of 51 (plus another variable later, when arriving at $p_{3}$), which is the behaviour we wanted in the first place.

#### Initial Register Filling at the Beginning of a Basic Block

When visiting basic block $B$, the set of variables that must reside in a register is stored in $B.\text{in\_regs}$. For each basic block, the initial value of this set has to be computed before we start processing it. The heuristic for computing this set is different for a "regular" basic block and for a loop entry. For a regular basic block,as we assume a topological order traversal of the CFG, all its direct predecessors will have been processed. Live-in variables fall into three sets:

1. The ones that are available at the exit of all direct predecessor basic blocks:
![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310110244893.png)

2. The ones that are available in some of the direct predecessor basic blocks: 
![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310110244570.png)
3. The ones that are available in none of them.

As detailed in Algorithm 22.3, $B$.in_regs is initialized with _allpreds_in_regs_ plus, as space allows, elements of somepreds_in_regs sorted in decreasing order of their _spill_profitability_ metric. Note that this will add shuffle code (here, loads) on the previous basic blocks or on the corresponding incoming edges that are critical. In practice, this can be handled in a post-pass, in a similar fashion as during the SSA destruction presented in Chap. 21: indeed, lowering the $\phi$-functions will create shuffle code that must be executed prior to entering the basic block.
![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310110245450.png)
For a basic block at the entry of a loop, as illustrated by the example of Fig. 22.4, one does not want to account for allocation on the direct predecessor basic block but starts from scratch instead. Here, we assume the first basic block has already been processed, and one wants to compute $B$.in_regs:

1. Example (a): Even if at the end of the direct predecessor basic block, at $p_{0}$, $x$ is not available in a register, one wants to insert a reload of $x$ at $p_{1}$, that is, include $x$ in $B$.in_regs. Not doing so would involve a reload at every iteration of the loop at $p_{2}$.
2. Example (b): Even if at the entry of the loop, $x$ is available in a register, one wants to spill it at $p_{1}$ and restore at $p_{4}$ so as to lower the register pressure that is too high within the loop. Not doing so would involve a store in the loop at $p_{2}$ and a reload on the back edge of the loop at $p_{3}$. This means excluding $x$ from $B$.in_regs.

![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310110245049.png)

This leads to Algorithm 22.4 where $B$.livein represents the set of live-in variables of $B$ and $L$._Maxlive_ is the maximal register pressure in the whole loop $L$. Init_inregs first fills $B$.in_regs with live-in variables that are used within the loop $L$. Then, we add live-through variables to $B$.in_regs, but only those that can survive the loop: If $L$._Maxlive_$>R$, then $L$._Maxlive_$-R$ variables will have to be spilled (hopefully some live-through variables), so no more than $|B$.livein$|-(L$._Maxlive_$-R)$ are allocated to a register at the entry of $B$.

![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310110246935.png)



#### Putting All Together

The overall algorithm for spilling comprises several phases: First, we pre-compute both liveness and profitability metrics. Then we traverse the CFG in topological order, and each basic block is scanned using the initial value of $B$.in_regs as explained above. During this phase, we maintain the sets of live variables available in register and in memory at basic block boundaries. The last phase of the algorithms handles the insertion of shuffle code (loads and stores) where needed.

## 22.3 Colouring and Coalescing

We advocate here a decoupled register allocation: First, lower the register pressure so that $\mathit{Maxlive}\leq R$; second, assign variable to registers. Live range splitting ensures that, after the first phase is done, no more spilling will be required as $R$ will be sufficient, possibly at the cost of inserting register-to-register copies.

We already mentioned in Sect. 22.1.3 that the well-known "Iterated Register Coalescing" (IRC) allocation scheme, which uses a simplification scheme, can take advantage of the SSA-form property. We will show here that, indeed, the underlying structural property makes a graph colouring simplification scheme (recalled below) an "optimal" scheme. This is especially important because, besides minimizing the amount of spill code, the second objective of register allocation is to perform a "good coalescing," that is, try to minimize the amount of register-to-register copies: a decoupled approach is practically viable if the coalescing phase is effective in merging most of the live ranges, introduced by the splitting from SSA, by assigning live ranges linked by copies to the same register.

In this section, we will first present the traditional graph colouring heuristic, based on a simplification scheme, and show how it successfully colours programs under SSA form. We will then explain in greater detail the purpose of coalescing and how it translates when performed on SSA-form program. Finally, we will show how to extend the graph-based (from IRC) and the scan-based (of Algorithm 22.1) greedy colouring schemes to perform efficient coalescing.

### 22.3.1 Greedy Colouring Scheme

In traditional graph colouring register allocation algorithms, the assignment of registers to variables is done by colouring the interference graph using a simplification scheme. This result in what is called a _greedy colouring scheme_: this scheme is based on the observation that, given $R$ colours--representing the registers, if a node in the graph has at most $R-1$ neighbours, there will always be one colour available for this node whatever colours the remaining nodes have. Such a node can be _simplified_, that is, removed from the graph and placed on a stack. This process can be iterated with the remaining nodes, whose degree may have decreased. If the graph becomes empty, we know it is possible to colour the graph with $R$ colours, by assigning colours to nodes in the reverse order of their simplification, that is, popping nodes from the stack and assigning them one available colour. This is always possible since they have at most $R-1$ coloured neighbours. We call the whole process the _greedy colouring scheme_: Its first phase, whose goal is to eliminate nodes from the graph to create the stack, is presented in Algorithm 22.5, while the second phase, which assigns variables to registers, is presented in Algorithm 22.6.

The greedy colouring scheme is a colouring heuristic for general graphs, and as such, it can fail. The simplification can get stuck whenever all remaining nodes havedegree at least $R$. In that case, we do not know whether the graph is $R$-colourable or not. In traditional register allocation, this is the trigger for spilling some variables so as to unstuck the simplification process. However, under the SSA form, if spilling has already been done so that the maximum register pressure is at most $R$, the greedy colouring scheme can never get stuck! We will not formally prove this fact here but will nevertheless try to give insight as to why this is true.

![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310110247411.png)
![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310110248174.png)

The key to understanding that property is to picture the dominance tree, with live ranges are sub-trees of this tree, such as the one in Fig. 22.2b. At the end of each dangling branch, there is a "leaf" variable: the one that is defined last in this branch. These are the variables $y_{1}$, $y_{2}$, $y_{3}$, and $x_{3}$ in Fig. 22.2b. We can visually see that this variable will not have many intersecting variables: those are the variables alive at its definition point, i.e., no more than $\mathit{Maxlive}-1$, hence less than $R-1$. In Fig. 22.2b, with $\mathit{Maxlive}=3$, we see that each of them has no more than two neighbours.

Considering again the greedy scheme, this means each of them is a candidate for simplification. Once removed, another variable will become the new leaf of that particular branch (e.g., $x_{1}$ if $y_{1}$ is simplified). This means simplification can always happens at the end of the branches of the dominance tree, and the simplification process can progress upward until the whole tree is simplified.

In terms of graph theory, the general problem is knowing whether a graph is $k$-colourable or not. Here, we can define a new class of graphs that contains the graphs that can be coloured with this simplification scheme.

**Definition 22.2**: A graph is greedy-$k$-colourable if it can be simplified using the Simplify function of Algorithm 22.5.

We could prove the following theorem:

**Theorem 1**: _Setting $k=\mathit{Maxlive}$, the interference graph of a code under SSA form is always greedy-$k$-colourable._

This tells us that, if we are under SSA form and the spilling has already been done so that $\mathit{Maxlive}\leq R$, the classical greedy colouring scheme is _guaranteed_ to perform register allocation with $R$ colours without any additional spilling, as the graph is greedy-$R$-colourable.5

Footnote 5: Observe that, with a program originally under SSA form, practical implementation may still choose to interleave the process of spilling and coalescing/colouring. Result will be unchanged, but speed might be impacted.

### 22.3.2 Coalescing Under SSA Form

The goal of _coalescing_ is to minimize the number of register-to-register _copies_ instructions in the final code. In a graph-based allocation approach where the colour-everywhere strategy is applied, each node of the interference graph corresponds to a unique variable that will be allocated to a register along its entire live range. Coalescing two nodes, or by extension coalescing the two corresponding variables, means merging the nodes in the graph, thus imposing allocating the same register to those variables. Any copy instruction between those two variables becomes useless and can be safely removed.

This way, coalescing, when done during the register allocation phase, is used to minimize the amount of register-to-register copies in the final code. While there may not be so many such copies at the high level (e.g., instructions "$a\gets b$")--especially after a phase of copy propagation under SSA (see Chap. 8), many such instructions are added in different compiler phases by the time compilation reaches the register allocation phase. For instance, adding copies is a common way to deal with register constraints (see the practical discussion in Sect. 22.4).

An even more obvious and unavoidable reason in our case is the presence of $\phi$-functions due to the SSA form: The semantic of a $\phi$-function corresponds to parallel copies on incoming edges of basic blocks, and destructing SSA, that is, getting rid of $\phi$-functions that are not machine instructions, is done through the insertion of copy instructions. It is thus better to assign variables linked by a $\phi$-function to the same register, so as to "remove" the associated copies between subscripts of the same variable. As already formalized (see Sect. 21.2 of Chap. 21) for the aggressive coalescing scheme, we define a notion of _affinity_, acting as the converse of the relation of interference and expressing how much two variables "want" to share the same register. By adding a metric to this notion, it measures the benefit one could get if the two variables were assigned to the same register: the weight represents how many instructions coalescing would save at execution.

### 22.3.3 Graph-Based Approach

Coalescing comes with several flavours, which can be either aggressive or conservative. Aggressively coalescing an interference graph means coalescing non-interfering nodes (that is, constraining the colouring) regardless of the chromatic number of the resulting graph. An aggressive coalescing scheme is presented in Chap. 21. Conservatively coalescing an interference graph means coalescing non-interfering nodes without increasing the chromatic number of the graph. In both cases, the objective function is the maximization of satisfied affinities, that is, the maximization of the number of (weighted) affinities between nodes that have been coalesced together. In the current context, we will focus on the conservative scheme, as we do not want more spilling.

Obviously, because of the reducibility to graph-$k$-colouring, both coalescing problems are NP-complete. However, graph colouring heuristics such as the Iterated Register Coalescing use incremental coalescing schemes where affinities are considered one after another. Incrementally, for two nodes linked by an affinity, the heuristic will try to determine whether coalescing those two nodes will, with regard to the colouring heuristic, increase the chromatic number of the graph or not. If not, then the two corresponding nodes are (conservatively) coalesced. The IRC considers two conservative coalescing rules that we recall here. Nodes with degree strictly less than $R$ are called _low-degree_ nodes (those are simplifiable), while others are called _high-degree_ nodes.

**Briggs rule**: merges $u$ and $v$ if the resulting node has less than $R$ neighbours of high degree. This node can always be simplified after its neighbours, low-degree neighbours, are simplified; thus the graph remains greedy-$R$-colourable.
**George rule**: merges $u$ and $v$ if all neighbours of $u$ with high degree are also neighbours of $v$. After coalescing and once all low-degree neighbours are simplified, one gets a sub-graph of the original graph, thus greedy-$R$-colourable too.

The Iterated Register Coalescing algorithm normally also performs spilling and includes many phases that are interleaved with colouring and coalescing, called in the literature "freezing," "potential spills," "select," and "actual spill" phases.

A pruned version of the coalescing algorithm used in the IRC can be obtained by removing the freezing mechanism (explained below) and the spilling part. It is presented in Algorithm 22.7. In this code, both the processes of coalescing and simplification are combined. It works as follows:

1. Low-degree nodes that are not copy-related (no affinities) are simplified as much as possible.
2. When no more nodes can be simplified this way, an affinity is chosen. If one of the two rules (Briggs or George) succeeds, the corresponding nodes are merged. If not, the affinity is erased.
3. The process iterates (from stage 1) until the graph is empty.

Originally, those rules were used for any graph, not necessarily greedy-$R$-colourable, and with an additional clique of pre-coloured nodes--the physical machine registers. With such general graphs, some restrictions on the applicability of those two rules had to be applied when one of the two nodes was a pre-coloured one. But in the context of greedy-$R$-colourable graphs, we do not need such restrictions.

However, in practice, those two rules give insufficient results to coalesce the many copies introduced, for example, by a basic SSA destruction conversion. The main reason is because the decision is too local: it depends on the degree of neighbours only. But these neighbours may have a high degree just because their neighbours are not simplified yet, that is, the coalescing test may be applied too early in the simplify phase.

This is the reason why the IRC actually iterates: instead of giving up coalescing when the test fails, the affinity is "frozen," that is, placed in a sleeping list and "awakened" when the degree of one of the nodes implied in the rule changes. Thus, affinities are in general tested several times, and copy-related nodes--nodes linked by affinities with other nodes--should not be simplified too early to ensure the affinities get tested.

The advocated scheme corresponds to the pseudo-code of Algorithm 22.7 and is depicted in Fig. 22.5. It tries the coalescing of a given affinity only once and thus does not require any complex freezing mechanism as done in the original IRC. This is made possible thanks to the following enhancement of the conservative coalescing rule: Recall that the objective of Briggs and George rules is to testwhether the coalescing $v$ and $u$ breaks the greedy-$R$-colourable property of $G$ or not. If the Briggs and George tests fail, testing this property can be done by running function _Simplify($G$)_ itself! Theoretically, this greatly increases the complexity, as for each affinity, a full simplification process could be potentially performed. However, experience shows that the overhead is somewhat balanced by the magnitude lowering of calls to "can_be_coalesced." This approach still looks quite rough; hence, we named it the _Brute_ coalescing heuristic. While this is more costly than only using Briggs and George rules, adding this "brute" rule improves substantially the quality of the result, in terms of suppressed copy instructions.

![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310110248234.png)

![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310110249540.png)


###  22.3.4 Scan-Based Approach

The most natural way to perform coalescing using a scan-based (linear scan or tree scan) approach would be to simply do biased colouring. Consider again the "tree scan" depicted in Algorithm 22.1. Let us suppose the current program point $p$ is a copy instruction from variable $v$ to $v^{\prime}$. The colour of $v$ that is freed at line 3 can be reused for $v^{\prime}$ at line 5. In practice, this extremely local strategy does not work that well for $\phi$-functions. Consider as an example variables $x_{1}$, $x_{2}$, and $x_{3}$ in the program Fig. 22.2b. As there is no specific reason for the greedy allocation to assign the same register to both $x_{1}$ and $x_{2}$, when it comes to assigning one to $x_{3}$, the allocator will often be able to only satisfy one of its affinities.

To overcome this limitation, the idea is to use an aggressive pre-coalescing as a pre-pass of our colouring phase. We can use one of the algorithms presented in Chap. 21, but the results of aggressive coalescing should only be kept in a separate structure and _not applied_ to the program. The goal of this pre-pass is to put copy-related variables into _equivalence classes_. In a classical graph colouring allocator, the live ranges of the variables in a class are fused. We do not do so but use the classes to bias the colouring. Each equivalence class has a colour, which is initially unset, and is set as soon as one variable of the class is assigned to register. When assigning a colour to a variable, tree scan checks if the colour of the class is available, and picks it if it is. If not, it chooses a different colour (based on the other heuristics presented here) and updates the colour of the class.

## 22.4 Practical Discussions and Further Reading

The amount of papers on register allocation is humongous. The most popular approach is based on graph colouring, including many extensions [71; 190; 219] of the seminal paper of Chaitin et al. [61]. The Iterated Register Coalescing, mentioned several times in this chapter, is from George et al. [124]. The linear scan approach is also extremely popular in particular in the context of just-in-time compilation. This elegant idea goes back to Traub et al. [287] and Poletto and Sarkar [230]. If this original version of linear scan contains a lot of hidden subtleties (such as the computation of live ranges, handling of shuffle code in the presence of critical edges, etc.), its memory footprint is smaller than a graph colouring approach, and its practical complexity is smaller than the IRC. However, mostly due to the highly over-approximated live ranges, its apparent simplicity comes with a poor-quality resulting code. This leads to the development of interesting but more complex extensions such as the works of Wimmer and Sarkar [252; 305]. All those approaches are clearly subsumed by the tree scan approach [80], both in terms of simplicity, complexity, and quality of result. On the other hand, the linear scan extension proposed by Barik in his thesis [17] is an interesting competitor, its footprint being, possibly, more compact than the one used by a tree scan.

There exists an elegant relationship between tree scan colouring and graph colouring: Back in 1974, Gavril [122] showed that the intersection graphs of subtrees are the _chordal graphs_. By providing an elimination scheme that exposes the underlying tree structure, this relationship allowed to prove that chordal graphs can be optimally coloured in linear time with respect to the number of edges in the graph. This is the rediscovering of this relationship in the context of live ranges of variables for SSA-form programs that motivated different research groups [39; 51; 136; 221] to revisit register allocation in the light of this interesting property. Indeed, at that time, most register allocation schemes were incomplete assuming that the assignment part was hard by referring to the NP-completeness reduction of Chaitin et al. to graph colouring. However, the observation that SSA-based live range splitting allows to decouple the allocation and assignment phases was not new [87; 111]. Back in the nineties, the LaTte [314] just-in-time compiler already implemented the ancestor of our tree scan allocator. The most aggressive live range splitting that was proposed by Appel and George [10] allowed to stress the actual challenge that past approaches were facing when splitting live range to help colouring, which is coalescing [40]. The PhD theses of Hack [135], Bouchez [38], and Colombet [79] address the difficult challenge of making a neat idea applicable to real life but without trading the elegant simplicity of the original approach. For some more exhaustive related work references, we refer to the bibliography of those documents.

### **Looking Under the Carpet**

As done in (too) many register allocation papers, the heuristics described in this chapter assume a simple non-realistic architecture where all variables or registers are equivalent and where the instruction set architecture does not impose any specific constraint on register usage. Reality is different, including: 1. register constraints such as 2-address mode instructions that impose two of the three operands to use the same register, or instructions that impose the use of specific registers; 2. registers of various sizes (vector registers usually leading to register aliasing), historically known as register pairing problem; 3. instruction operands that cannot reside in memory. Finally, SSA-based--but also any scheme that relies on live range splitting--must deal with critical edges possibly considered abnormal (i.e., that cannot be split) by the compiler.

In the context of graph colouring, _register constraints_ are usually handled by adding an artificial clique of pre-coloured node in the graph and splitting live ranges around instructions with pre-coloured operands. This approach has several disadvantages. First, it substantially increases the number of variables. Second, it makes coalescing much harder. This motivated Colombet et al. [80] to introduce the notion of antipathies (affinities with negative weight) and extend the coalescing rules accordingly. The general idea is, instead of enforcing architectural constraints, to simply express the cost (through affinities and antipathies) of shuffle code inserted by a post-pass repairing. In a scan-based context, handling of register constraints is usually done locally [203; 252]. The biased colouring strategy used in this chapter is proposed by Braun et al. and Colombet et al. [46; 80] and allows to reduce need for shuffle code.

In the context of graph-based heuristics, _vector registers_ are usually handled through a generalized graph colouring approach [262; 281]. In the context of scan-based heuristics, the puzzle solver [222] is an elegant formulation that allows to express the local constraints as a puzzle.

One of the main problems of the graph-based approach is its underlying assumption that the live range of a variable is atomic. But when spilled, not all instructions can access it through a _memory operand_. In other words, spilling has the effect of removing the majority of the live ranges, but leaving "chads" that correspond to shuffle code around instructions that use the spilled variable. These replace the removed node in the interference graph, which is usually re-built at some point. Those subtleties and associated complexity issues are exhaustively studied by Bouchez et al. [41].

As already mentioned, the semantics of $\phi$-functions correspond to parallel copies on the incoming edges of the basic block where they textually appear. When lowering $\phi$-functions, the register for the def operand may not match the register of the use operand. If done naively, this imposes the following:

1. _Splitting_ of the corresponding control-flow edge
2. Inserting of copies on this freshly created basic block
3. Using of spill code in case parallel copy requires a temporary register but none is available

This issue shares similarities with the SSA destruction that was described in Chap. 21. The advocated approach is, just as for the register constraints, to express the cost of an afterward repairing in the objective function of the register allocation scheme. Then, locality, the repairing can be expressed as a standard graph colouring problem on a very small graph containing the variables created by $\phi$-nodes isolation (see Fig. 21.1). However, it turns out that most of the associated shuffle codes can usually be moved (and even annihilated) to and within the surrounding basic blocks. Such post-pass optimizations correspond to the optimistic copy insertion of Braun et al. [46] or the parallel copy motion of Bouchez et al. [43].

#### **Decoupled Load/Store and Coalescing Optimization Problems**

The scan-based spilling heuristic described in this chapter is inspired from the heuristic developed by Braun and Hack [45]. It is an extension of Belady's algorithm [22], which was originally designed for page eviction, but can easily be shown to be optimal for interval graphs (straight-line code and single use variables). For straight-line code but multiple uses per variable, Farrach and Liberatore [114] showed that if this further-first strategy works reasonably well, it can also be formalized as a flow problem. For a deeper discussion about the complexity of the spilling problem under different configurations, we refer to the work of Bouchez et al. [41]. To conclude on the spilling part of the register allocation problem, we need to mention the important problem of load/store placement. As implicitly done by our heuristic given in Algorithm 22.4, one should, whenever possible, hoist shuffle code outside of loops. This problem corresponds to the global code motion addressed in Chap. 11 that should idealistically be coupled to the register allocation problem. Another related problem, not mentioned in this chapter, is rematerialization [48]. Experience shows that: 1. Rematerialization is one of the main sources of performance improvement for register allocation. 2. In the vast majority of cases, rematerialization simply amounts to rescheduling some of the instructions. This remark allows to highlight one of the major limitations of the presented approach common to almost all papers in the area of register allocation: while scheduling and register allocation are clearly highly coupled problems, all those approaches only consider a fixed schedule and only few papers try to address the coupled problem [27; 78; 204; 209; 226; 242; 286; 301].

Static Single Assignment has not been adopted by compiler designers for a long time. One of the reasons is related to the numerous copy instructions inserted by SSA destruction that the compiler could not get rid of afterwards. The coalescing heuristics were not effective enough in removing all such copies, so even if it was clear that live range splitting was useful for improving colourability, that is, avoiding spill code, people were reluctant to split live ranges, preferring to do it on demand [82]. The notion of value-based interference described in Paragraph 21.2 of Chap. 21 is an important step for improving the quality of existing schemes. Details on the brute-force approach that extends the conservative coalescing scheme depicted in this chapter can be found in Bouchez et al. [42]. However, there exist several other schemes whose complexities are studied by Bouchez et al. [40]. Among others, the optimistic approach of Park and Moon [219] has been shown to be extremely competitive [42; 132] in the context of the, somehow restricted, "Optimal Coalescing Challenge" initiated by Appel and George, designers of the Iterated Register Coalescing [124].
