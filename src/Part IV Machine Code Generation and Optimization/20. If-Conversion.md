# Chapter 20. If-Conversion
**Christian Bruel**

Very long instruction word (VLIW) or explicitly parallel instruction computing (EPIC) architectures make instruction-level Parallelism (ILP) visible within the Instruction Set Architecture (ISA), relying on static schedulers to organize the compiler output such that multiple instructions can be issued in each cycle.

If-conversion is the process of transforming a control-flow region with conditional branches into an equivalent predicated or speculated sequence of instructions (into a region of basic blocks, possibly single) referred to as a Hyperblock. If-converted code replaces control dependencies by data dependencies and thus exposes instruction-level Parallelism very naturally within the new region at the software level.

Removing control hazards improves performance in several ways. When the misprediction penalty is removed, the instruction fetch throughput is increased and the instruction cache locality is improved. Enlarging the size of basic blocks allows earlier execution of long latency operations and the merging of multiple control-flow paths into a single flow of execution that can later be exploited by scheduling frameworks such as VLIW scheduling, hyperblock scheduling, or modulo scheduling.

Consider the simple example given in Fig. 20.1, which represents the execution of an if-then-else-end statement on a 4-issue processor with non-biased branches. In this figure, $r=q$? $r_{1}:r_{2}$ stands for a select instruction where $r$ is assigned $r_{1}$ if $q$ is true, and $r_{2}$ otherwise. With standard basic block ordering, assuming that all instructions have a one-cycle latency, the schedule height rises from five cycles in the most optimistic case to six cycles. After if-conversion, the execution path is reduced to four cycles with no branches, regardless of the testoutcome, and assuming a very optimistic one-cycle branch penalty. The main benefit here is that it can be executed without branch disruption.
![Fig.20.1.svg](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310100151325.svg)

From this introductory example, we can observe that:

* The two possible execution paths have been merged into a single execution path, implying a better exploitation of the available resources.
* The schedule height has been reduced, because instructions can be control-speculated before the branch.
* The variables have been renamed, and a _merge_ pseudo-instruction has been introduced.

Thanks to SSA, the merging point is already materialized in the original control flow as a $\phi$ pseudo-instruction, and register renaming has been performed by SSA construction. Given this, the transformation to generate if-converted code seems natural locally. Exploiting these properties on larger-scale control-flow regions requires a framework that we will develop further.

## 20.1 Architectural Requirements

The _merge_ pseudo-operations need to be mapped to a conditional form of execution in the target's architecture. As illustrated in Fig. 20.2, we differentiate the following three models of conditional execution:

* _Fully predicated execution_: Any instruction can be executed conditionally on the value of a predicate operand.
* _(Control) speculative execution_: The instruction is executed unconditionally and then committed using conditional move (cmov or select) instructions.
* _Partially predicated execution_: Only a subset of the ISA is predicated, usually memory operations that are not easily speculated; other instructions are speculated.

In this figure, we use the notation $r=c$? $r_{1}:r_{2}$ to represent a select -like operation. Its semantic is identical to the gating $\phi_{if}$-function presented in Chap. 14: $r$ takes the value of $r_{1}$ if $c$ is true, and $r_{2}$ otherwise. Similarly, we also use the notation $c$? $r=op$ to represent the predicated execution of $op$ if the predicate $c$ is true; $\overline{c}$? $r=op$ if the predicate $c$ is false.

To be speculated, an instruction must not have any side effects or hazards. For instance, a memory load must not trap because of an invalid address. Memory operations are a major impediment to if-conversion. This is regrettable, because like any other long latency instructions, speculative loads can be very effective in fetching data earlier in the instruction stream, thereby reducing stalls. Modern architectures provide architectural support to dismiss invalid address exceptions. Examples are the ldw.d dismissible load operation in the Multiflow Trace series of computers, or in the STMicroelectronics ST231 processor, but also the speculative load of the Intel IA64. The main difference is that with a dismissible model, invalid memory access exceptions are not delivered, which can be problematic in an embedded or kernel environment that relies on memory exception for correct behaviour. A speculative model serves to catch the exception thanks to the token bit check instruction. Some architectures, such as the IA64, offer both speculative and predicated memory operations. Stores can also be executed conditionally by speculating part of their address value, with additional constraints on the ordering of the memory operations due to possible aliases between the two paths. Figure 20.3 shows examples of various forms of speculative memory operations.
![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310100152062.png)
![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310100152436.png)


Note that the $\text{select}$ instruction is an architecture instruction that does not need to be replaced during the SSA destruction phase. If the target architecture does not provide such a gating instruction, it can be emulated using two conditional moves. This translation can be done afterwards, and the select instruction can still be used as an intermediate form. It allows the program to stay in full SSA form where all the data dependencies are made explicit, and can thus be fed to all SSA optimizers.

This chapter is organized as follows. We begin by describing the SSA techniques to convert a CFG region into SSA form to produce an if-converted SSA representation using speculation. We then describe how this framework is extended to use predicated instructions, using the $\psi$-SSA form presented in Chap. 15. Finally, we outline a global framework to pull together these techniques, incrementally enlarging the scope of the if-converted region to its maximum beneficial size.

## 20.2 Basic Transformations

Unlike global approaches that identify a control-flow region and if-convert it in one shot, the technique described in this chapter is based on incremental reductions. To this end, we consider basic SSA transformations whose goal is to isolate a simple diamond-DAG structure (informally an if-then-else-end) that can be easily if-converted. The complete framework, which identifies and incrementally performs the transformation, is described in Sect. 20.3.

### 20.2.1 SSA Operations on Basic Blocks

The basic transformation that actually if-converts the code is the $\phi$_removal_, which takes a simple diamond-DAG as an input, i.e., a single-entry node/single-exit node (SESE) DAG with only two distinct forward paths from its entry node to its exit node. The $\phi$ removal consists in (1) speculating the code of both branches in the entry basic block (denoted _head_); then (2) replacing the $\phi$-function by a select; and finally (3) simplifying the control flow to a single basic block. This transformation is illustrated in Fig. 20.4.
![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310100155052.png)
The goal of the $\phi$_reduction_ transformation is to isolate a diamond-DAG from a structure that resembles a diamond-DAG but has side entries to its exit block. This diamond-DAG can then be reduced using the $\phi$ removal transformation. Nested if-then-else-end in the original code can create such a control-flow region. Of note is the similarity with the nested arity-two $\phi_{ij}$-functions used for gated SSA (see Chap. 14). In the most general case, the joint node of the considered region has $n$ direct predecessors with $\phi$-functions of the form $B_{0}:r=\phi(B_{1}:r_{1},\,B_{2}:r_{2},\,\ldots,\,B_{n}:r_{n})$ and is such that removing edges from $B_{3},\,\ldots,\,B_{n}$ would give a diamond-DAG. After the transformation, $B_{1}$ and $B_{2}$ point to a freshly created basic block, say $B_{12}$, that itself points to $B_{0}$; a new variable $B_{12}:r_{12}=\phi(B_{1}:r_{1},\,B_{2}:r_{2})$ is created in this new basic block; the $\phi$-function in $B_{0}$ is replaced by $B_{0}:r=\phi(B_{12}:r_{12},\,\ldots,\,B_{n}:r_{n})$. This is illustrated in Fig. 20.5.

The goal of _path duplication_ is to isolate a diamond-DAG from a structure that resembles a diamond-DAG but has side exit edges. Through path duplication, all edges that point to a node different from the exit node or to the willing entry node are "redirected" to the exit node. $\phi$ reduction can then be applied to the region obtained. More formally, consider two distinguished nodes, the first named _head_ and the second a single-exit node of the region named _exit_, such that there are exactly two different control-flow paths from _head_ to _exit_; consider (if it exists) the first node $side_{i}$ on one of the forward paths $head\to side_{0}\to\ldots side_{p}\to exit$, which has at least two direct predecessors. The transformation duplicates the path $P=side_{i}\to\cdots\to side_{p}\to exit$ into $P^{\prime}=side^{\prime}_{i}\to\cdots\to side^{\prime}_{p}\to exit$ and redirects $side_{i-1}$ (or $head$ if $i=0$) to $side^{\prime}_{i}$. All the $\phi$-functions that are along $P$ and $P^{\prime}$ for which the number of direct predecessors has changed have to be updated accordingly. Hence, a $r=\phi(side_{p}:r_{1}$, $B_{2}:r_{2}$, $\ldots$, $B_{n}:r_{n}$) in $exit$ will be updated into $r=\phi(side^{\prime}_{p}:r_{1}$, $B_{2}:r_{2}$, $\ldots$, $B_{n}:r_{n}$, $side_{p}:r_{1}$); a $r=\phi(side_{i-1}:r_{0}$, $r_{1}$, $\ldots$, $r_{m}$) originally in $side_{i}$ will be updated into $r=\phi(r_{1},\ldots,r_{m})$ in $side_{i}$ and into $r=\phi(r_{0})$, i.e., $r=r_{0}$ in $side^{\prime}_{i}$. Variable renaming (see Chap. 5) along with copy folding can then be performed on $P$ and $P^{\prime}$. All steps are illustrated in Fig. 20.6.
![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310100157345.png)
The last transformation, namely the _conjunctive predicate merge_, concerns the if-conversion of a control-flow pattern that sometimes appears on codes to represent logical and or conditional operations. As illustrated in Fig. 20.7, the goal is to isolate a diamond-DAG from a structure that resembles a diamond-DAG but has side exit edges. As opposed to path duplication, the transformation is actually restricted to a very simple pattern, highlighted in Fig. 20.7, made up of three distinct basic blocks: _head_, which branches with predicate $p$ to _side_, or _exit. side_, which is empty and branches itself with predicate $q$ to another basic block outside of the region, or _exit_. Conceptually, the transformation can be understood as first isolating the outgoing path $p\to q$ and then if-converting the obtained diamond-DAG.

![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310100157138.png)

Implementing the same framework on a non-SSA-form program would require more effort: The $\phi$ reduction would do the renaming, involving either a global data-flow analysis or the insertion of copies at the _exit_ node of the diamond-DAG; inferring the minimum amount of select operations would also require the upkeep of liveness information. SSA form solves the renaming issue without additional effort, and, as illustrated in Fig. 20.8, the minimality and the pruned type of the SSA form avoid inserting useless select operati

### 20.2.2 Handling of Predicated Execution Model

The $\phi$ removal transformation described above involved a speculative execution model. As we will illustrate below, in the context of a predicated execution model, the choice of speculation versus predication is an optimization decision that should not be imposed by the intermediate representation. Also, transforming speculated code into predicated code can be viewed as a coalescing problem. The use of $\psi$-SSA (see Chap. 15) as the intermediate form of if-conversion serves to postpone the decision to speculate some code, while the coalescing problem is naturally handled by the $\psi$-SSA destruction phase.

Just as (control) speculating an operation on a control-flow graph means ignoring the control dependence with the conditional branch, speculating an operation on an if-converted code is the same as removing the data dependence with the corresponding predicate. On the other hand, after register allocation, speculation adds anti-dependencies. This trade-off is illustrated in the example of Fig. 20.9: For the fully predicated version of the code, the computation of $p$ has to be done before the computations of $x_{1}$ and $x_{2}$; speculating the computation of $x_{1}$ removes the dependence with p and allows it to be executed in parallel with the test 
![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310100208464.png)
; if the computation of both $x_{1}$ and $x_{2}$ is speculated, they cannot be coalesced, and when destruction of $\psi-SSA$, the $\psi$-function will give rise to some select instructions; if only the computation of $x_{1}$ is speculated, then $x_{1}$ and $x_{2}$ can be coalesced to x, but then an anti-dependence from $x=a+b$ and $p \ ? \ x \ = \ c$ appears, forbidding its execution in parallel.

In practice, speculation is performed during the $\phi$ removal transformation, whenever it is possible (operations with side effect cannot be speculated) and considered as beneficial. As illustrated in Fig. 20.10b, only the operations part of _one_ of the diamond-DAG branches are actually speculated. This partial speculation leads to the manipulation of predicated code.

![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310100208712.png)
![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310100208940.png)

Speculating code is the easiest part, as it could be done prior to the actual if-conversion by simply hoisting the code above the conditional branch. Still, it is worth pointing out that since $\psi$-functions are part of the intermediate representation, they can be considered for inclusion in a candidate region for if-conversion, and in particular for speculation. However, the strength of $\psi$-SSA allows $\psi$-functions to be treated just like any other operation. Consider the code in Fig. 20.10a containing a subregion that has already been processed. To speculate the operation $d_{1}=\mathtt{f}(x)$, the operation defining $x$, i.e., the $\psi$-function, also has to be speculated. Similarly, all the operations defining the operands $x_{1}$ and $x_{2}$ should also be speculated. If one of them can produce hazardous execution, then the $\psi$-function cannot be speculated, which in turn forbids the operation $d_{1}=\mathtt{f}(x)$ from being speculated. Marking operations that cannot be speculated can be done easily using a forward propagation along def-use chains.

All operations that cannot be speculated, possibly including some $\psi$-functions, should be predicated. Suppose we are considering a non-speculated operation that we aim to if-convert and that is part of the then branch on predicate $q$. Just as for $x_{2}=c$ in Fig. 20.10a, this operation might already be predicated (on $\overline{p}$ here) prior to the if-conversion. In that case, a _projection_ on $q$ is performed, meaning that instead of predicating $x_{2}=c$ by $\overline{p}$ it gets predicated by $q\land\overline{p}$. A $\psi$-function can also be projected on a predicate $q$, as described in Chap. 15: All gates of each operand are individually projected on $q$. As an example, originally non-gating operand $x_{1}$ gets gated by $q$, while the $\overline{p}$-gated operand $x_{2}$ gets gated by $s=q\land\overline{p}$. Note that as opposed to speculating it, predicating a $\psi$-function does not impose predicating the operations that defined its operands. The only subtlety related to projection is the generation of the new predicate as the logical conjunction of the original guard (e.g., $\overline{p}$) and the current branch predicate (e.g., $q$). Here, $s$ needs to be computed at some point. The heuristic consists in first listing the set of all necessary predicates and then emitting the corresponding code at the earlier place. Here, the used predicates are $q$, $\overline{q}$, and $q\land\overline{p}$, and $q$ and $\overline{q}$ are already available. The earlier place where $q\land\overline{p}$ can be computed is just after calculating $p$.

Once operations have been speculated or projected (on $q$ for the then branch, on $\overline{q}$ for the else), each $\phi$-function at the merge point is replaced by a $\psi$-function: operands of speculated operations are placed first and guarded by true; operands of projected operations follow, guarded by the predicate of the corresponding branch.

## 20.3 Global Analysis and Transformations

Frequently executed regions are rarely just composed of simple if-then-else control-flow regions, but processors have limited resources: the number of registers will determine the acceptable level of data dependencies to minimize register pressure; the number of predicate registers will determine the depth of the if-conversion so that the number of conditions does not exceed the number of available predicates; and the number of processing units will determine the number of instructions that can be executed simultaneously. The inner-outer incremental process advocated in this chapter serves to evaluate precisely the profitability of if-conversion.

### 20.3.1 SSA Incremental If-Conversion Algorithm

The algorithm takes as input a CFG in SSA form and applies incremental reductions using the list of candidate-conditional basic blocks sorted in post-order. Each basic block in the list designates the head of a sub-graph that can be if-converted using the transformations described in Sect. 20.2. Post-order traversal serves to process each region from the inner to the outer. When the if-converted region cannot grow anymore because of resources, or because a basic block cannot be if-converted, then the next sub-graph candidate is considered until the entire CFG is explored. Note that as the reduction proceeds, maintaining SSA can be done using the general technique described in Chap. 5. Basic local ad hoc updates can also be implemented instead.

Consider for example the CFG reported in Fig. 20.11a. The exit node $B_{6}$ and basic block $B_{3}$ (which contains a function call) cannot be if-converted. The post-order list of conditional blocks (represented in bold) is $[B_{9}$, $B_{14}$, $B_{13}$, $B_{11}$, $B_{8}$, $B_{7}$, $B_{5}$, $B_{2}]$. (1) The first candidate region is composed of $\{B_{9}$, $B_{2}$, $B_{10}\}$; $\phi$ reduction can be applied, promoting the instructions of $B_{10}$ in $B_{9}$; $B_{2}$ becomes the single direct successor of $B_{9}$. (2) The region headed by $B_{14}$ is then considered; $B_{15}$ cannot yet be promoted because of the side entries coming both from $B_{12}$ and $B_{13}$; $B_{15}$ is duplicated into a $B_{15^{\prime}}$ with $B_{2}$ as direct successor; $B_{15^{\prime}}$ can then be promoted into $B_{14}$, which now has a single direct successor $B_{2}$. (3) The region headed by $B_{13}$, which has $B_{14}$ and $B_{15}$ as direct successors, is now considered; $B_{15}$ is again duplicated into $B_{15^{\prime}}$, so as to promote $B_{14}$ and $B_{15^{\prime}}$ into $B_{13}$ through $\phi$ reduction; $B_{15^{\prime}}$ already contains predicated operations from the previous transformation, so a new merging predicate is computed and inserted. After the completion of $\phi$ removal, $B_{13}$ has a unique direct successor, $B_{2}$. (4) $B_{11}$ is the head of the new candidate region; here, $B_{12}$ and $B_{13}$ can be promoted. Again, since $B_{13}$ contains predicated and predicate setting operations, a fresh predicate must be created to hold the merged conditions. (5) $B_{8}$ is then considered; $B_{11}$ needs to be duplicated to $B_{11^{\prime}}$. The process finishes with the region head $B_{7}$.

### 20.3.2 Tail Duplication

As shown in Fig. 20.11, some basic blocks (such as $B_{3}$) may have to be excluded from the region to if-convert. _Tail duplication_ can be used for this purpose. Similarto path duplication described in Sect. 20.1, the goal of tail duplication is to get rid of the incoming edges of a region to if-convert. This is usually done in the context of hyperblock formation, a technique that, unlike the inner-outer incremental technique described in this chapter, consists in if-converting a region in "one shot." Consider again the example of Fig. 20.11a, and suppose that the set of selected basic blocks defining the region to if-convert consists of all basic blocks from $B_{2}$ to $B_{15}$ excluding $B_{3}$ and $B_{6}$. Getting rid of the incoming edge from $B_{3}$ to $B_{5}$ is possible by duplicating all basic blocks of the region reachable from $B_{5}$, as shown in Fig. 20.11c.

<center>
<img src="https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310100211186.svg"/>
</center>

Consider a region $\mathcal{R}$ made up of a set of basic blocks, a distinguished one _entry_ and the others denoted $(B_{i})_{2\leq i\leq n}$, such that any $B_{i}$ is reachable from _entry_ in $\mathcal{R}$. Suppose a basic block $B_{s}$ has some direct predecessors $\mathit{out}_{1},\ \ldots,\ \mathit{out}_{m}$ that are not in $\mathcal{R}$. Tail duplication involves the following steps: (1) for all $B_{j}$ (including $B_{s}$) reachable from $B_{s}$ in $\mathcal{R}$, create a basic block $B_{j}^{\prime}$ as a copy of $B_{j}$; (2) any branch from $B_{j}^{\prime}$ that points to a basic block $B_{k}$ of the region is rerouted to its duplicate $B_{k}^{\prime}$;

(3) any branch from a basic block $out_{k}$ to $B_{s}$ is rerouted to $B_{s}^{\prime}$. In our example, we would have $entry=B_{2}$, $B_{s}=B_{6}$, and $out=B_{4}$.

A global approach would follow the steps in Fig. 20.11c: First, select the region; second, get rid of the incoming edges using tail duplication; and finally, perform if-conversion of the whole region in one shot. It is worth pointing out that there is no phasing issue with tail duplication. To illustrate this point, consider the example of Fig. 20.12 where $B_{2}$ cannot be if-converted. The selected region is made up of all other basic blocks. Using a global approach as in standard hyperblock formation, tail duplication would be performed prior to any if-conversion. This would result in the CFG on the left part of the figure. Note that a new node, $B_{7}$, has been added here after the tail duplication by a process called branch coalescing. Applying if-conversion on the two disjoint regions, whose heads are, respectively, $B_{4}$ and $B_{4^{\prime}}$, would result in the final code shown at the bottom of the figure. Our incremental scheme would first perform if-conversion of the region headed by $B_{4}$, resulting in the code depicted in the CFG on the right. Applying tail duplication to get rid of the side entry from $B_{2}$ would result in exactly the same final code at the bottom.
![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310100213663.png)

### 20.3.3 Profitability

Fusing execution paths can overcommit the architectural ability to execute the multiple instructions in parallel: Data dependencies and register renaming introduce new register constraints. Moving operations earlier in the instruction stream increases live ranges. Aggressive if-conversion can easily exceed processor resources, leading to excessive register pressure or moving infrequently used long latency instructions into the critical path. The prevalent idea is that a region can be if-converted if the cost of the resulting if-converted basic block is smaller than the cost of each individual basic block of the original region weighted by their respective execution probability. To evaluate these costs, we consider all possible paths impacted by the if-conversion.


For all transformations except the conjunctive predicate merge, there are two such paths starting at the basic block head. For the code in Fig. 20.13, we would have $path _{p}=\left[\right. head, B_{1}, exit \left[\right. and path \bar{p}=\left[\right. head, B_{1}^{\prime}, B_{2}^{\prime}, exit [$ of respective probabilities $\operatorname{prob}(p) and \operatorname{prob}(\bar{p})$. For a $path P_{q}=\left[B_{0}, B_{1}, \ldots, B_{n}\right. [$ of probability $\operatorname{prob}(q)$, its cost is given by $\widehat{P_{q}}=\operatorname{prob}(q) \times \sum_{i=0}^{n-1}\left[\widehat{B_{i}, B_{i+1}}\left[\right.\right.,$ where $\left[\widehat{B_{i}, B_{i+1}}[\right.$ represents the cost of basic block $\widehat{\left[B_{i}\right]}$ estimated using its schedule height plus the branch latency br_lat, if the edge $\left(B_{i}, B_{i+1}\right)$ corresponds to a conditional branch, 0 otherwise. Note that if $B_{i}$ branches to $S_{q}$ on predicate q and falls through to $S_{\bar{q}}$, we have

![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310100213564.png)
Let $path _{p}=\left[\right. head, B_{1}, \ldots, B_{n}, exit \left[\right.$ and $path \bar{p}=\left[\right. head , B_{1}^{\prime}, \ldots, B_{m}^{\prime}, exit [$ be the two paths starting at the basic block head, where p is the branch predicate. Then, the overall cost before if-conversion simplifies to the following:

![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310100219079.png)

![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310100219843.png)
This is to be compared to the cost after if-conversion:
![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310100220839.png)
where $\circ$ is the composition function that merges basic blocks together, removes associated branches, and creates the predicate operations.

The profitability of the logical conjunctive merge in Fig. 20.14 can be evaluated similarly. There are three paths impacted by the transformation: $path_{p\wedge q}=[head,\,side,\,B_{1}[,\,path_{p\wedge\overline{q}}=[head,\,side,\, exit[,$ and $path_{\overline{p}}=[head,\,exit[$ of respective probabilities $\mathsf{prob}(p\wedge q)$, $\mathsf{prob}(p\wedge\overline{q})$, and $\mathsf{prob}(\overline{p})$. The overall cost before the transformation (if branches are on $p$ and $q$)$\widehat{path_{p\wedge q}}+\widehat{path_{p\wedge\overline{q}}}+\widehat{path_{ \overline{p}}}$ simplifies to
![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310100221197.png)
which should be compared to (if the branch on the new $head$ block is on $p\wedge q$)
![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310100220321.png)

Note that if $\mathsf{prob}(p)\ll 1$, emitting a conjunctive merge might not be beneficial. In that case, another strategy such as path duplication from the _exit_ block will be evaluated. Profitability for any conjunctive predicate merge (disjunctive or conjunctive; convergent or not) is evaluated similarly.

A speed-oriented objective function needs the target machine description to derive the instruction latencies, resource usage, and scheduling constraints. The local dependencies computed between instructions are used to compute the dependence height. The branch probability is obtained from static branch prediction heuristics, profile information, or user-inserted directives. Naturally, this heuristic can be either pessimistic, because it does not take into account new optimization opportunities introduced by the branch removal or explicit new dependencies, or optimistic because of inaccurate register pressure estimation leading to register spilling on the critical path, or uncertainty in the branch prediction. But since the SSA incremental if-conversion framework reduces the scope for the decision function to a localized part of the CFG, the size and complexity of the inner region under consideration make the profitability a comprehensive process. This cost function is fast enough to be reapplied to each region during the incremental processing, with the advantage that all the instructions introduced by the if-conversion process in the inner regions, such as new predicate merging instructions or new temporary pseudo-registers, can be taken into account.

## 20.4 Concluding Remarks and Further Reading

In this chapter, we presented how an if-conversion algorithm can take advantage of the SSA properties to efficiently assign predicates and lay out the new control flow in an incremental, inner-outer process. As opposed to the alternative top-down approach, the region selection can be reevaluated at each nested transformation, using local analysis. Hyperblocks [191] were proposed as the primary if-converted scheduling framework, excluding basic blocks that do not justify their inclusion in the if-converted flow of control. Region selection and if-conversion can be performed as a single pass, hyperblocks being created lazily, with well-known techniques such as tail duplication or branch coalescing used only when the benefit is established. Predication and speculation are often presented as two different alternatives for if-conversion. They can coexist in an efficient if-conversion process such that every model of conditional execution is accepted in the same framework, thanks to the conditional moves and $\psi$[275] transformations.
