# Chapter 1. Introduction

Jeremy Singer

In computer programming, as in real life, names are useful handles for concrete entities. The key message of this book is that having _unique names_ for _distinct entities_ reduces uncertainty and imprecision.

For example, consider overhearing a conversation about "Homer." Without any more contextual clues, you cannot disambiguate between Homer Simpson and Homer the classical Greek poet; or indeed, any other people called Homer that you may know. As soon as the conversation mentions Springfield (rather than Smyrna), you are fairly sure that the Simpsons television series (rather than Greek poetry) is the subject. On the other hand, if everyone had a _unique_ name, then there would be no possibility of confusing twentieth century American cartoon characters with ancient Greek literary figures.

This book is about the _Static Single Assignment form_ (SSA), which is a naming convention for storage locations (variables) in low-level representations of computer programs. The term _static_ indicates that SSA relates to properties and analysis of program text (code). The term _single_ refers to the uniqueness property of variable names that SSA imposes. As illustrated above, this enables a greater degree of precision. The term _assignment_ means variable definitions. For instance, in the code
![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310090731728.png)
the variable $x$ is being assigned the value of expression $(y+1)$. This is a definition, or assignment statement, for $x$. A compiler engineer would interpret the above assignment statement to mean that the lvalue of $x$ (i.e., the memory location labeled as $x$) should be modified to store the value $(y+1)$.

## 1.1 Definition of SSA

The simplest, least constrained, definition of SSA can be given using the following informal prose:

_A program is defined to be in SSA form if each variable is a target of exactly one assignment statement in the program text._

However, there are various, more specialized, varieties of SSA, which impose further constraints on programs. Such constraints may relate to graph-theoretic properties of variable definitions and uses, or the encapsulation of specific control-flow or data-flow information. Each distinct SSA variety has specific characteristics. Basic varieties of SSA are discussed in Chap. 2. Part III of this book presents more complex extensions.

One important property that holds for all varieties of SSA, including the simplest definition above, is _referential transparency_: i.e., since there is only a single definition for each variable in the program text, a variable's value is _independent of its position_ in the program. We may refine our knowledge about a particular variable based on branching conditions, e.g., we know the value of $x$ in the conditionally executed block following an if statement that begins with
![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310090732538.png)
However, the _underlying value_ of $x$ does not change at this if statement. Programs written in pure functional languages are referentially transparent. Such referentially transparent programs are more amenable to formal methods and mathematical reasoning, since the meaning of an expression depends only on the meaning of its subexpressions and not on the order of evaluation or side effects of other expressions. For a referentially opaque program, consider the following code fragment.
![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310090732728.png)
A naive (and incorrect) analysis may assume that the values of $y$ and $z$ are equal, since they have identical definitions of $(x+1)$. However, the value of variable $x$ depends on whether the current code position is before or after the second definition of $x$, i.e., variable values depend on their _context_. When a compiler transforms this program fragment to SSA code, it becomes referentially transparent. The translationprocess involves renaming to eliminate multiple assignment statements for the same variable. Now it is apparent that $y$ and $z$ are equal if and only if $x_{1}$ and $x_{2}$ are equal.
![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310090733498.png)
## 1.2 Informal Semantics of SSA

In the previous section, we saw how straight-line sequences of code can be transformed to SSA by simple renaming of variable definitions. The _target_ of the definition is the variable being defined, on the left-hand side of the assignment statement. In SSA, each definition target must be a unique variable name. Conversely variable names can be used multiple times on the right-hand side of any assignment statements, as _source_ variables for definitions. Throughout this book, renaming is generally performed by adding integer subscripts to original variable names. In general this is an unimportant implementation feature, although it can prove useful for compiler debugging purposes.

The $\phi$-function is the most important SSA concept to grasp. It is a special statement, known as a _pseudo-assignment_ function. Some call it a "notational fiction."1 The purpose of a $\phi$-function is to merge values from different incoming paths, at control-flow merge points.

Footnote 1: Kenneth Zadeck reports that $\phi$-functions were originally known as _phoney_-functions, during the development of SSA at IBM Research. Although this was an in-house joke, it did serve as the basis for the eventual name.

Consider the following code example and its corresponding control-flow graph (CFG) representation:

![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310090733006.png)


There is a distinct definition of $y$ in each branch of the if statement. So multiple definitions of $y$ reach the print statement at the control-flow merge point. When a compiler transforms this program to SSA, the multiple definitions of $y$ are renamed as $y_{1}$ and $y_{2}$. However, the print statement could use either variable, dependent on the outcome of the if conditional test. A $\phi$-function introduces a new variable $y_{3}$, which takes the value of either $y_{1}$ or $y_{2}$. Thus the SSA version of the program is
![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310090734543.png)

In terms of their position, $\phi$-functions are generally placed at control-flow merge points, i.e., at the heads of basic blocks that have multiple direct predecessors in control-flow graphs. A $\phi$-function at block $b$ has $n$ parameters if there are $n$ incoming control-flow paths to $b$. The behaviour of the $\phi$-function is to select dynamically the value of the parameter associated with the actually executed control-flow path into $b$. This parameter value is assigned to the fresh variable name, on the left-hand side of the $\phi$-function. Such pseudo-functions are required to maintain the SSA property of unique variable definitions, in the presence of branching control flow. Hence, in the above example, $y_{3}$ is set to $y_{1}$ if control flows from basic block $A$ and set to $y_{2}$ if it flows from basic block $B$. Notice that the CFG representation here adopts a more expressive syntax for $\phi$-functions than the standard one, as it associates direct predecessor basic block labels $B_{i}$ with corresponding SSA variable names $a_{i}$, i.e., $a_{0}=\phi(B_{1}:a_{1},\ldots,B_{n}:a_{n})$. Throughout this book, basic block labels will be omitted from $\phi$-function operands when the omission does not cause ambiguity.

It is important to note that, if there are multiple $\phi$-functions at the head of a basic block, then these are executed in parallel, i.e., simultaneously _not_ sequentially. This distinction becomes important if the target of a $\phi$-function is the same as the source of another $\phi$-function, perhaps after optimizations such as copy propagation (see Chap. 8). When $\phi$-functions are eliminated in the SSA destruction phase, they are sequentialized using conventional copy operations, as described in Algorithm 21.6. This subtlety is particularly important in the context of register allocated code (see Chap. 22).

Strictly speaking, $\phi$-functions are not directly executable in software, since the dynamic control-flow path leading to the $\phi$-function is not explicitly encoded as an input to $\phi$-function. This is tolerable, since $\phi$-functions are generally only used during static analysis of the program. They are removed before any program interpretation or execution takes place. However, there are various executable extensions of $\phi$-functions, such as $\phi_{\mathit{if}}$ or $\gamma$ functions (see Chap. 14), which take an extra parameter to encode the implicit control dependence that dictates the argument the corresponding $\phi$-function should select. Such extensions are useful for program interpretation (see Chap. 14), if conversion (see Chap. 20), or hardware synthesis (see Chap. 23).

We present one further example in this section, to illustrate how a loop control-flow structure appears in SSA. Here is the non-SSA version of the program and its corresponding control-flow graph SSA version:
![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310090734425.png)
The SSA code features two $\phi$-functions in the loop header; these merge incoming definitions from before the loop for the first iteration and from the loop body for subsequent iterations.

It is important to outline that SSA should not be confused with (_dynamic_) single assignment (DSA or simply SA) form used in automatic parallelization. _Static_ Single Assignment does not prevent multiple assignments to a variable during program execution. For instance, in the SSA code fragment above, variables $y_{3}$ and $x_{3}$ in the loop body are redefined dynamically with fresh values at each loop iteration.

Full details of the SSA construction algorithm are given in Chap. 3. For now, it is sufficient to see that:
1. A $\phi$-function has been inserted at the appropriate control-flow merge point where multiple reaching definitions of the same variable converged in the original program.
2. Integer subscripts have been used to rename variables $x$ and $y$ from the original program.

## 1.3 Comparison with Classical Data-Flow Analysis

As we will discover further in Chaps. 13 and 8, one of the major advantages of SSA form concerns data-flow analysis. Data-flow analysis collects information about programs at compile time in order to make optimizing code transformations. During actual program execution, information flows between variables. Static analysis captures this behaviour by propagating _abstract_ information, or data-flow facts, using an operational representation of the program such as the control-flow graph (CFG). This is the approach used in classical data-flow analysis.

Often, data-flow information can be propagated more efficiently using a _functional_, or _sparse_, representation of the program such as SSA. When a program is translated into SSA form, variables are renamed at definition points. For certain data-flow problems (e.g., constant propagation) this is exactly the set of program points where data-flow facts may change. Thus it is possible to associate data-flow facts directly with variable names, rather than maintaining a vector of data-flow facts indexed over all variables, at each program point.

Figure 1 illustrates this point through an example of non-zero value analysis. For each variable in a program, the aim is to determine statically whether that variable can contain a zero integer value (i.e., null) at runtime. Here 0 represents the fact that the variable is null, $\hat{\emptyset}$ the fact that it is non-null, and $\top$ the fact that it is maybe-null. With classical dense data-flow analysis on the CFG in Fig. 1a, we would compute information about variables $x$ and $y$ for each of the entry and exit points of the six basic blocks in the CFG, using suitable data-flow equations. Using sparse SSA-based data-flow analysis on Fig. 1b, we compute information about each variable based on a simple analysis of its definition statement. This gives us seven data-flow facts, one for each SSA version of variables $x$ and $y$.

For other data-flow problems, properties may change at points that are not variable definitions. These problems can be accommodated in a sparse analysis framework by inserting additional pseudo-definition functions at appropriate points to induce additional variable renaming. See Chap. 13 for one such instance. However, this example illustrates some key advantages of the SSA-based analysis.

![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310090735373.png)

1. Data-flow information _propagates directly_ from definition statements to uses, via the def-use links implicit in the SSA naming scheme. In contrast, the classical data-flow framework propagates information throughout the program, including points where the information does not change or is not relevant.
2. The results of the SSA data-flow analysis are _more succinct_. In the example, there are fewer data-flow facts associated with the sparse (SSA) analysis than with the dense (classical) analysis.

Part II of this textbook gives a comprehensive treatment of some SSA-based data-flow analysis.

## 1.4 SSA in Context

**Historical Context**  Throughout the 1980s, as optimizing compiler technology became more mature, various intermediate representations (IRs) were proposed to encapsulate data dependence in a way that enabled fast and accurate data-flow analysis. The motivation behind the design of such IRs was the exposure of direct links between variable definitions and uses, known as _def-use chains_, enabling efficient propagation of data-flow information. Example IRs include the program dependence graph [118] and program dependence web [215]. Chapter 14 gives further details on dependence graph style IRs.

Static Single Assignment form was one such IR, which was developed at IBM Research and announced publicly in several research papers in the late 1980s [6; 89; 249]. SSA rapidly acquired popularity due to its intuitive nature and straightforward construction algorithm. The SSA property gives a standardized shape for variable def-use chains, which simplifies data-flow analysis techniques.

**Current Usage**  The majority of current commercial and open source compilers, including GCC, LLVM, the HotSpot Java virtual machine, and the V8 JavaScript engine, use SSA as a key intermediate representation for program analysis. As optimizations in SSA are fast and powerful, SSA is increasingly used in just-in-time (JIT) compilers that operate on a high-level target-independent program representation such as Java byte-code, CLI byte-code (.NET MSIL), or LLVM bitcode.

Initially created to facilitate the development of high-level program transformations, SSA form has gained much interest due to its favourable properties that often enable the simplification of algorithms and reduction of computational complexity. Today, SSA form is even adopted for the final code generation phase (see Part IV), i.e., the back-end. Several industrial and academic compilers, static or just-in-time, use SSA in their back-ends, e.g., LLVM, HotSpot, LAO, libFirm, Mono. Many compilers that use SSA form perform SSA elimination before register allocation, including GCC, HotSpot, and LLVM. Recent research on register allocation (see Chap. 22) even allows the retention of SSA form until the very end of the code generation process.

**SSA for High-Level Languages**  So far, we have presented SSA as a useful feature for compiler-based analysis of low-level programs. It is interesting to note that some high-level languages enforce the SSA property. The SISAL language is defined in such a way that programs automatically have referential transparency, since multiple assignments are not permitted to variables. Other languages allow the SSA property to be applied on a per-variable basis, using special annotations like final in Java, or const and readonly in C#.

The main motivation for allowing the programmer to enforce SSA in an explicit manner in high-level programs is that _immutability simplifies concurrent programming_. Read-only data can be shared freely between multiple threads, without any data dependence problems. This is becoming an increasingly important issue, with the shift to multi- and many-core processors.
High-level functional languages claim referential transparency as one of the cornerstones of their programming paradigm. Thus functional programming supports the SSA property implicitly. Chapter 6 explains the dualities between SSA and functional programming.

## 1.5 About the Rest of This Book

In this chapter, we have introduced the notion of SSA. The rest of this book presents various aspects of SSA, from the pragmatic perspective of compiler engineers and code analysts. The ultimate goals of this book are:

1. To demonstrate clearly the _benefits_ of SSA-based analysis
2. To dispel the _fallacies_ that prevent people from using SSA

This section gives pointers to later parts of the book that deal with specific topics.

### 1.5.1 Benefits of SSA

SSA imposes a strict discipline on variable naming in programs, so that each variable has a unique definition. Fresh variable names are introduced at assignment statements, and control-flow merge points. This serves to simplify the structure of variable _def-use_ relationships (see Sect. 2.1) and _live ranges_ (see Sect. 2.3), which underpin data-flow analysis. Part II of this book focuses on data-flow analysis using SSA. There are three major advantages to SSA:

**Compile time benefit.**: Certain compiler optimizations can be more efficient when operating on SSA programs, since referential transparency means that data-flow information can be associated directly with variables, rather than with variables at each program point. We have illustrated this simply with the non-zero value analysis in Sect. 1.3.
**Compiler development benefit.**: Program analyses and transformations can be easier to express in SSA. This means that compiler engineers can be more productive, in writing new compiler passes, and debugging existing passes. For example, the _dead code elimination_ pass in GCC 4.x, which relies on an underlying SSA-based intermediate representation, takes only 40% as many lines of code as the equivalent pass in GCC 3.x, which does not use SSA. The SSA version of the pass is simpler, since it relies on the general-purpose, factored-out, data-flow propagation engine.
**Program runtime benefit.**: Conceptually, any analysis and optimization that can be done under SSA form can also be done identically out of SSA form. Because of the compiler development mentioned above, several compiler optimizations are shown to be more effective when operating on programs in SSA form. These include the class of _control-flow insensitive analyses_, e.g., [139].

### 1.5.2 Fallacies About SSA

Some people believe that SSA is too cumbersome to be an effective program representation. This book aims to convince the reader that such a concern is unnecessary, given the application of suitable techniques. The table below presents some common myths about SSA, and references in this first part of the book contain material to dispel these myths.
![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310090737784.png)
