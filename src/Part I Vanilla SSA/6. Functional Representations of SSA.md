# Chapter 6. Functional Representations of SSA

**Lennart Beringer**

This chapter discusses alternative representations of SSA using the terminology and structuring mechanisms of functional programming languages. The reading of SSA as a discipline of functional programming arises from a correspondence between dominance and syntactic scope that subsequently extends to numerous aspects of control and data-flow structure.

The development of functional representations of SSA is motivated by the following considerations:

1. Relating the core ideas of SSA to concepts from other areas of compiler and programming language research provides conceptual insights into the SSA discipline and thus contributes to a better understanding of the practical appeal of SSA to compiler writers.
2. Reformulating SSA as a functional program makes explicit some of the syntactic conditions and semantic invariants that are implicit in the definition and use of SSA. Indeed, the introduction of SSA itself was motivated by a similar goal: to represent aspects of program structure--namely the def-use relationships--explicitly in syntax, by enforcing a particular naming discipline. In a similar way, functional representations directly enforce invariants such as "all $\phi$-functions in a block must be of the same arity," "the variables assigned to by these $\phi$-functions must be distinct," "$\phi$-functions are only allowed to occur at the beginning of a basic block," or "each use of a variable should be dominated by its (unique) definition." Constraints such as these would typically have to be validated or (re-)established after each optimization phase of an SSA-based compiler, but are typically enforced _by construction_ if a functional representation is chosen.

Consequently, less code is required, improving the robustness, maintainability, and code readability of the compiler.
3. The intuitive meaning of "unimplementable" $\phi$-instructions is complemented by a concrete execution model, facilitating the rapid implementation of interpreters. This enables the compiler developers to experimentally validate SSA-based analyses and transformations at their genuine language level, without requiring SSA destruction. Indeed, functional intermediate code can often be directly emitted as a program in a high-level mainstream functional language, giving the compiler writer access to existing interpreters and compilation frameworks. Thus, rapid prototyping is supported and high-level evaluation of design decisions is enabled.
4. Formal frameworks of program analysis that exist for functional languages become applicable. Type systems provide a particularly attractive formalism due to their declarativeness and compositional nature. As type systems for functional languages typically support higher-order functions, they can be expected to generalize more easily to interprocedural analyses than other static analysis formalisms.
5. We obtain a formal basis for comparing variants of SSA--such as the variants discussed elsewhere in this book--for translating between these variants, and for constructing and destructing SSA. Correctness criteria for program analyses and associated transformations can be stated in a uniform manner and can be proved to be satisfied using well-established reasoning principles.

Rather than discussing all these considerations in detail, the purpose of the present chapter is to informally highlight particular aspects of the correspondence and then point the reader to some more advanced material. Our exposition is example-driven but leads to the identification of concrete correspondence pairs between the imperative/SSA world and the functional world.

Like the remainder of the book, our discussion is restricted to code occurring in a single procedure.

## 6.1 Low-Level Functional Program Representations

Functional languages represent code using declarations of the form
![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310090820652.png)
where the syntactic category of expression $e$ conflates the notions of expressions and commands of imperative languages. Typically, $e$ may contain further nested or (mutually) recursive function declarations. A declaration of the form (6.1) binds the formal parameters $x_{i}$ and the function name $f$ within $e$.

### 6.1.1 Variable Assignment Versus Binding

A language construct provided by almost all functional languages is the _let-binding_:

![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310090820866.png)


The effect of this expression is to evaluate $e_{1}$ and bind the resulting value to variable $x$ for the duration of the evaluation of $e_{2}$. The code affected by this binding, $e_{2}$, is called the _static scope_ of $x$ and is easily syntactically identifiable. In the following, we occasionally indicate scopes by code-enclosing boxes and list the variables that are in scope using subscripts.

For example, the scope associated with the top-most binding of $v$ to 3 in code
![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310090820421.png)

spans both inner let-bindings, the scopes of which are themselves not nested inside one other as the inner binding of $v$ occurs in the $e_{1}$ position of the let-binding for $y$.

In contrast to an assignment in an imperative language, a let-binding for variable $x$ hides any previous value bound to $x$ for the duration of evaluating $e_{2}$ but does not permanently overwrite it. Bindings are treated in a stack-like fashion, resulting in a tree-shaped nesting structure of boxes in our code excerpts. For example, in the above code, the inner binding of $v$ to value $2\times 3=6$ shadows the outer binding of $v$ to value 3 precisely for the duration of the evaluation of the expression $4\times v$. Once this evaluation has terminated (resulting in the binding of $y$ to 24), the binding of $v$ to 3 becomes visible again, yielding the overall result of 72.

The concepts of binding and static scope ensure that functional programs enjoy the characteristic feature of SSA, namely the fact that each use of a variable is uniquely associated with a point of definition. Indeed, the point of definition for a use of $x$ is given by the _nearest enclosing binding of_$x$. Occurrences of variables in an expression that are not enclosed by a binding are called _free_. A well-formed procedure declaration contains all free variables of its body among its formal parameters. Thus, the notion of scope makes explicit the invariant that each use of a variable should be dominated by its (unique) definition.

In contrast to SSA, functional languages achieve the association of definitions to uses without imposing the global uniqueness of variables, as witnessed by the duplicate binding occurrences for $v$ in the above code. As a consequence of this decoupling, functional languages enjoy a strong notion of _referential transparency_: The choice of $x$ as the variable holding the result of $e_{1}$ depends only on the freevariables of $e_{2}$. For example, we may rename the inner $v$ in code (6.2) to $z$ without altering the meaning of the code:

![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310090821321.png)


Note that this conversion formally makes the outer $v$ visible for the expression $4\times z$, as indicated by the index $v$, $z$ decorating its surrounding box.

In order to avoid altering the meaning of the program, the choice of the newly introduced variable has to be such that confusion with other variables is avoided. Formally, this means that a renaming
![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310090821069.png)
can only be carried out if $y$ is not a _free_ variable of $e_{2}$. Moreover, in the event that $e_{2}$ already contains some preexisting bindings to $y$, the _substitution_ of $x$ by $y$ in $e_{2}$ (denoted by $e_{2}$[$y \leftrightarrow x$] above) first renames these preexisting bindings in a suitable manner. Also note that the renaming only affects $e_{2}$--any occurrences of $x$ or $y$ in $e_{1}$ refer to conceptually _different_ but identically named variables, but the static scoping discipline ensures these will never be confused with the variables involved in the renaming. In general, the semantics-preserving renaming of bound variables is called $\alpha$-renaming. Typically, program analyses for functional languages are compatible with $\alpha$-renaming in that they behave equivalently for fragments that differ only in their choice of bound variables, and program transformations $\alpha$-rename bound variables whenever necessary.

A consequence of referential transparency, and thus a property typically enjoyed by functional languages, is _compositional equational reasoning_: the meaning of a piece of code $e$ is only dependent on its free variables and can be calculated from the meaning of its subexpressions. For example, the meaning of a phrase **let**$x$ = $e_{1}$**in**$e_{2}$**end** only depends on the free variables of $e_{1}$ and on the free variables of $e_{2}$_other than_$x$. Hence, languages with referential transparency allow one to replace a subexpression by some semantically equivalent phrase without altering the meaning of the surrounding code. Since semantic preservation is a core requirement of program transformations, the suitability of SSA for formulating and implementing such transformations can be explained by the proximity of SSA to functional languages.

### 6.1.2 Control Flow: Continuations

The correspondence between let-bindings and points of variable definition in assignments extends to other aspects of program structure, in particular to code in _continuation-passing-style_ (CPS), a program representation routinely used in compilers for functional languages [9].

Satisfying a roughly similar purpose as return addresses or function pointers in imperative languages, a continuation specifies how the execution should proceed once the evaluation of the current code fragment has terminated. Syntactically, continuations are expressions that may occur in functional position (i.e., are typically applied to argument expressions), as is the case for the variable $k$ in the following modification from code (6.2):

![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310090822686.png)
In effect, $k$ represents any function that may be applied to the result of expression (6.2).

Surrounding code may specify the concrete continuation by binding $k$ to a suitable expression. It is common practice to write these continuation-defining expressions in $\lambda$-notation, i.e., in the form $\lambda\,x.\,e$ where $x$ typically occurs free in $e$. The effect of the expression is to act as the (unnamed) function that sends $x$ to $e(x)$, i.e., formal parameter $x$ represents the place-holder for the argument to which the continuation is applied. Note that $x$ is $\alpha$-renameable, as $\lambda$ acts as a binder. For example, a client of the above code fragment wishing to multiply the result by 2 may insert code (6.4) in the $e_{2}$ position of a let-binding for $k$ that contains $\lambda\,x.\,2\times x$ in its $e_{1}$-position, as in the following code:

![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310090822732.png)
2
When the continuation $k$ is applied to the argument $y\times v$, the (dynamic) value $y\times v$ (i.e., 72) is substituted for $x$ in the expression $2\times x$, just like in an ordinary function application.

Alternatively, the client may wrap fragment (6.4) in a function definition with formal argument $k$ and construct the continuation in the calling code, where he would be free to choose a different name for the continuation-representing variable:
![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310090823977.png)

This makes CPS form a discipline of programming using higher-order functions, as continuations are constructed "on the fly" and communicated as arguments of other function calls. Typically, the caller of $f$ is itself parametric in _its_ continuation, as in

![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310090823358.png)

where $f$ is invoked with a newly constructed continuation $k^{\prime}$ that applies the addition of 7 to its formal argument $x$ (which at runtime will hold the result of $f$) before passing the resulting value on as an argument to the outer continuation $k$. In a similar way, the function

![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310090823069.png)


constructs from $k$ a continuation $k^{\prime}$ that is invoked (with different arguments) in each branch of the conditional. In effect, the sharing of $k^{\prime}$ amounts to the definition of a control-flow merge point, as indicated by the CFG corresponding to $h$ in Fig. 6.1a. Contrary to the functional representation, the top-level continuation parameter $k$ is not explicitly visible in the CFG--it roughly corresponds to the frame slot that holds the return address in an imperative procedure call.

![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310090823434.png)

The SSA form of this CFG is shown in Fig. 6.1b. If we apply similar renamings of $z$ to $z_{1}$ and $z_{2}$ in the two branches of (6.8), we obtain the following fragment:

![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310090823418.png)


We observe that the role of the formal parameter $z$ of continuation $k^{\prime}$ is exactly that of a $\phi$-function: to unify the arguments stemming from various calls sites by binding them to a common name for the duration of the ensuing code fragment--in this case just the return expression. As expected from the above understanding of scope and dominance, the scopes of the bindings for $z_{1}$ and $z_{2}$ coincide with the dominance regions of the identically named imperative variables: both terminate at the point of function invocation/jump to the control-flow merge point.

The fact that transforming (6.8) into (6.9) only involves the referentially transparent process of $\alpha$-renaming indicates that program (6.8) already contains the essential structural properties that SSA distills from an imperative program.

Programs in CPS equip _all_ function declarations with continuation arguments. By interspersing ordinary code with continuation-forming expressions as shown above, they model the flow of control exclusively by communicating, constructing, and invoking continuations.

### 6.1.3 Control Flow: Direct Style

An alternative to the explicit passing of continuation terms via additional function arguments is the _direct style_, in which we represent code as a set of locally named tail-recursive functions, for which the last operation is a call to a function, eliminating the need to save the return address.

In direct style, no continuation terms are constructed dynamically and then passed as function arguments, and we hence exclusively employ $\lambda$-free function definitions in our representation. For example, code (6.8) may be represented as
![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310090824429.png)
where the local function $h^{\prime}$ plays a similar role to the continuation $k^{\prime}$ and is jointly called from both branches. In contrast to the CPS representation, however, the body of $h^{\prime}$ returns its result directly rather than by passing it on as an argument to some continuation. Also note that neither the declaration of $h$ nor that of $h^{\prime}$ contains additional continuation parameters. Thus, rather than handing its result directly over to some caller-specified receiver (as communicated by the continuation argument $k$), $h$ simply returns control back to the caller, who is then responsible for any further execution. Roughly speaking, the effect is similar to the imperative compilation discipline of always setting the return address of a procedure call to the instruction pointer immediately following the call instruction.

A stricter format is obtained if the granularity of local functions is required to be that of basic blocks:
![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310090824460.png)
Now, function invocations correspond precisely to jumps, reflecting more directly the CFG from Fig. 6.1.

The choice between CPS and direct style is orthogonal to the granularity level of functions: both CPS and direct style are compatible with the strict notion of basic blocks but also with more relaxed formats such as extended basic blocks. In the extreme case, _all_ control-flow points are explicitly named, i.e., one local function or continuation is introduced per instruction. The exploration of the resulting design space is ongoing and has so far not yet led to a clear consensus in the literature. In our discussion below, we employ the arguably easier-to-read direct style, but the gist of the discussion applies equally well to CPS.

Independent of the granularity level of local functions, the process of moving from the CFG to the SSA form is again captured by suitably $\alpha$-renaming the bindings of $z$ in $h_{1}$ and $h_{2}$:
![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310090825223.png)
Again, the role of the formal parameter $z$ of the control-flow merge point function $h^{\prime}$ is identical to that of a $\phi$-function. In accordance with the fact that the basic blocks representing the arms of the conditional do not contain $\phi$-functions, the local functions $h_{1}$ and $h_{2}$ have empty parameter lists--the free occurrence of $y$ in the body of $h_{1}$ is bound at the top level by the formal argument of $h$.

### 6.1.4 Let-Normal Form

For both direct style and CPS the correspondence to SSA is most pronounced for code in _let-normal form_: Each intermediate result must be explicitly named by a variable, and function arguments must be names or constants. Syntactically, let-normal form isolates basic instructions in a separate category of primitive terms $a$ and then requires let-bindings to be of the form **let**$x$ = $a$**in**$e$**end**. In particular, neither jumps (conditional or unconditional) nor let-bindings are primitive. Let-normalized form is obtained by repeatedly rewriting code as follows:

![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310090825986.png)
subject to the side condition that y is not free in $e^{\prime \prime}$. For example, let-normalizing code (6.3) pulls the let-binding for z to the outside of the binding for y, yielding
![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310090825101.png)

Programs in let-normal form thus do not contain let-bindings in the $e_{1}$-position of outer let-expressions. The stack discipline in which let-bindings are managed is simplified as scopes are nested inside each other. While still enjoying referential transparency, let-normal code is in closer correspondence to imperative code than non-normalized code as the chain of nested let-bindings directly reflects the sequence of statements in a basic block, interspersed occasionally by the definition of continuations or local functions. Exploiting this correspondence, we apply a simplified notation in the rest of this chapter where the scope-identifying boxes (including their indices) are omitted and chains of let-normal bindings are abbreviated by single comma-separated (and ordered) let-blocks. Using this convention, code (6.13) becomes

![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310090826803.png)
Summarizing our discussion up to this point, Table 6.1 collects some correspon- dences between functional and imperative/SSA concepts.
![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310090827347.png)
![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310090827156.png)

## 6.2 Functional Construction and Destruction of SSA

The relationship between SSA and functional languages is extended by the correspondences shown in Table 6.2. We discuss some of these aspects by considering the translation into SSA, using the program in Fig. 6.2 as a running example.

### 6.2.1 Initial Construction Using Liveness Analysis

A simple way to represent this program in let-normalized direct style is to introduce one function $f_{i}$ for each basic block $b_{i}$. The body of each $f_{i}$ arises by introducing


one let-binding for each assignment and converting jumps into function calls. In order to determine the formal parameters of these functions we perform a liveness analysis. For each basic block $b_{i}$, we choose an arbitrary enumeration of its live-in variables. We then use this enumeration as the list of formal parameters in the declaration of the function $f_{i}$, and also as the list of actual arguments in calls to $f_{i}$. We collect all function definitions in a block of mutually tail-recursive functions at the top level:
![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310091524012.png)
The resulting program has the following properties:

* All function declarations are _closed_: The free variables of their bodies are contained in their formal parameter lists;1 Footnote 1: Apart from the function identifiers $f_{i}$, which can always be chosen distinct from the variables.
* Variable names are not unique, but the unique association of definitions to uses is satisfied;
* Each subterm $e_{2}$ of a let-binding **let $x$ = $e_{1}$ in $e_{2}$ end** corresponds to the direct control-flow successor of the assignment to $x$.

If desired, we may $\alpha$-rename to make names globally unique. As the function declarations in code (6.15) are closed, all variable renamings are independent from each other. The resulting code (6.16) corresponds precisely to the SSA program shown in Fig. 6.3 (see also Table 6.2): Each formal parameter of a function $f_{i}$ is the target of one $\phi$-function for the corresponding block $b_{i}$. The arguments of these $\phi$-functions are the arguments in the corresponding positions in the calls to $f_{i}$. As the number of arguments in each call to $f_{i}$ coincides with the number of formal parameters of $f_{i}$, the $\phi$-functions in $b_{i}$ are all of the same arity, namely the number of call sites to $f_{i}$. In order to coordinate the relative positioning of the arguments of the $\phi$-functions, we choose an arbitrary enumeration of these call sites.

![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310091952150.png)
![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310091952930.png)

Under this perspective, the above construction of parameter lists amounts to equipping each $b_{i}$ with $\phi$-functions for all its live-in variables, with subsequent renaming of the variables. Thus, the above method corresponds to the construction of _pruned_ (but not minimal) SSA--see Chap. 2.

While resulting in a legal SSA program, the construction clearly introduces more $\phi$-functions than necessary. Each superfluous $\phi$-function corresponds to the situation where all call sites to some function $f_{i}$ pass identical arguments. The technique for eliminating such arguments is called $\lambda$_-dropping_ and is the inverse of the more widely known transformation $\lambda$_-lifting_.

### 6.2.2 $\lambda$_-Dropping_

$\lambda$-dropping may be performed before or after variable names are made distinct, but for our purpose, the former option is more instructive. The transformation consists of two phases, _block sinking_ and _parameter dropping_.

#### 6.2.2.1 Block Sinking

Block sinking analyses the static call structure to identify which function definitions may be moved inside each other. For example, whenever our set of function declarations contains definitions $f(x_{1},\ldots,x_{n})=e_{f}$ and $g(y_{1},\ldots,y_{m})=e_{g}$ where $f\neq g$ and such that all calls to $f$ occur in $e_{f}$ or $e_{g}$, we can move the declaration for $f$ into that of $g$--note the similarity to the notion of dominance. If applied aggressively, block sinking indeed amounts to making the entire dominance tree structure explicit in the program representation. In particular, algorithms for computing the dominator tree from a CFG discussed elsewhere in this book can be applied to identify block sinking opportunities, where the CFG is given by the call graph of functions.

In our example (6.15), $f_{3}$ is only invoked from within $f_{2}$, and $f_{2}$ is only called in the bodies of $f_{2}$ and $f_{1}$ (see the dominator tree in Fig. 6.3 (right)). We may thus move the definition of $f_{3}$ into that of $f_{2}$, and the latter one into $f_{1}$.

Several options exist as to where $f$ should be placed in its host function. The first option is to place $f$ at the beginning of $g$, by rewriting to 
![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310091955481.png)

This transformation does not alter the semantic of the code, as the declaration of $f$ is closed: moving $f$ into the scope of the formal parameters $y_{1},\ldots,y_{m}$ (and also into the scope of $g$ itself) does not alter the bindings to which variable uses inside $e_{f}$ refer.

Applying this transformation to example (6.15) yields the following code:

![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310092004079.png)

An alternative strategy is to insert $f$ near the end of its host function $g$, in the vicinity of the calls to $f$. This brings the declaration of $f$ additionally into the scope of all let-bindings in $e_{g}$. Again, referential transparency and preservation of semantics are respected as the declaration on $f$ is closed. In our case, the alternative strategy yields the following code:
![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310092005260.png)
In general, one would insert $f$ directly prior to its call if $g$ contains only a single call site for $f$. In the event that $g$ contains multiple call sites for $f$, these are (due to their tail-recursive positioning) in different arms of a conditional, and we would insert $f$ directly prior to this conditional.

Both outlined placement strategies result in code whose nesting structure reflects the dominance relationship of the imperative code. In our example, code (6.17) and (6.18) both nest $f_{3}$ inside $f_{2}$ inside $f_{1}$, in accordance with the dominator tree of the imperative program show on Fig. 6.3.

#### 6.2.2.2 Parameter Dropping

The second phase of $\lambda$-dropping, _parameter dropping_, removes superfluous parameters based on the syntactic scope structure. Removing a parameter $x$ from the declaration of some function $f$ has the effect that any use of $x$ inside the body of $f$ will not be bound by the declaration of $f$ any longer, but by the (innermost) binding for $x$ that _contains_ the declaration of $f$. In order to ensure that removing the parameter does not alter the meaning of the program, we thus have to ensure that this $f$-containing binding for $x$ also contains any call to $f$, since the binding applicable at the call site determines the value that is passed as the actual argument (before $x$ is deleted from the parameter list).

For example, the two parameters of $f_{3}$ in (6.18) can be removed without altering the meaning of the code, as we can statically predict the values they will be instantiated with: Parameter $y$ will be instantiated with the result of $x\times z$ (which is bound to $y$ in the first line in the body of $f_{2}$), and $v$ will always be bound to the value passed via the parameter $v$ of $f_{2}$. In particular, the bindings for $y$ and $v$ at the _declaration_ site of $f_{3}$ are identical to those applicable at the _call_ to $f_{3}$.

In general, we may drop a parameter $x$ from the declaration of a possibly recursive function $f$ if the following two conditions are met:

1. The tightest scope for $x$ enclosing the declaration of $f$ coincides with the tightest scope for $x$ surrounding each call site to $f$ outside of its declaration.
2. The tightest scope for $x$ enclosing any recursive call to $f$ (i.e., a call to $f$ in the body of $f$) is the one associated with the formal parameter $x$ in the declaration of $f$.

The rationale for these clauses is that removing $x$ from $f$'s parameter list means that any free occurrence of $x$ in $f$'s body is now bound outside of $f$'s declaration. Therefore, for each call to $f$ to be correct, one needs to ensure that this outside binding coincides with the one containing the call.

Similarly, we may simultaneously drop a parameter $x$ occurring in _all_ declarations of a block of mutually recursive functions $f_{1}$,..., $f_{n}$, if the scope for $x$ at the point of declaration of the block coincides with the tightest scope in force at any call site to some $f_{i}$ outside the block, and if in each call to some $f_{i}$ inside some $f_{j}$, the tightest scope for $x$ is the one associated with the formal parameter $x$ of $f_{j}$. In both cases, dropping a parameter means removing it from the list of formal parameter lists of the function declarations concerned, and also from the argument lists of the corresponding function calls.

In code (6.18), these conditions sanction the removal of both parameters from the non-recursive function $f_{3}$. The scope applicable for $v$ at the site of declaration of $f_{3}$ and also at its call site is the one rooted at the formal parameter $v$ of $f_{2}$. In case of $y$, the common scope is the one rooted at the let-binding for $y$ in the body of $f_{2}$. We thus obtain the following code:
![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310092005963.png)
Considering the recursive function $f_{2}$ next we observe that the recursive call is in the scope of the let-binding for $y$ in the body of $f_{2}$, preventing us from removing $y$. In contrast, neither $v$ nor $z$ has binding occurrences in the body of $f_{2}$. The scopes applicable at the external call site to $f_{2}$ coincide with those applicable at its site of declaration and are given by the scopes rooted in the let-bindings for $v$ and $z$. Thus, parameters $v$ and $z$ may be removed from $f_{2}$:
![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310092006793.png)

Interpreting the uniquely renamed variant of (6.20) back in SSA yields the desired code with a single $\phi$-function, for variable $y$ at the beginning of block $b_{2}$, see Fig. 6.4. The reason that this $\phi$-function cannot be eliminated (the redefinition of $y$ in the loop) is precisely the reason why $y$ survives parameter dropping.

Given this understanding of parameter dropping we can also see why inserting functions near the end of their hosts during block sinking (as in code (6.18)) is in general preferable to inserting them at the beginning of their hosts (as in code (6.17)): The placement of function declarations in the vicinity of their calls potentially enables the dropping of more parameters, namely those that are let-bound in the body of the host function.

![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310092006828.png)

An immediate consequence of this strategy is that blocks with a single direct predecessor indeed do not contain $\phi$-functions. Such a block $b_{f}$ is necessarily dominated by its direct predecessor $b_{g}$, hence we can always nest $f$ inside $g$. Inserting $f$ in $e_{g}$ directly prior to its call site implies that condition (1) is necessarily satisfied for all parameters $x$ of $f$. Thus, all parameters of $f$ can be dropped and no $\phi$-function is generated.

### 6.2.3 Nesting, Dominance, Loop-Closure

As we observed above, analysing whether function definitions may be nested inside one another is tantamount to analysing the imperative dominance structure: function $f_{i}$ may be moved inside $f_{j}$ exactly if all non-recursive calls to $f_{i}$ come from within $f_{j}$, i.e., exactly if all paths from the initial program point to block $b_{i}$ traverse $b_{j}$, i.e., exactly if $b_{j}$ dominates $b_{i}$. This observation is merely the extension to function identifiers of our earlier remark that lexical scope coincides with the dominance region, and that points of definition/binding occurrences should dominate the uses. Indeed, functional languages do not distinguish between code and data when aspects of binding and use of variables are concerned, as witnessed by our use of the let-binding construct for binding code-representing expressions to the variables $k$ in our syntax for CPS.

Thus, the dominator tree immediately suggests a function nesting scheme, where all children of a node are represented as a single block of mutually recursive function declarations.[^2]

[^2]: Refinements of this representation will be sketched in Sect. 6.3.

The choice as to where functions are placed corresponds to variants of SSA. For example, in loop-closed SSA form (see Chaps. 14 and 10), SSA names that are defined in a loop must not be used outside the loop. To this end, special-purpose unary $\phi$-nodes are inserted for these variables at the loop exit points. As the loop is unrolled, the arity of these trivial $\phi$-nodes grows with the number of unrollings, and the program continuation is always supplied with the value the variable obtained in the final iteration of the loop. In our example, the only loop-defined variable used in $f_{3}$ is $y$--and we already observed in code (6.17) how we can prevent the dropping of $y$ from the parameter list of $f_{3}$: we insert $f_{3}$ at the _beginning_ of $f_{2}$, preceding the let-binding for $y$. Of course, we would still like to drop as many parameters from $f_{2}$ as possible, hence we apply the following placement policy during block sinking: Functions that are targets of loop-exiting function calls and have live-in variables that are defined in the loop are placed at the beginning of the loop headers. Other functions are placed at the end of their hosts. Applying this policy to our original program (6.15) yields (6.21).

![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310092008105.png)

We may now drop $v$ (but not $y$) from the parameter list of $f_{3}$, and $v$ and $z$ from $f_{2}$, to obtain code (6.22).

![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310092020981.png)
The SSA form corresponding to (6.22) contains the desired loop-closing $\phi$-node for $y$ at the beginning of $b_{3}$, as shown in Fig. 6.5a. The nesting structure of both (6.21) and (6.22) coincides with the dominance structure of the original imperative code and its loop-closed SSA form.
![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310092020657.png)
We unroll the loop by duplicating the body of $f_{2}$, _without duplicating the declaration of_$f_{3}$:

![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310092021984.png)

Both calls to $f_{3}$ are in the scope of the declaration of $f_{3}$ and contain the appropriate loop-closing arguments. In the SSA reading of this code--shown in Fig. 6.5b--the first instruction in $b_{3}$ has turned into a non-trivial $\phi$-node. As expected, the parameters of this $\phi$-node correspond to the two control-flow arcs leading into $b_{3}$, one for each call site to $f_{3}$ in code (6.23). Moreover, the call and nesting structure of (6.23) is indeed in agreement with the control flow and dominance structure of the loop-unrolled SSA representation.

### 6.2.4 Destruction of SSA

The above example code excerpts where variables are not made distinct exhibit a further pattern: The argument list of any call coincides with the list of formal parameters of the invoked function. This discipline is not enjoyed by functional programs in general, and is often destroyed by optimizing program transformations. However, programs that do obey this discipline can be immediately converted to imperative non-SSA form. Thus, the task of SSA destruction amounts to converting a functional program with arbitrary argument lists into one where argument lists and formal parameter lists coincide for each function. This can be achieved by introducing additional let-bindings of the form $\textbf{let x = y  in e end}$. For example, a call $f(v,z,y)$ where $f$ is declared as  $\textbf{function f(x, y, z)=e}$ may be converted to
![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310092024679.png)
in correspondence to the move instructions introduced in imperative formulations of SSA destruction (see Chaps. 3 and 21). Appropriate transformations can be formulated as manipulations of the functional representation, although the target format is not immune to $\alpha$-renaming and thus only syntactically a functional language. For example, we can give a local algorithm that considers each call site individually and avoids the "lost-copy" and "swap" problems (cf. Chap. 21): Instead of introducing let-bindings for all parameter positions of a call, the algorithm scales with the number and size of cycles that span identically named arguments and parameters (like the cycle between $y$ and $z$ above), and employs a single additional variable (called $a$ in the above code) to break all these cycles one by one.

## 6.3 Refined Block Sinking and Loop Nesting Forests

As discussed when outlining $\lambda$-dropping, block sinking is governed by the dominance relation between basic blocks. Thus, a typical dominance tree with root $b$ and subtrees rooted at $b_{1},\ldots,b_{n}$ is most naturally represented as a block of function declarations for the $f_{i}$, nested inside the declaration of $f$:
![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310092024587.png)
By exploiting additional control-flow structure between the $b_{i}$, it is possible to obtain refined placements, namely placements that correspond to notions of _loop nesting forests_ that have been identified in the SSA literature.

These refinements arise if we enrich the above dominance tree by adding arrows $b_{i}\to b_{j}$ whenever the CFG contains a directed edge from one of the dominance successors of $b_{i}$ (i.e., the descendants of $b_{i}$ in the dominance tree) to $b_{j}$.

In the case of a _reducible_ CFG, the resulting graph contains only trivial loops. Ignoring these self-loops, we perform a post-order DFS (or more generally a reverse topological ordering) among the $b_{i}$ and stagger the function declarations according to the resulting order. As an example, consider the CFG in Fig. 6.6a and its enriched dominance tree shown in Fig. 6.6b. A possible (but not unique) ordering of the children of $b$ is $\left[b_{5}, b_{1}, b_{3}, b_{2}, b_{4}\right]$, resulting in the nesting shown in code (6.25).

![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310092026412.png)

![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310092026342.png)
The code respects the dominance relationship in much the same way as the naive placement, but additionally makes $f_{1}$ inaccessible from within $e_{5}$, and makes $f_{3}$ inaccessible from within $f_{1}$ or $f_{5}$. As the reordering does not move function declarations _inside_ each other (in particular: no function declaration is brought into or moved out of the scope of the formal parameters of any other function) the reordering does not affect the potential to subsequently perform parameter dropping.

Declaring functions using $\lambda$-abstraction brings further improvements. This enables us not only to syntactically distinguish between loops and non-recursive control-flow structures using the distinction between **let** and **letrec** present in many functional languages, but also to further restrict the visibility of function names. Indeed, while $b_{3}$ is immediately dominated by $b$ in the above example, its only control-flow predecessors are $b_{2}/g$ and $b_{4}$. We would hence like to make the declaration of $f_{3}$ local to the _tuple_$(f_{2},\,f_{4})$, i.e., invisible to $f$. This can be achieved by combining **let/letrec** bindings with pattern matching, if we insert the shared declaration of $f_{3}$_between_ the declaration of the names $f_{2}$ and $f_{4}$ and the $\lambda$-bindings of their formal parameters $p_{i}$:
![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310092027959.png)


The recursiveness of $f_{4}$ is inherited by the function pair $\left(f_{2}, f_{4}\right)$ but f_{3} remains non-recursive. In general, the role of $f_{3}$ is played by any merge point $b_{i}$ that is not directly called from the dominator node $b$.
In the case of irreducible CFGs, the enriched dominance tree is no longer acyclic (even when ignoring self-loops). In this case, the functional representation not only depends on the chosen DFS order but additionally on the partitioning of the enriched graph into loops. As each loop forms a strongly connected component (SCC), different partitionings are possible, corresponding to different notions of loop nesting forests. Of the various loop nesting forest strategies proposed in the SSA literature [236], the scheme introduced by Steensgaard [271] is particularly appealing from a functional perspective.
In Steensgaard's notion of loops, the headers $H$ of a loop $L=(B, H)$ are precisely the entry nodes of its body B, i.e., those nodes in B that have a direct predecessor outside of B. For example, $G_{0}$ shown in Fig. 6.7 contains the outer loop $L_{0}=(\{u, v, w, x\},\{u, v\})$, whose constituents $B_{0}$ are determined as the maximal SCC of G_{0}. Removing the back edges of L_{0} from G_{0} (i.e., edges from B_{0} to H_{0} ) yields G_{1}, whose (only) SCC determines a further inner loop $L_{1}=(\{w, x\},\{w, x\})$. Removing the back edges of $L_{1}$ from $G_{1}$ results in the acyclic $G_{2}$, terminating the process.

Figure 6.8a shows the CFG-enriched dominance tree of $G_{0}$. The body of loop $L_{0}$ is easily identified as the maximal SCC, and likewise the body of $L_{1}$ once the cycles (u, w) and (x, v) are broken by the removal of the back edges $w \rightarrow u$ and $x \rightarrow v$.
The loop nesting forest resulting from Steensgaard's construction is shown in Fig. 6.8b. Loops are drawn as ellipses decorated with the appropriate header nodes and nested in accordance with the containment relation $B_{1} \subset B_{0}$ between the bodies.

In the functional representation, a loop $L=\left(B,\left\{h_{1}, \ldots, h_{n}\right\}\right)$ yields a function declaration block for functions $h_{1}, \ldots, h_{n}$, with private declarations for the nonheaders from $B \backslash H$. In our example, loop $L_{0}$ provides entry points for the headers u and v but not for its non-headers w and x. Instead, the loop comprised of the latter nodes, $L_{1}$, is nested inside the definition of $L_{0}$, in accordance with the loop nesting forest.
![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310092031531.png)

By placing $L_{1}$ inside $L_{0}$ according to the scheme from code (6.26) and making **exit** private to $L_{1}$, we obtain the representation (6.27), which captures all the essential information of Steensgaard's construction. Effectively, the functional reading of the loop nesting forest extends the earlier correspondence between the nesting of individual functions and the dominance relationship to groups of functions and basic blocks: loop $L_{0}$ $\text{dominates}$ $L_{1}$ in the sense that any path from **entry** to a node in $L_{1}$ passes through $L_{0}$; more specifically, any path from entry to a header of $L_{1}$ passes through a header of $L_{0}$.
In general, each step of Steensgaard's construction may identify several loops, as a CFG may contain several maximal SCCs. As the bodies of these SCCs are necessarily non-overlapping, the construction yields a forest comprised of trees shaped like the loop nesting forest in Fig. 6.8b. As the relationship between the trees is necessarily acyclic, the declarations of the function declaration tuples corresponding to the trees can be placed according to the loop-extended notion of dominance.


## 6.4 Further Reading and Concluding Remarks
## Further Reading

Shortly after the introduction of SSA, O'Donnell [213] and Kelsey [160] noted the correspondence between let-bindings and points of variable declaration and its extension to other aspects of program structure using continuation-passing style. Appel [10,11] popularized the correspondence using a direct-style representation, building on his earlier experience with continuation-based compilation [9].

Continuations and low-level functional languages have been an object of intensive study since their inception about four decades ago [174, 294]. For retrospective accounts of the historical development, see Reynolds [246] and Wadsworth [299]. Early studies of CPS and direct style include work by Reynolds and Plotkin [229, 244, 245]. Two prominent examples of CPS-based compilers are those by Sussman et al. [279] and Appel [9]. An active area of research concerns the relative merit of the various functional representations, algorithms for formal conversion between these formats, and their efficient compilation to machine code, in particular with respect to their integration with program analyses and optimizing transformations [92, 161, 243]. A particularly appealing variant is that of Kennedy [161], where the approach to explicitly name control-flow points such as merge points is taken to its logical conclusion. By mandating _all_ control-flow points to be explicitly named, a uniform representation is obtained that allows optimizing transformations to be implemented efficiently, avoiding the administrative overhead to which some of the alternative approaches are susceptible.

Occasionally, the term _direct style_ refers to the combination of tail-recursive functions and let-normal form, and the conditions on the latter notion are strengthened so that only variables may appear as branch conditions. Variations of this discipline include _administrative normal form_ (A-normal form, ANF) [121], B-form [280], and SIL [285].

Closely related to continuations and direct-style representation are _monadic_ intermediate languages as used by Benton et al. [24] and Peyton-Jones et al. [223]. These partition expressions into categories of _values_ and _computations_, similar to the isolation of primitive terms in let-normal form [229, 245]. This allows one to treat side-effects (memory access, IO, exceptions, etc.) in a uniform way, following Moggi [200], and thus simplifies reasoning about program analyses and the associated transformations in the presence of impure language features.

Lambda-lifting and dropping are well-known transformations in the functional programming community, and are studied in-depth by Johnsson [155] and Danvy et al. [91].

Rideau et al. [247] present an in-depth study of SSA destruction, including a verified implementation in the proof assistant Coq for the "windmills" problem, i.e., the task of correctly introducing $\phi$-compensating assignments. The local algorithm to avoid the lost-copy problem and swap problem identified by Briggs et al. [50] was given by Beringer [25]. In this solution, the algorithm to break the cycles is in line with the results of May [196].

We are not aware of previous work that transfers the analysis of loop nesting forests to the functional setting, or of loop analyses in the functional world that correspond to loop nesting forests. Our discussion of Steensgaard's construction was based on a classification of loop nesting forests by Ramalingam [236], which also served as the source of the example in Fig. 6.7. Two alternative constructions discussed by Ramalingam are those by Sreedhar, Gao and Lee [266], and Havlak [142]. To us, it appears that a functional reading of Sreedhar, Gao, and Lee's construction would essentially yield the nesting mentioned at the beginning of Sect. 6.3. Regarding Havlak's construction, the fact that entry points of loops are not necessarily classified as headers appears to make an elegant representation in functional form at least challenging.

Extending the syntactic correspondences between SSA and functional languages, similarities may be identified between their characteristic program analysis frameworks, _data-flow analyses_ and _type systems_. Chakravarty et al. [62] prove the correctness of a functional representation of Wegmann and Zadeck's SSAbased sparse conditional constant propagation algorithm [303]. Beringer et al. [26] consider data-flow equations for liveness and read-once variables, and formally translate their solutions to properties of corresponding typing derivations. Laud et al. [177] present a formal correspondence between data-flow analyses and type systems but consider a simple imperative language rather than SSA.

At present, intermediate languages in functional compilers do not provide syntactic support for expressing nesting forest directly. Indeed, most functional compilers do not perform advanced analyses of nested loops. As an exception to this rule, the MLton compiler ([http://mlton.org](http://mlton.org)) implements Steensgaard's algorithm for detecting loop nesting forests, leading to a subsequent analysis of the loop unrolling and loop switching transformations [278].

## Concluding Remarks

In addition to low-level functional languages, alternative representations for SSA have been proposed, but their discussion is beyond the scope of this chapter. Glesner [129] employs an encoding in terms of abstract state machines [134] that disregards the sequential control flow inside basic blocks but retains control flow between basic blocks to prove the correctness of a code generation transformation. Later work by the same author uses a more direct representation of SSA in the theorem prover Isabelle/HOL for studying further SSA-based analyses.

Matsuno and Ohori [195] present a formalism that captures core aspects of SSA in a notion of (non-standard) types while leaving the program text in unstructured non-SSA form. Instead of classifying values or computations, types represent the definition points of variables. Contexts associate program variables at each program point with types in the standard fashion, but the non-standard notion of types means that this association models the reaching-definitions relationship rather than characterizing the values held in the variables at runtime. Noting a correspondence between the types associated with a variable and the sets of def-use paths, the authors admit types to be formulated over type variables whose introduction and use corresponds to the introduction of $\phi$-nodes in SSA.

Finally, Pop et al.'s model [233] dispenses with control flow entirely and instead views programs as sets of equations that model the assignment of values to variables in a style reminiscent of partial recursive functions. This model is discussed in more detail in Chap. 10.