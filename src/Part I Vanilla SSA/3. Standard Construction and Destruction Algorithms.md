# Chapter 3. Standard Construction and Destruction Algorithms


Jeremy Singer and Fabrice Rastello

This chapter describes the standard algorithms for construction and destruction of SSA form. SSA _construction_ refers to the process of translating a non-SSA program into one that satisfies the SSA constraints. In general, this transformation occurs as one of the earliest phases in the middle-end of an optimizing compiler, when the program has been converted to three-address intermediate code. SSA _destruction_ is sometimes called out-of-SSA translation. This step generally takes place in an optimizing compiler after all SSA optimizations have been performed, and prior to code generation. Note however that there are specialized code generation techniques that can work directly on SSA-based intermediate representations such as instruction selection (see Chap. 19), if-conversion (see Chap. 20), and register allocation (see Chap. 22).

The algorithms presented in this chapter are based on material from the seminal research papers on SSA. These original algorithms are straightforward to implement and have acceptable efficiency. Therefore such algorithms are widely implemented in current compilers. Note that more efficient, albeit more complex, alternative algorithms have been devised. These are described further in Chaps. 4 and 21.

![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310090745901.png)

Figure 3.1 shows the control-flow graph (CFG) of an example program. The set of nodes is {_r_, $A$, $B$, $C$, $D$, _E_}, and the variables used are {_x_, $y$, _tmp_}. Note that the program shows the complete control-flow structure, denoted by directed edges between the nodes. However, the program only shows statements that define relevant variables, together with the unique return statement at the exit point of the CFG. All of the program variables are undefined on entry. On certain control-flow paths,some variables may be used without being defined, e.g., $x$ on the path $r\to A\to C$. We discuss this issue later in the chapter. We intend to use this program as a running example throughout the chapter, to demonstrate various aspects of SSA construction.

## 3.1 Construction

The original construction algorithm for SSA form consists of two distinct phases.

1. $\phi$**-function insertion** performs _live range splitting_ to ensure that any use of a given variable $v$ is reached1 by exactly one definition of $v$. The resulting live ranges exhibit the property of having a single definition, which occurs at the beginning of each live range. Footnote 1: A program point $p$ is said to be _reachable_ by a definition of $v$ if there exists a path in the CFG from that definition to $p$ that does not contain any other definition of $v$.
2. **Variable renaming** assigns a unique variable name to each live range. This second phase rewrites variable names in program statements such that the program text contains only one definition of each variable, and every use refers to its corresponding unique reaching definition.

As already outlined in Chap. 2, there are different flavours of SSA with distinct properties. In this chapter, we focus on the _minimal_ SSA form.
### 3.1.1 Join Sets and Dominance Frontiers

In order to explain how $\phi$-function insertions occur, it will be helpful to review the related concepts of _join sets_ and _dominance frontiers_.

For a given set of nodes $S$ in a CFG, the join set $\mathscr{J}(S)$ is the set of _join nodes_ of $S$, i.e., nodes in the CFG that can be reached by two (or more) distinct elements of $S$ using disjoint paths. Join sets were introduced in Chap. 2, Sect. 2.2.

Let us consider some join set examples from the program in Fig. 1.

1. $\mathscr{J}(\{B,C\})=\{D\}$, since it is possible to get from $B$ to $D$ and from $C$ to $D$ along different, non-overlapping, paths.
2. Again, $\mathscr{J}(\{r,A,B,C,D,E\})=\{A,D,E\}$ (where $r$ is the entry), since the nodes $A$, $D$, and $E$ are the only nodes with multiple direct predecessors in the program.

The _dominance frontier_ of a node $n$, $\mathrm{DF}(n)$, is the border of the CFG region that is dominated by $n$. More formally,

* Node $x$_strictly dominates_ node $y$ if $x$ dominates $y$ and $x\neq y$.
* The set of nodes $\mathrm{DF}(n)$ contains all nodes $x$ such that $n$ dominates a direct predecessor of $x$ but $n$ does not strictly dominate $x$.

For instance, in our Fig. 1, the dominance frontier of the $y$ defined in block B is the first operation of D, while the DF of the $y$ defined in block C would be the first operations of D and E.

Note that DF is defined over individual nodes, but for simplicity of presentation, we overload it to operate over sets of nodes too, i.e., $\mathrm{DF}(S)=\bigcup_{s\in S}\mathrm{DF}(s)$. The _iterated dominance frontier_$\mathrm{DF}^{+}(S)$ is obtained by iterating the computation of DF until reaching a fixed point, i.e., it is the limit $DF_{i\to\infty}(S)$ of the sequence:
![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310090746811.png)
Construction of minimal SSA requires for each variable $v$ the insertion of $\phi$-functions at $\mathscr{J}(\mathrm{Defs}(v))$, where $\mathrm{Defs}(v)$ is the set of nodes that contain definitions of $v$. The original construction algorithm for SSA form uses the iterated dominance frontier $\mathrm{DF}^{+}(\mathrm{Defs}(v))$. This is an over-approximation of join set, since $\mathrm{DF}^{+}(S)=\mathscr{J}(S\cup\{r\})$, i.e., the original algorithm assumes an _implicit_ definition of every variable at the entry node $r$.

### 3.1.2  $\phi$-Function Insertion

This concept of dominance frontiers naturally leads to a straightforward approach that places $\phi$-functions on a per-variable basis. For a given variable $v$, we place $\phi$-functions at the iterated dominance frontier $\mathrm{DF}^{+}(\mathrm{Defs}(v))$ where $\mathrm{Defs}(v)$ is the set of nodes containing definitions of $v$. This leads to the construction of SSA form that has the dominance property, i.e., where each definition of each renamed variable dominates its entire live range.

![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310090747389.png)

Consider again our running example from Fig. 3.1. The set of nodes containing definitions of variable $x$ is $\{B,\,C,\,D\}$. The iterated dominance frontier of this set is $\{A,\,D,\,E\}$ (it is also the DF, no iteration needed here). Hence we need to insert $\phi$-functions for $x$ at the beginning of nodes $A$, $D$, and $E$. Figure 3.2 shows the example CFG program with $\phi$-functions for $x$ inserted.

As far as the actual algorithm for $\phi$-functions insertion is concerned, we will assume that the dominance frontier of each CFG node is pre-computed and that the iterated dominance frontier is computed on the fly, as the algorithm proceeds. The algorithm works by inserting $\phi$-functions iteratively using a worklist of definition points, and flags (to avoid multiple insertions). The corresponding pseudo-code for $\phi$-function insertion is given in Algorithm 3.1. The worklist of nodes $W$ is used to record definition points that the algorithm has not yet processed, i.e., it has not yet inserted $\phi$-functions at their dominance frontiers. Because a $\phi$-function is itself a definition, it may require further $\phi$-functions to be inserted. This is the cause of node insertions into the worklist $W$ during iterations of the inner loop in Algorithm 3.1. Effectively, we compute the iterated dominance frontier on the fly. The set $F$ is used to avoid repeated insertion of $\phi$-functions on a single block. Dominance frontiers of distinct nodes may intersect, e.g., in the example CFG in Fig. 3.1, $\mathrm{DF}(B)$ and $\mathrm{DF}(C)$ both contain $D$, but once a $\phi$-function for a particular variable has been inserted at a node, there is no need to insert another, since a single $\phi$-function per variable handles all incoming definitions of that variable to that node.
![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310090748372.png)

We give a walkthrough example of Algorithm 3 in Table 1. It shows the stages of execution for a single iteration of the outermost for loop, inserting $\phi$-functions for variable $x$. Each row represents a single iteration of the while loop that iterates over the worklist $W$. The table shows the values of $X$, $F$, and $W$ at the start of each while loop iteration. At the beginning, the CFG looks like Fig. 1. At the end, when all the $\phi$-functions for $x$ have been placed, then the CFG looks like Fig. 2.

![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310090748532.png)
Provided that the dominator tree is given, the computation of the dominance frontier is quite straightforward. As illustrated by Fig. 3, this can be understood using the DJ-graph notation. The skeleton of the DJ-graph is the dominator tree of the CFG that makes the D-edges (dominance edges). This is augmented with J-edges (join edges) that correspond to all edges of the CFG whose source does not strictly dominate its destination. A DF-edge (dominance frontier edge) is an edge whose destination is in the dominance frontier of its source. By definition, there is a DF-edge $(a,b)$ between every CFG nodes $a$, $b$ such that $a$ dominates a direct predecessor of $b$, but does not strictly dominate $b$. In other words, for each $J$-edge $(a,b)$, all ancestors of $a$ (including $a$) that do not strictly dominate $b$ have $b$ in their dominance frontier. For example, in Fig. 3, $(F,\,G)$ is a J-edge, so $\{(F,\,G),\,\,(E,\,G),\,\,(B,\,G)\}$ are DF-edges. This leads to the pseudo-code given in Algorithm 2, where for every edge $(a,b)$ we visit all ancestors of $a$ to add $b$ to their dominance frontier.

Since the iterated dominance frontier is simply the transitive closure of the dominance frontier, we can define the $\mathrm{DF}^{+}$-graph as the transitive closure of the DF-graph. In our example, as $\{(C,E),\ (E,G)\}$ are DF-edges, $(C,G)$ is a $\mathrm{DF}^{+}$-edge. Hence, a definition of $x$ in $C$ will lead to inserting $\phi$-functions in $E$ and $G$. We can compute the iterated dominance frontier for each variable independently, as outlined in this chapter, or "cache" it to avoid repeated computation of the iterated dominance frontier of the same node. This leads to more sophisticated algorithms detailed in Chap. 4.

![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310090749349.png)

Once $\phi$-functions have been inserted using this algorithm, the program usually still contains several definitions per variable; however, now there is a single definition statement in the CFG that reaches each use. For each variable use in a $\phi$-function, it is conventional to treat them as if the use actually occurs on the corresponding incoming edge or at the end of the corresponding direct predecessor node. If we follow this convention, then def-use chains are aligned with the CFG dominator tree. In other words, _the single definition that reaches each use dominates that use_.

Figure 3: An example CFG and its corresponding DJ-graph ($D$-edges are top-down), DF-graph, and $\mathrm{DF}^{+}$-graph

### 3.1.3 Variable Renaming

To obtain the desired property of a Static Single Assignment per variable, it is necessary to perform variable renaming, which is the second phase in the SSA construction process. $\phi$-function insertions have the effect of splitting the live range(s) of each original variable into pieces. The variable renaming phase associates each individual live range with a new variable name, also called a _version_. The pseudo-code for this process is presented in Algorithm 3.3. Because of the dominance property outlined above, it is straightforward to rename variables using a depth-first traversal of the dominator tree. During the traversal, for each variable $v$, it is necessary to remember the version of its unique reaching definition at some point $p$ in the graph. This corresponds to the closest definition that dominates $p$. In Algorithm 3.3, we compute and cache the reaching definition for $v$ in the per-variable slot "$v$.reachingDef" that is updated as the algorithm traverses the dominator tree of the SSA graph. This per-variable slot stores the in-scope, "new" variable name (version) for the equivalent variable at the same point in the un-renamed program.

![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310090749292.png)
![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310090750580.png)

The variable renaming algorithm translates our running example from Fig. 3.1 into the SSA form of Fig. 3.4a. The table in Fig. 3.4b gives a walkthrough example of Algorithm 3.3, only considering variable $x$. The labels $l_{i}$ mark instructions in the program that mention $x$, shown in Fig. 3.4a. The table records (1) when$x$.reachingDef is updated from $x_{\text{old}}$ into $x_{\text{new}}$ due to a call of updateReachingDef, and (2) when $x$.reachingDef is $x_{\text{old}}$ before a definition statement, then $x_{\text{new}}$ afterwards.

![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310090750548.png)

The original presentation of the renaming algorithm uses a per-variable stack that stores all variable versions that dominate the current program point, rather than the slot-based approach outlined above. In the stack-based algorithm, a stack value is pushed when a variable definition is processed (while we explicitly update the reachingDef field at this point). The top stack value is peeked when a variable use is encountered (we read from the reachingDef field at this point). Multiple stack values may be popped when moving to a different node in the dominator tree (we always check whether we need to update the reachingDef field before we read from it). While the slot-based algorithm requires more memory, it can take advantage of an existing working field for a variable and be more efficient in practice.

### 3.1.4 Summary

Now let us review the flavour of SSA form that this simple construction algorithm produces. We refer back to several SSA properties that were introduced in Chap. 2.

* It is _minimal_ (see Sect. 2.2). After the $\phi$-function insertion phase, but before variable renaming, the CFG contains the minimal number of inserted $\phi$-functions to achieve the property that exactly one definition of each variable $v$ reaches every point in the graph.
* It is _not pruned_ (see Sect. 2.4). Some of the inserted $\phi$-functions may be dead, i.e., there is not always an explicit use of the variable subsequent to the $\phi$-function (e.g., $y_{5}$ in Fig. 3.4a).
* It is _conventional_ (see Sect. 2.5). The transformation that renames all $\phi$-related variables into a unique representative name and then removes all $\phi$-functions is a correct SSA destruction algorithm.
* Finally, it has the _dominance_ property (that is, it is _strict_--see Sect. 2.3). Each variable use is dominated by its unique definition. This is due to the use of iterated dominance frontiers during the $\phi$-placement phase, rather than join sets. Whenever the iterated dominance frontier of the set of definition points of a variable differs from its join set, then at least one program point can be reached both by $r$ (the entry of the CFG) and one of the definition points. In other words, as in Fig. 3.1, one of the uses of the $\phi$-function inserted in block $A$ for $x$ does not have any actual reaching definition that dominates it. This corresponds to the $\perp$ value used to initialize each reachingDef slot in Algorithm 3.3. Actual implementation code can use a NULL value, create a fake undefined variable at the entry of the CFG, or create undefined pseudo-operations on the fly just before the particular use.

## 3.2 Destruction

SSA form is a sparse representation of program information, which enables simple, efficient code analysis and optimization. Once we have completed SSA-based optimization passes, and certainly before code generation, it is necessary to eliminate $\phi$-functions since these are not executable machine instructions. This elimination phase is known as _SSA destruction_.

When freshly constructed, an untransformed SSA code is conventional and its destruction is straightforward: One simply has to rename all $\phi$-related variables (source and destination operands of the same $\phi$-function) into a unique representative variable. Then, each $\phi$-function should have syntactically identical names for all its operands, and thus can be removed to coalesce the related live ranges.

We refer to a set of $\phi$-related variables as a $\phi$-web. We recall from Chap. 2 that conventional SSA is defined as a flavour under which each $\phi$-web is free from interferences. Hence, if all variables of a $\phi$-web have non-overlapping live ranges, then the SSA form is conventional. The discovery of $\phi$-webs can be performed efficiently using the classical _union-find_ algorithm with a disjoint-set data structure, which keeps track of a set of elements partitioned into a number of disjoint (non-overlapping) subsets. The $\phi$-webs discovery algorithm is presented in Algorithm 3.4.

![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310090751881.png)


While freshly constructed SSA code is conventional, this may not be the case after performing some optimizations such as copy propagation. Going back to conventional SSA form requires the insertion of copies. The simplest (although not the most efficient) way to destroy non-conventional SSA form is to split all _critical edges_, and then replace $\phi$-functions by copies at the end of direct predecessor basic blocks. A critical edge is an edge from a node with several direct successors to a node with several direct predecessors. The process of splitting an edge, say $(b_{1},b_{2})$, involves replacing edge $(b_{1},b_{2})$ by (1) an edge from $b_{1}$ to a freshly created basic block and by (2) another edge from this fresh basic block to $b_{2}$. As $\phi$-functions have a parallel semantic, i.e., have to be executed simultaneously not sequentially, the same holds for the corresponding copies inserted at the end of direct predecessor basic blocks. To this end, a pseudo instruction called a _parallel copy_ is created to represent a set of copies that have to be executed in parallel. The replacement of parallel copies by sequences of simple copies is handled later on. Algorithm 3.5 presents the corresponding pseudo-code that makes non-conventional SSA conventional. As already mentioned, SSA destruction of such form is straightforward. However, Algorithm 3.5 can be slightly modified to directly destruct SSA by deleting line 13, replacing $a^{\prime}_{i}$ by $a_{0}$ in the following lines, and adding "remove the $\phi$-function" after them.
![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310090751413.png)
We stress that the above destruction technique has several drawbacks: first because of specific architectural constraints, region boundaries, or exception handling code, the compiler might not permit the splitting of a given edge; second, the resulting code contains many temporary-to-temporary copy operations. In theory, reducing the frequency of these copies is the role of the coalescing during the register allocation phase. A few memory- and time-consuming coalescing heuristics mentioned in Chap. 22 can handle the removal of these copies effectively. Coalescing can also, with less effort, be performed prior to the register allocation phase. As opposed to a (so-called conservative) coalescing during register allocation, this _aggressive_ coalescing would not cope with the interference graph colourability. Further, the process of copy insertion itself might take a substantial amount of time and might not be suitable for dynamic compilation. The goal of Chap. 21 is to cope both with non-splittable edges and difficulties related to SSA destruction at machine code level, but also aggressive coalescing in the context of resource constrained compilation.

Once $\phi$-functions have been replaced by parallel copies, we need to sequentialize the parallel copies, i.e., replace them by a sequence of simple copies. This phase can be performed immediately after SSA destruction or later on, perhaps even after register allocation (see Chap. 22). It might be useful to postpone the copy sequentialization since it introduces arbitrary interference between variables. As an example, $a_{1}\!\leftarrow\!a_{2}\parallel b_{1}\!\leftarrow\!b_{2}$ (where $\mathit{inst}_{1}\parallel\mathit{inst}_{2}$ represents two instructions $\mathit{inst}_{1}$ and $\mathit{inst}_{2}$ to be executed simultaneously) can be sequentialized into $a_{1}\!\leftarrow\!a_{2}$; $b_{1}\!\leftarrow\!b_{2}$, which would make $b_{2}$ interfere with $a_{1}$ while the other way round $b_{1}\!\leftarrow\!b_{2}$; $a_{1}\!\leftarrow\!a_{2}$ would make $a_{2}$ interfere with $b_{1}$ instead.

If we still decide to replace parallel copies into a sequence of simple copies immediately after SSA destruction, this can be done as shown in Algorithm 3.6. To see that this algorithm converges, one can visualize the parallel copy as a graph where nodes represent resources and edges represent transfers of values: the number of steps is exactly the number of cycles plus the number of non-self-edges of this graph. The correctness comes from the invariance of the behaviour of _seq_; _pcopy_. An optimized implementation of this algorithm will be presented in Chap. 21.

![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310090751841.png)


## 3.3 SSA Property Transformations

As discussed in Chap. 2, SSA comes in different flavours. This section describes algorithms that transform arbitrary SSA code into the desired flavour. Making SSA _conventional_ corresponds exactly to the first phase of SSA destruction (described in Sect. 3.2) that splits critical edges and introduces parallel copies (sequentialized later in bulk or on-demand) around $\phi$-functions. As already discussed, this straightforward algorithm has several drawbacks addressed in Chap. 21.

Making SSA _strict_, i.e., fulfil _the dominance property_, is as "hard" as constructing SSA. Of course, a pre-pass through the graph can detect the offending variables that have definitions that do not dominate their uses. Then there are several possible single-variable $\phi$-function insertion algorithms (see Chap. 4) that can be used to patch up the SSA, by restricting attention to the set of non-conforming variables. The renaming phase can also be applied with the same filtering process. As the number of variables requiring repair might be a small proportion of all variables, a costly traversal of the whole program can be avoided by building the def-use chains (for non-conforming variables) during the detection pre-pass. Renaming can then be done on a per-variable basis or better (if pruned SSA is preferred) the reconstruction algorithm presented in Chap. 5 can be used for both $\phi$-functions placement and renaming.

The construction algorithm described above does not build _pruned_ SSA form. If available, liveness information can be used to filter out the insertion of $\phi$-functions wherever the variable is not live: The resulting SSA form is pruned. Alternatively, pruning SSA form is equivalent to a dead code elimination pass after SSA construction. As use-def chains are implicitly provided by SSA form, dead-$\phi$-function elimination simply relies on marking actual uses (non-$\phi$-function ones) as _useful_ and propagating _usefulness_ backwards through $\phi$-functions. Algorithm 3.7 presents the relevant pseudo-code for this operation. Here, _stack_ is used to store useful and unprocessed variables defined by $\phi$-functions.
![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310090752318.png)
To construct pruned SSA form via dead code elimination, it is generally much faster to first build _semi-pruned SSA form_, rather than minimal SSA form, and then apply dead code elimination. Semi-pruned SSA form is based on the observation that many variables are _local_, i.e., have a small live range that is within a single basic block. Consequently, pruned SSA would not instantiate any $\phi$-functions for these variables. Such variables can be identified by a linear traversal over each basic block of the CFG. All of these variables can be filtered out: minimal SSA form restricted to the remaining variables gives rise to the so-called semi-pruned SSA form.

### 3.3.1 Pessimistic $\phi$-Function Insertion

Construction of minimal SSA, as outlined in Sect. 3.1, comes at the price of a sophisticated algorithm involving the computation of iterated dominance frontiers. Alternatively, $\phi$-functions may be inserted in a _pessimistic_ fashion, as detailed below. In the pessimistic approach, a $\phi$-function is inserted at the start of each CFG node (basic block) for each variable that is live at the start of that node. A less sophisticated, or _crude_, strategy is to insert a $\phi$-function for each variable at the start of each CFG node; the resulting SSA will not be pruned. When a pessimistic approach is used, many inserted $\phi$-functions are redundant. Code transformations such as code motion, or other CFG modifications, can also introduce redundant $\phi$-functions, i.e., make a minimal SSA program become non-minimal.

The application of _copy propagation_ and rule-based $\phi$-function rewriting can remove many of these redundant $\phi$-functions. As already mentioned in Chap. 2, copy propagation can break the dominance property by propagating variable $a_{j}$ through $\phi$-functions of the form $a_{i}=\phi\left(a_{x_{1}},\ldots,a_{x_{k}}\right)$ where all the source operands are syntactically equal to either $a_{j}$ or $\bot$. If we want to avoid breaking the dominance property we simply have to avoid applying copy propagations that involve $\bot$. A more interesting rule is the one that propagates $a_{j}$ through a $\phi$-function of the form $a_{i}=\phi\left(a_{x_{1}},\ldots,a_{x_{k}}\right)$ where all the source operands are syntactically equal to either $a_{i}$ or $a_{j}$. These $\phi$-functions turn out to be "identity" operations, where all the source operands become syntactically identical and equivalent to the destination operand. As such, they can be trivially eliminated from the program. Identity $\phi$-functions can be simplified this way from inner to outer loops. To be efficient, variable def-use chains should be pre-computed--otherwise as many iterations as the maximum loop depth might be necessary for copy propagation. When the CFG is reducible,2 this simplification produces minimal SSA. The underlying reason is that the def-use chain of a given $\phi$-web is, in this context, isomorphic with a subgraph of the reducible CFG: All nodes except those that can be reached by two different paths (from actual non-$\phi$-function definitions) can be simplified by iterative application of $T1$ (removal of self-edge) and $T2$ (merge of a node with its unique direct predecessor) graph reductions. Figure 3.5 illustrates these graph transformations. To be precise, we can simplify an SSA program as follows:

Footnote 2: A CFG is reducible if there are no jumps into the middle of loops from the outside, so the only entry to a loop is through its header. Section 3.4 gives a fuller discussion of reducibility, with pointers to further reading.

1. Remove any $\phi$-node $a_{i}=\phi\left(a_{x_{1}},\ldots,a_{x_{k}}\right)$ where all $a_{x_{n}}$ are $a_{j}$. Replace all occurrences of $a_{i}$ by $a_{j}$. This corresponds to $T2$ reduction.
2. Remove any $\phi$-node $a_{i}=\phi\left(a_{x_{1}},\ldots,a_{x_{k}}\right)$ where all $a_{x_{n}}\in\left\{a_{i},\,a_{j}\right\}$. Replace all occurrences of $a_{i}$ by $a_{j}$. This corresponds to $T1$ followed by $T2$ reduction.

![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310090752574.png)

This approach can be implemented using a worklist, which stores the candidate nodes for simplification. Using the graph made up of def-use chains (see Chap. 14), the worklist can be initialized with $\phi$-functions that are direct successors of non-$\phi$-functions. However, for simplicity, we may initialize it with all $\phi$-functions. Of course, if loop nesting forest information is available, the worklist can be avoided by traversing the CFG in a single pass from inner to outer loops, and in a topological order within each loop (header excluded). But since we believe the main motivation for this approach to be its simplicity, the pseudo-code shown in Algorithm 3 uses a work queue.

This algorithm is guaranteed to terminate in a fixed number of steps. At every iteration of the while loop, it removes a $\phi$-function from the work queue $W$. Whenever it adds new $\phi$-functions to $W$, it removes a $\phi$-function from the program. The number of $\phi$-functions in the program is bounded so the number of insertions to $W$ is bounded. The queue could be replaced by a worklist, and the insertions/removals done at random. The algorithm would be less efficient, but the end result would be the same.

![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310090752136.png)


## 3.4 Further Reading

The early literature on SSA form [89, 90] introduces the two phases of the construction algorithm we have outlined in this chapter and discusses algorithmic complexity on common and worst-case inputs. These initial presentations trace the ancestry of SSA form back to early work on data-flow representations by Shapiro and Saint [258].

Briggs et al. [50] discuss pragmatic refinements to the original algorithms for SSA construction and destruction, with the aim of reducing execution time. They introduce the notion of semi-pruned form, show how to improve the efficiency of the stack-based renaming algorithm, and describe how copy propagation must be constrained to preserve correct code during SSA destruction.

There are numerous published descriptions of alternative algorithms for SSA construction, in particular for the $\phi$-function insertion phase. The pessimistic approach that first inserts $\phi$-functions at all control-flow merge points and then removes unnecessary ones using simple _T1/T2_ rewrite rules was proposed by Aycock and Horspool [14]. Brandis and Mossenbock [44] describe a simple, syntax-directed approach to SSA construction from well structured high-level source code. Throughout this textbook, we consider the more general case of SSA construction from arbitrary CFGs.

A _reducible_ CFG is one that will collapse to a single node when it is transformed using repeated application of _T1/T2_ rewrite rules. Aho et al. [2] describe the concept of reducibility and trace its history in early compilers literature.

Sreedhar and Gao [263] pioneer linear-time complexity $\phi$-function insertion algorithms based on DJ-graphs. These approaches have been refined by other researchers. Chapter 4 explores these alternative construction algorithms in depth.

Blech et al. [30] formalize the semantics of SSA, in order to verify the correctness of SSA destruction algorithms. Boissinot et al. [35] review the history of SSA destruction approaches and highlight misunderstandings that led to incorrect destruction algorithms. Chapter 21 presents more details on alternative approaches to SSA destruction.

There are instructive dualisms between concepts in SSA form and functional programs, including construction, dominance, and copy propagation. Chapter 6 explores these issues in more detail.